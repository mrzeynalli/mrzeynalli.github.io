[{"content":"This project is carried out by me and my classmate under an assignment for one of my courses at the University of Edinburgh. The project received an A grade; all the coding and visualizations along with a mini report are shared on my GitHub account.\nProject description\nLanguage: Python Working file: Jupyter notebook In 2018, former President of South Africa, Jacob Zuma, established a commission of enquiry in state capture, known as The Judicial Commission of Inquiry into Allegations of State Capture, Corruption and Fraud in the Public Sector including Organs of State, or simply the Zondo Commission. The commission collected one exabyte of evidence, and on 22 June 2022 released its final report. The reports are available publicly in this link.\nImage is taken from www.corruptionwatch.org.za\nThis project aims to analyze the contents of these reports to capture the names of publicly traded companies whose names are mentioned more frequently. The results would be beneficial for prudent investors who seek ethical investments opportunities in South Africa.\nData collection and cleaning Two datasets were used in this project. First dataset were created by converting the report .pdf files into .csv format. PyPDF2 module was used for transforming each pdf content into a text-formatted object, and Pandas was used to store those text-formatted objects into a dataframe. The final dataset of the texts were converted to .csv file and uploaded to Github, where the file\u0026rsquo;s raw format was used.\nimport pandas, PyPDF2 # storing all the pdf files in a list report_paths = [\u0026#39;OCR version - State Capture Commission Report Part 1 Vol I.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part II Vol II.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part III Vol I - Bosasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part III Vol II - Bosasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part III Vol III - Bosasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part III Vol IV - Bosasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part IV Vol I - NT,EOH,COJ,Alexkor.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part IV Vol II- FS.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part IV Vol III - Eskom.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part IV Vol IV - Eskom.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part V Vol I - SSA.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part V Vol II - SABC,Waterkloof,Prasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part VI Vol I - Estina,Vrede.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part VI Vol II - CR.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part VI Vol III - Flow of Funds.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part VI Vol IV - Recommendations.pdf\u0026#39;] # creating a dataframe file zondo_reports = pd.DataFrame(columns=[\u0026#39;report\u0026#39;, \u0026#39;text\u0026#39;]) i = 0 # iterating through the pdf list for path in report_paths: # converting each pdf content into a text object pdfFileObj = open(path, \u0026#39;rb\u0026#39;) print(\u0026#39;opened\u0026#39;, path) pdfReader = PyPDF2.PdfReader(pdfFileObj) text=\u0026#39;\u0026#39; for page in pdfReader.pages: text += page.extract_text() print(\u0026#39;extracted text for\u0026#39;, path) # adding the converted text objects into dataframe zondo_reports.loc[i] = [path, text] i+=1 pdfFileObj.close() print(\u0026#39;closed\u0026#39;, path) After obtaining the first dataset, we needed another dataset consisting of the companies that are publicly traded in South Africa. For this objective, we used Johannesburg Stock Exchange (JSE) portal to pick up the candidate companies. January 2021 dataset of the companies were downloaded from the JSE website in .xlsx format. The file then were re-formatted into .csv to be readable from GitHub. The .csv file was uploaded to my GitHub account, and its raw format was called using read_csv method of Pandas.\nAs of now, we have a dataset containing reports contents and a dataset containing the publicly traded companies in South Africa.\nData analysis To carry out the text analysis, nltk module was primarily used. The below code displays how different classes of that module were imported:\nimport nltk, string from nltk.tokenize import word_tokenize nltk.download(\u0026#39;punkt\u0026#39;) from nltk.corpus import stopwords nltk.download(\u0026#39;stopwords\u0026#39;) from nltk.probability import FreqDist Firstly, we used word_tokenize function to tokenize all the reports contents into a single variable. Although the best practice is to lowercase all the tokens, we intentionally left the words as they were, given the fact that we were looking for company names, which are proper nouns. Thus, leaving proper nouns as they were was a better choice for the analysis.\nTo get rid of unnecessary words, we created a variable consisting of stop words from nltk.corpus and punctuations from string module. Further, some additional junk words were added to be removed from the tokens.\n# remove some stops from the tokens junk_tokens = [\u0026#39;Mr\u0026#39;,\u0026#39;Ms\u0026#39;,\u0026#39;Dr\u0026#39;,\u0026#39;P\u0026#39;,\u0026#39;``\u0026#39;, \u0026#39;\\\u0026#39;s\u0026#39;,\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\\\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\\\u0026#39;\\\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\u0026#34;\u0026#39;,\u0026#39;\u0026#34;\u0026#39;,\u0026#39;................................\u0026#39;,\u0026#39;L\u0026#39;] removables = set(stopwords.words(\u0026#39;English\u0026#39;) + list(string.punctuation) + list(string.digits) + junk_tokens) filtered_tokens = [token for token in all_content_tokens if token not in removables] Numerous company names come with additional descriptive words attached to them such as \u0026lsquo;Holding\u0026rsquo;, \u0026lsquo;Corporation\u0026rsquo;, \u0026lsquo;Limited\u0026rsquo;, and etc. We created a function that checks the name for each company and leaves those additions out. The new names were added to the dataframe under a column name \u0026lsquo;search term\u0026rsquo;. Afterwards, finally, time came for searching for company names (using the search terms) inside the reports (texts that are stored in the first dataframe).\nFor companies with one search term, we just searched the tokens as they are single words. For the companies with two or three words in their names, we used bigrams and trigrams from nltk, respectively. An additional column was added to the dataframe indicating True if the company name is mentioned in the reports and False otherwise.\n# Bigrams and trigrams are created to search for two-word and three-word search terms individually. bigrams = list(nltk.bigrams(filtered_tokens)) trigrams = list(nltk.trigrams(filtered_tokens)) # Create new column that will store whether a company name is mentioned or not. listed_companies[\u0026#34;FoundInReport\u0026#34;] = \u0026#34;False\u0026#34; for ind in listed_companies.index: searchterm = listed_companies[\u0026#39;SearchTerm\u0026#39;][ind] if len(searchterm) == 1: if searchterm[0] in filtered_tokens: # search for one-word search terms print(\u0026#39;1 word company appeared in text:\u0026#39;, searchterm) listed_companies.at[ind,\u0026#39;FoundInReport\u0026#39;] = True if len(searchterm) == 2: if searchterm in bigrams: # search for two-word search terms print(\u0026#39;2 word company appeared in text:\u0026#39;, searchterm) listed_companies.at[ind,\u0026#39;FoundInReport\u0026#39;] = True if len(searchterm) == 3: if searchterm in trigrams: # search for three-word search terms print(\u0026#39;3 word company appeared in text:\u0026#39;, searchterm) listed_companies.at[ind,\u0026#39;FoundInReport\u0026#39;] = True Next, we used FreqDist function from nltk.probability to calculate how many times each company name is mentioned in all reports combined.\nData visualization and project results Matplotlib and seaborn libraries were used for the purpose of visualization. The following figures were drawn to illustrate the results.\nFigure 1: The frequency of company name mentions by sector Figure 1 shows how many times the names of companies in each sector is mentioned in the reports. Banking companies top the chart, by having 236 name mentions in total. It is followed by Mining, Software\u0026amp;Computing Services, and Media, by being mentioned 197, 110, and 76 times, respectively.\nFigure 2: Frequency of mentions for company Figure 2 clearly depicts the company names that were mentioned in the reports. Glencore Plc is the company with the highest quantity of mentions (190). Standard Bank Group Limited and Nedbank Group Limited are the main banks whose names are widely used in the reports. The following are the EOH Holding Limited and MultiChoice Group Limited.\nNote: It is important to note that the results of this analysis do not impose any kind of allegation against any of the companies mentioned above. As a matter of fact, the analysis focuses on the numerical counts rather than the context of the mentions. More clearly, it does not claim that the company names are mentioned in a negative manner. Thus, this project has only one primary purpose to be a guide for the investors. Additional in-depth research is up to the investors when they ponder of investing on those firms.\n","permalink":"http://localhost:1313/posts/test/","summary":"\u003cp\u003eThis project is carried out by me and my classmate under an assignment for one of my courses at the University of Edinburgh. The project received an A grade; all the coding and visualizations along with a mini report are shared on \u003ca href=\"https://github.com/mrzeynalli/Zondo-Reports-Analysis\"\u003emy GitHub account.\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cem\u003eProject description\u003c/em\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eLanguage:\u003c/em\u003e Python\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eWorking file:\u003c/em\u003e Jupyter notebook\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn 2018, former President of South Africa, Jacob Zuma, established a commission of enquiry in state capture, known as The Judicial Commission of Inquiry into Allegations of State Capture, Corruption and Fraud in the Public Sector including Organs of State, or simply the Zondo Commission. The commission collected one exabyte of evidence, and on 22 June 2022 released its final report. The reports are available publicly in this link.\u003c/p\u003e","title":"Zondo Reports — Text Analysis"},{"content":"This project is carried out by me independently. The dataset is obtained publicly from Kaggle. All codes and their explanations are stored in my GitHub repository.\nLanguage: Python Libraries: sklearn, pandas, numpy, matplotlib, seaborn IDE: Jupyter notebook Project type: Machine learning, Unsupervised learning, K-Means clustering FIFA 23 is a football video game created by Electronic Arts (EA). It became the best-selling football video game in Christmas UK retail charts. According to EA statistics, the game contains more than 700 teams with over 19,000 football players, playing in at least 30 football leagues. The data used in this project is taken from Kaggle. The objective of this project is to classify the players into various segments.\nPhoto by Hatem Boukhit on Unsplash\nAlthough I implemented this project just out of my interest in football and my general engagement in playing the FIFA video game, the results may be helpful for any external observer such that the valuation of players might be determined or justified based on the segment they are in. Additionally, a team\u0026rsquo;s success may be tested over the number of players from each segment with the purpose of finding out any correlation.\nData collection and cleaning The dataset used initially contained 17,660 rows and 29 columns. In other words, 17,600 players were present in the dataset explained with 29 features. The features were both quantitative and qualitative, ranging from the player\u0026rsquo;s name to their market value. I first used Pandas to call the raw format of the CSV file from the GitHub repo. After careful observation of the dataset, I noticed that some players, who already retired from a club, are still shown as the players of that particular club. I decided to eliminate those players in order to have the most up-to-date statistics. To tackle this problem, an interesting point in the dataset helped me.\nThe already-retired players\u0026rsquo; names start with their latest kit number (\u0026ldquo;15 Xavi\u0026rdquo;, \u0026ldquo;22 D. Alves\u0026rdquo;). This enabled me to separate those players by applying an algorithm to detect the player names that start with digits. The codes are as follows:\n# This list object will store the indices of players whose names start with digit player_indices_to_remove = [] # This loop iterates through the indices of all players to detect the ones with digit-starting player names for index in fifa23_df.index.to_list(): player = fifa23_df[\u0026#39;Name\u0026#39;].loc[index] player_first_name = fifa23_df[\u0026#39;Name\u0026#39;].loc[index][0] # If the player name starts with digit, it adds the index of that observation to removable indices list if player_first_name.isnumeric(): player_indices_to_remove.append(index) # Now, we drop the indices of players who are unnecesarily included in the dataset fifa23_df.drop(player_indices_to_remove,axis=0,inplace=True) # We reset the indices of the dataframe fifa23_df.reset_index(drop=True,inplace=True) The codes go over each player\u0026rsquo;s name by iterating through the indices of the observations, taking out the very first character of that name, and checking if that character is numeric. Later, indices of the positive cases are added to the previously created list that aims to store retired players. Subsequently, those players are dropped out of the dataset, and the indexing is re-set.\nAfter tackling this problem, I faced yet another data problem. The numeric values indicating monetary values are put in with their respective currency and the prefixes, M and K, for millions and thousands, respectively. I needed to first eliminate the currency symbol and convert the values into their actual value. I formulated the below function that handles this specific duty:\ndef curreny_correction(column,dataframe, curreny_sign = str): # Split the value by the given currency symbol: euro in our instance splits = dataframe[column].str.split(curreny_sign, expand = True)[1] values = splits.str[:-1] # Store the values prefixes= splits.str[-1:] # Store the prefixes # Create a list object that will store the float-converted format of the values values_float = [] for value, prefix in zip(values, prefixes): # The wage and value point are either in thousands (K) or millions (M) or 0 if prefix == \u0026#39;M\u0026#39;: # Checks if letter is \u0026#39;M\u0026#39; or the value is million try: # When values are zero, they cannot be converted into float and raises ValueError. #I debug the coding for those occasions float_value = float(value) * 1000000 values_float.append(float_value) except ValueError: # Adds just 0 when ValueError is raised float_value = 0 values_float.append(float_value) elif prefix == \u0026#39;K\u0026#39;: # If the letter is \u0026#39;K\u0026#39; or the value is thousands try: # When values are zero, they cannot be converted into float and raises ValueError. #I debug the coding for those occasions float_value = float(value) * 1000 values_float.append(float_value) except ValueError: # Adds just 0 when ValueError is raised float_value = 0 values_float.append(float_value) # Returns the float values that are stripped of currency symbol and exponential letters return values_float Features including \u0026ldquo;Value\u0026rdquo;, \u0026ldquo;Wage\u0026rdquo;, and \u0026ldquo;Release Clause\u0026rdquo; were converted using the above function. Different scripts were written to convert the features (\u0026ldquo;Position\u0026rdquo;, \u0026ldquo;Height\u0026rdquo;, \u0026ldquo;Weight\u0026rdquo;) with slightly different characters.\n# Correcting \u0026#39;Position\u0026#39; feature fifa23_df[\u0026#39;Position\u0026#39;] = fifa23_df[\u0026#39;Position\u0026#39;].str.split(\u0026#39;\u0026gt;\u0026#39;, expand=True)[1] # Correcting \u0026#39;Height\u0026#39; feature heigh_values = [float(value) for value in fifa23_df[\u0026#39;Height\u0026#39;].str.split(\u0026#34;cm\u0026#34;, expand = True)[0]] fifa23_df[\u0026#39;Height\u0026#39;] = heigh_values # Correcting \u0026#39;Weight\u0026#39; feature weight_values = [float(value) for value in fifa23_df[\u0026#39;Weight\u0026#39;].str.split(\u0026#34;kg\u0026#34;, expand = True)[0]] fifa23_df[\u0026#39;Weight\u0026#39;] = weight_values I eliminated 11 features based on whether they are non-useful for the analysis or have huge null values. The final dataset ready for the analysis contained 17 features and 10,104 observations.\nData visualization I created a couple of pre-analytics graphs to get a better understanding of the data I am working with. Seaborn and matplotlib modules were used for visualization purposes.\nFigure 1: Overall Rating Score of players per their Preferred Foot Figure 1 displays how the players are distributed on their rating score based on their preferred foot. There seems to be not a big difference between the feet. However, a very slight superiority can be observed for the left foot (presumably, because of Leo Messi the GOAT).\nFigure 2: Scatter plot of Overall Rating Score and Age When it comes to the relationship between age and overall rating, an apparent positive linear relationship is visible (Figure 2). Seemingly, the greater the age of the player, the higher the rating is. The relationship seems to fade away after the age of 35.\nFigure 3: Correlation matrix The correlation among the numeric variables of the dataset can be seen in Figure 3. High correlation scores are visible between Wage and Value, Height and Weight, Value and Overall, and Value and Potential. Although Overall Rating and Age are highly positively correlated, interestingly, there is not much correlation between Potential and Age. This signals that the young players who do not have high overall at the moment can increase their rating score substantially.\nk-Means clustering Initially, the numeric features need to be scaled, given the fact that the presence of outliers along with huge variations among the ranges of different variables can negatively impact the k-Means clustering process. I used StandardScaler from sklearn.preprocessing to standardize the quantitative variables.\nFor specifying the best number of clusters, I carried out Within-Cluster Sum of Squares (WCSS) and Average Silhouette methods.\nFigure 4: WSCC method WSCC method calculates the cluster differences for each cluster number. By observing the above plot (Figure 4) for each k, it can be observed that the variations tend to slow down after around 5. Thus, the graph signals 5 to be the best number of clusters.\nFigure 5: Average Silhouette method In the average silhouette method, the distances between neighboring cluster items and local cluster items are calculated. Figure 5 shows the final average silhouette scores for each k cluster number. Accordingly, the highest score signals the best k (besides 2).\nAs a result, I have enough proof to use 5 as my cluster number.\nProject results: Final clusters I used the kMeans function from sklearn.cluster to do the clustering. 5 different clusters were formed for the players in the dataset. The clusters and the number of players in each cluster are the following:\nCluster 0 has 2640 players Cluster 1 has 3228 players Cluster 2 has 903 players Cluster 3 has 3197 players Cluster 4 has 136 players Table 1: Cluster results\ncluster Age Overall Potential Value Wage Height Weight Release Clause 0 20.47 56.40 67.42 386295.45 44692.80 180.29 72.88 7.700856e+05 1 23.80 67.22 73.81 2275497.21 14163.88 175.94 69.68 4.378210e+06 2 26.35 78.79 81.78 19393023.26 49131.78 182.18 75.93 3.738128e+07 3 25.44 66.95 72.08 1892169.22 15918.36 187.44 81.06 3.557997e+06 4 26.29 85.38 88.04 67459558.82 147213.24 182.27 76.98 1.309640e+08 Table 1 demonstrates how the players are separated based on the clusters.\nCluster 0 is characterized by younger players, who have relatively low rating scores and potential. Their values and wages are low, correspondingly. They are the kind of football players, who play in and are transferred by middle-sized clubs. Yet, they usually have a high release clause because of their young age.\nCluster 1 accommodates young and middle-aged players with yet low rating scores. These players usually start and end their careers in small- and middle-sized teams. However, given their still young age, they have a high release clause, as well.\nCuster 2 has, on average, the oldest players among the clusters. The values and wages the players of this category have are fairly large, which signals their importance in the team. They mostly play in middle-sized and big teams, given their high values. Given their height and rating, I reckon that these are the strikes that play a crucial role in the attack.\nCluster 3 seems to have the same type of players as cluster 2. The difference is that cluster 3 players play in small- and middle-sized teams, explained by their low value and wage as well as low rating score and potential. They are quite tall and key players on the attack.\nCluster 4 takes the best players in the arena. They are still performing high and are the key players in their big teams. Arguably, they have been succeeding on their teams for quite a long time. Now, as they are old, their release clauses are also very low. Messi, Ronaldo, and Lewandowski should be in this cluster.\n","permalink":"http://localhost:1313/posts/fifa23_players_analysis/","summary":"\u003cp\u003eThis project is carried out by me independently. The dataset is obtained publicly from Kaggle. All codes and their explanations are stored in \u003ca href=\"https://github.com/mrzeynalli/fifa_23_players_analysis\"\u003emy GitHub repository\u003c/a\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eLanguage:\u003c/em\u003e Python\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eLibraries:\u003c/em\u003e sklearn, pandas, numpy, matplotlib, seaborn\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eIDE:\u003c/em\u003e Jupyter notebook\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eProject type:\u003c/em\u003e Machine learning, Unsupervised learning, K-Means clustering\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFIFA 23 is a football video game created by Electronic Arts (EA). It became the best-selling football video game in \u003ca href=\"https://www.gamesindustry.biz/fifa-is-christmas-no1-as-god-of-war-drops-to-third-place-uk-boxed-charts#:~:text=FIFA%2023%20was%20the%20best,a%2013%25%20boost%20in%20sales.\"\u003eChristmas UK retail charts\u003c/a\u003e. According to \u003ca href=\"https://www.ea.com/games/fifa/news/fifa-23-all-leagues-clubs-teams-list#:~:text=With%20more%20than%2019%2C000%20players,%2C%20Bundesliga%2C%20LaLiga%20Santander%2C%20CONMEBOL\"\u003eEA statistics\u003c/a\u003e, the game contains more than 700 teams with over 19,000 football players, playing in at least 30 football leagues. The data used in this project is taken from \u003ca href=\"https://www.kaggle.com/datasets/bryanb/fifa-player-stats-database\"\u003eKaggle\u003c/a\u003e. The objective of this project is to classify the players into various segments.\u003c/p\u003e","title":"FIFA23 Players Analysis: k-Means Clustering"},{"content":"This project is carried out by me and my classmate under an assignment for one of my courses at the University of Edinburgh. The project received an A grade; all the coding and visualizations along with a mini report are shared on my GitHub account.\nProject description\nLanguage: Python Working file: Jupyter notebook In 2018, former President of South Africa, Jacob Zuma, established a commission of enquiry in state capture, known as The Judicial Commission of Inquiry into Allegations of State Capture, Corruption and Fraud in the Public Sector including Organs of State, or simply the Zondo Commission. The commission collected one exabyte of evidence, and on 22 June 2022 released its final report. The reports are available publicly in this link.\nImage is taken from www.corruptionwatch.org.za\nThis project aims to analyze the contents of these reports to capture the names of publicly traded companies whose names are mentioned more frequently. The results would be beneficial for prudent investors who seek ethical investments opportunities in South Africa.\nData collection and cleaning Two datasets were used in this project. First dataset were created by converting the report .pdf files into .csv format. PyPDF2 module was used for transforming each pdf content into a text-formatted object, and Pandas was used to store those text-formatted objects into a dataframe. The final dataset of the texts were converted to .csv file and uploaded to Github, where the file\u0026rsquo;s raw format was used.\nimport pandas, PyPDF2 # storing all the pdf files in a list report_paths = [\u0026#39;OCR version - State Capture Commission Report Part 1 Vol I.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part II Vol II.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part III Vol I - Bosasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part III Vol II - Bosasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part III Vol III - Bosasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part III Vol IV - Bosasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part IV Vol I - NT,EOH,COJ,Alexkor.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part IV Vol II- FS.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part IV Vol III - Eskom.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part IV Vol IV - Eskom.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part V Vol I - SSA.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part V Vol II - SABC,Waterkloof,Prasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part VI Vol I - Estina,Vrede.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part VI Vol II - CR.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part VI Vol III - Flow of Funds.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part VI Vol IV - Recommendations.pdf\u0026#39;] # creating a dataframe file zondo_reports = pd.DataFrame(columns=[\u0026#39;report\u0026#39;, \u0026#39;text\u0026#39;]) i = 0 # iterating through the pdf list for path in report_paths: # converting each pdf content into a text object pdfFileObj = open(path, \u0026#39;rb\u0026#39;) print(\u0026#39;opened\u0026#39;, path) pdfReader = PyPDF2.PdfReader(pdfFileObj) text=\u0026#39;\u0026#39; for page in pdfReader.pages: text += page.extract_text() print(\u0026#39;extracted text for\u0026#39;, path) # adding the converted text objects into dataframe zondo_reports.loc[i] = [path, text] i+=1 pdfFileObj.close() print(\u0026#39;closed\u0026#39;, path) After obtaining the first dataset, we needed another dataset consisting of the companies that are publicly traded in South Africa. For this objective, we used Johannesburg Stock Exchange (JSE) portal to pick up the candidate companies. January 2021 dataset of the companies were downloaded from the JSE website in .xlsx format. The file then were re-formatted into .csv to be readable from GitHub. The .csv file was uploaded to my GitHub account, and its raw format was called using read_csv method of Pandas.\nAs of now, we have a dataset containing reports contents and a dataset containing the publicly traded companies in South Africa.\nData analysis To carry out the text analysis, nltk module was primarily used. The below code displays how different classes of that module were imported:\nimport nltk, string from nltk.tokenize import word_tokenize nltk.download(\u0026#39;punkt\u0026#39;) from nltk.corpus import stopwords nltk.download(\u0026#39;stopwords\u0026#39;) from nltk.probability import FreqDist Firstly, we used word_tokenize function to tokenize all the reports contents into a single variable. Although the best practice is to lowercase all the tokens, we intentionally left the words as they were, given the fact that we were looking for company names, which are proper nouns. Thus, leaving proper nouns as they were was a better choice for the analysis.\nTo get rid of unnecessary words, we created a variable consisting of stop words from nltk.corpus and punctuations from string module. Further, some additional junk words were added to be removed from the tokens.\n# remove some stops from the tokens junk_tokens = [\u0026#39;Mr\u0026#39;,\u0026#39;Ms\u0026#39;,\u0026#39;Dr\u0026#39;,\u0026#39;P\u0026#39;,\u0026#39;``\u0026#39;, \u0026#39;\\\u0026#39;s\u0026#39;,\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\\\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\\\u0026#39;\\\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\u0026#34;\u0026#39;,\u0026#39;\u0026#34;\u0026#39;,\u0026#39;................................\u0026#39;,\u0026#39;L\u0026#39;] removables = set(stopwords.words(\u0026#39;English\u0026#39;) + list(string.punctuation) + list(string.digits) + junk_tokens) filtered_tokens = [token for token in all_content_tokens if token not in removables] Numerous company names come with additional descriptive words attached to them such as \u0026lsquo;Holding\u0026rsquo;, \u0026lsquo;Corporation\u0026rsquo;, \u0026lsquo;Limited\u0026rsquo;, and etc. We created a function that checks the name for each company and leaves those additions out. The new names were added to the dataframe under a column name \u0026lsquo;search term\u0026rsquo;. Afterwards, finally, time came for searching for company names (using the search terms) inside the reports (texts that are stored in the first dataframe).\nFor companies with one search term, we just searched the tokens as they are single words. For the companies with two or three words in their names, we used bigrams and trigrams from nltk, respectively. An additional column was added to the dataframe indicating True if the company name is mentioned in the reports and False otherwise.\n# Bigrams and trigrams are created to search for two-word and three-word search terms individually. bigrams = list(nltk.bigrams(filtered_tokens)) trigrams = list(nltk.trigrams(filtered_tokens)) # Create new column that will store whether a company name is mentioned or not. listed_companies[\u0026#34;FoundInReport\u0026#34;] = \u0026#34;False\u0026#34; for ind in listed_companies.index: searchterm = listed_companies[\u0026#39;SearchTerm\u0026#39;][ind] if len(searchterm) == 1: if searchterm[0] in filtered_tokens: # search for one-word search terms print(\u0026#39;1 word company appeared in text:\u0026#39;, searchterm) listed_companies.at[ind,\u0026#39;FoundInReport\u0026#39;] = True if len(searchterm) == 2: if searchterm in bigrams: # search for two-word search terms print(\u0026#39;2 word company appeared in text:\u0026#39;, searchterm) listed_companies.at[ind,\u0026#39;FoundInReport\u0026#39;] = True if len(searchterm) == 3: if searchterm in trigrams: # search for three-word search terms print(\u0026#39;3 word company appeared in text:\u0026#39;, searchterm) listed_companies.at[ind,\u0026#39;FoundInReport\u0026#39;] = True Next, we used FreqDist function from nltk.probability to calculate how many times each company name is mentioned in all reports combined.\nData visualization and project results Matplotlib and seaborn libraries were used for the purpose of visualization. The following figures were drawn to illustrate the results.\nFigure 1: The frequency of company name mentions by sector Figure 1 shows how many times the names of companies in each sector is mentioned in the reports. Banking companies top the chart, by having 236 name mentions in total. It is followed by Mining, Software\u0026amp;Computing Services, and Media, by being mentioned 197, 110, and 76 times, respectively.\nFigure 2: Frequency of mentions for company Figure 2 clearly depicts the company names that were mentioned in the reports. Glencore Plc is the company with the highest quantity of mentions (190). Standard Bank Group Limited and Nedbank Group Limited are the main banks whose names are widely used in the reports. The following are the EOH Holding Limited and MultiChoice Group Limited.\nNote: It is important to note that the results of this analysis do not impose any kind of allegation against any of the companies mentioned above. As a matter of fact, the analysis focuses on the numerical counts rather than the context of the mentions. More clearly, it does not claim that the company names are mentioned in a negative manner. Thus, this project has only one primary purpose to be a guide for the investors. Additional in-depth research is up to the investors when they ponder of investing on those firms.\n","permalink":"http://localhost:1313/posts/test/","summary":"\u003cp\u003eThis project is carried out by me and my classmate under an assignment for one of my courses at the University of Edinburgh. The project received an A grade; all the coding and visualizations along with a mini report are shared on \u003ca href=\"https://github.com/mrzeynalli/Zondo-Reports-Analysis\"\u003emy GitHub account.\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cem\u003eProject description\u003c/em\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eLanguage:\u003c/em\u003e Python\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eWorking file:\u003c/em\u003e Jupyter notebook\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn 2018, former President of South Africa, Jacob Zuma, established a commission of enquiry in state capture, known as The Judicial Commission of Inquiry into Allegations of State Capture, Corruption and Fraud in the Public Sector including Organs of State, or simply the Zondo Commission. The commission collected one exabyte of evidence, and on 22 June 2022 released its final report. The reports are available publicly in this link.\u003c/p\u003e","title":"Zondo Reports — Text Analysis"},{"content":"This project is carried out by me independently. The dataset is obtained publicly from Kaggle. All codes and their explanations are stored in my GitHub repository.\nLanguage: Python Libraries: sklearn, pandas, numpy, matplotlib, seaborn IDE: Jupyter notebook Project type: Machine learning, Unsupervised learning, K-Means clustering FIFA 23 is a football video game created by Electronic Arts (EA). It became the best-selling football video game in Christmas UK retail charts. According to EA statistics, the game contains more than 700 teams with over 19,000 football players, playing in at least 30 football leagues. The data used in this project is taken from Kaggle. The objective of this project is to classify the players into various segments.\nPhoto by Hatem Boukhit on Unsplash\nAlthough I implemented this project just out of my interest in football and my general engagement in playing the FIFA video game, the results may be helpful for any external observer such that the valuation of players might be determined or justified based on the segment they are in. Additionally, a team\u0026rsquo;s success may be tested over the number of players from each segment with the purpose of finding out any correlation.\nData collection and cleaning The dataset used initially contained 17,660 rows and 29 columns. In other words, 17,600 players were present in the dataset explained with 29 features. The features were both quantitative and qualitative, ranging from the player\u0026rsquo;s name to their market value. I first used Pandas to call the raw format of the CSV file from the GitHub repo. After careful observation of the dataset, I noticed that some players, who already retired from a club, are still shown as the players of that particular club. I decided to eliminate those players in order to have the most up-to-date statistics. To tackle this problem, an interesting point in the dataset helped me.\nThe already-retired players\u0026rsquo; names start with their latest kit number (\u0026ldquo;15 Xavi\u0026rdquo;, \u0026ldquo;22 D. Alves\u0026rdquo;). This enabled me to separate those players by applying an algorithm to detect the player names that start with digits. The codes are as follows:\n# This list object will store the indices of players whose names start with digit player_indices_to_remove = [] # This loop iterates through the indices of all players to detect the ones with digit-starting player names for index in fifa23_df.index.to_list(): player = fifa23_df[\u0026#39;Name\u0026#39;].loc[index] player_first_name = fifa23_df[\u0026#39;Name\u0026#39;].loc[index][0] # If the player name starts with digit, it adds the index of that observation to removable indices list if player_first_name.isnumeric(): player_indices_to_remove.append(index) # Now, we drop the indices of players who are unnecesarily included in the dataset fifa23_df.drop(player_indices_to_remove,axis=0,inplace=True) # We reset the indices of the dataframe fifa23_df.reset_index(drop=True,inplace=True) The codes go over each player\u0026rsquo;s name by iterating through the indices of the observations, taking out the very first character of that name, and checking if that character is numeric. Later, indices of the positive cases are added to the previously created list that aims to store retired players. Subsequently, those players are dropped out of the dataset, and the indexing is re-set.\nAfter tackling this problem, I faced yet another data problem. The numeric values indicating monetary values are put in with their respective currency and the prefixes, M and K, for millions and thousands, respectively. I needed to first eliminate the currency symbol and convert the values into their actual value. I formulated the below function that handles this specific duty:\ndef curreny_correction(column,dataframe, curreny_sign = str): # Split the value by the given currency symbol: euro in our instance splits = dataframe[column].str.split(curreny_sign, expand = True)[1] values = splits.str[:-1] # Store the values prefixes= splits.str[-1:] # Store the prefixes # Create a list object that will store the float-converted format of the values values_float = [] for value, prefix in zip(values, prefixes): # The wage and value point are either in thousands (K) or millions (M) or 0 if prefix == \u0026#39;M\u0026#39;: # Checks if letter is \u0026#39;M\u0026#39; or the value is million try: # When values are zero, they cannot be converted into float and raises ValueError. #I debug the coding for those occasions float_value = float(value) * 1000000 values_float.append(float_value) except ValueError: # Adds just 0 when ValueError is raised float_value = 0 values_float.append(float_value) elif prefix == \u0026#39;K\u0026#39;: # If the letter is \u0026#39;K\u0026#39; or the value is thousands try: # When values are zero, they cannot be converted into float and raises ValueError. #I debug the coding for those occasions float_value = float(value) * 1000 values_float.append(float_value) except ValueError: # Adds just 0 when ValueError is raised float_value = 0 values_float.append(float_value) # Returns the float values that are stripped of currency symbol and exponential letters return values_float Features including \u0026ldquo;Value\u0026rdquo;, \u0026ldquo;Wage\u0026rdquo;, and \u0026ldquo;Release Clause\u0026rdquo; were converted using the above function. Different scripts were written to convert the features (\u0026ldquo;Position\u0026rdquo;, \u0026ldquo;Height\u0026rdquo;, \u0026ldquo;Weight\u0026rdquo;) with slightly different characters.\n# Correcting \u0026#39;Position\u0026#39; feature fifa23_df[\u0026#39;Position\u0026#39;] = fifa23_df[\u0026#39;Position\u0026#39;].str.split(\u0026#39;\u0026gt;\u0026#39;, expand=True)[1] # Correcting \u0026#39;Height\u0026#39; feature heigh_values = [float(value) for value in fifa23_df[\u0026#39;Height\u0026#39;].str.split(\u0026#34;cm\u0026#34;, expand = True)[0]] fifa23_df[\u0026#39;Height\u0026#39;] = heigh_values # Correcting \u0026#39;Weight\u0026#39; feature weight_values = [float(value) for value in fifa23_df[\u0026#39;Weight\u0026#39;].str.split(\u0026#34;kg\u0026#34;, expand = True)[0]] fifa23_df[\u0026#39;Weight\u0026#39;] = weight_values I eliminated 11 features based on whether they are non-useful for the analysis or have huge null values. The final dataset ready for the analysis contained 17 features and 10,104 observations.\nData visualization I created a couple of pre-analytics graphs to get a better understanding of the data I am working with. Seaborn and matplotlib modules were used for visualization purposes.\nFigure 1: Overall Rating Score of players per their Preferred Foot Figure 1 displays how the players are distributed on their rating score based on their preferred foot. There seems to be not a big difference between the feet. However, a very slight superiority can be observed for the left foot (presumably, because of Leo Messi the GOAT).\nFigure 2: Scatter plot of Overall Rating Score and Age When it comes to the relationship between age and overall rating, an apparent positive linear relationship is visible (Figure 2). Seemingly, the greater the age of the player, the higher the rating is. The relationship seems to fade away after the age of 35.\nFigure 3: Correlation matrix The correlation among the numeric variables of the dataset can be seen in Figure 3. High correlation scores are visible between Wage and Value, Height and Weight, Value and Overall, and Value and Potential. Although Overall Rating and Age are highly positively correlated, interestingly, there is not much correlation between Potential and Age. This signals that the young players who do not have high overall at the moment can increase their rating score substantially.\nk-Means clustering Initially, the numeric features need to be scaled, given the fact that the presence of outliers along with huge variations among the ranges of different variables can negatively impact the k-Means clustering process. I used StandardScaler from sklearn.preprocessing to standardize the quantitative variables.\nFor specifying the best number of clusters, I carried out Within-Cluster Sum of Squares (WCSS) and Average Silhouette methods.\nFigure 4: WSCC method WSCC method calculates the cluster differences for each cluster number. By observing the above plot (Figure 4) for each k, it can be observed that the variations tend to slow down after around 5. Thus, the graph signals 5 to be the best number of clusters.\nFigure 5: Average Silhouette method In the average silhouette method, the distances between neighboring cluster items and local cluster items are calculated. Figure 5 shows the final average silhouette scores for each k cluster number. Accordingly, the highest score signals the best k (besides 2).\nAs a result, I have enough proof to use 5 as my cluster number.\nProject results: Final clusters I used the kMeans function from sklearn.cluster to do the clustering. 5 different clusters were formed for the players in the dataset. The clusters and the number of players in each cluster are the following:\nCluster 0 has 2640 players Cluster 1 has 3228 players Cluster 2 has 903 players Cluster 3 has 3197 players Cluster 4 has 136 players Table 1: Cluster results\ncluster Age Overall Potential Value Wage Height Weight Release Clause 0 20.47 56.40 67.42 386295.45 44692.80 180.29 72.88 7.700856e+05 1 23.80 67.22 73.81 2275497.21 14163.88 175.94 69.68 4.378210e+06 2 26.35 78.79 81.78 19393023.26 49131.78 182.18 75.93 3.738128e+07 3 25.44 66.95 72.08 1892169.22 15918.36 187.44 81.06 3.557997e+06 4 26.29 85.38 88.04 67459558.82 147213.24 182.27 76.98 1.309640e+08 Table 1 demonstrates how the players are separated based on the clusters.\nCluster 0 is characterized by younger players, who have relatively low rating scores and potential. Their values and wages are low, correspondingly. They are the kind of football players, who play in and are transferred by middle-sized clubs. Yet, they usually have a high release clause because of their young age.\nCluster 1 accommodates young and middle-aged players with yet low rating scores. These players usually start and end their careers in small- and middle-sized teams. However, given their still young age, they have a high release clause, as well.\nCuster 2 has, on average, the oldest players among the clusters. The values and wages the players of this category have are fairly large, which signals their importance in the team. They mostly play in middle-sized and big teams, given their high values. Given their height and rating, I reckon that these are the strikes that play a crucial role in the attack.\nCluster 3 seems to have the same type of players as cluster 2. The difference is that cluster 3 players play in small- and middle-sized teams, explained by their low value and wage as well as low rating score and potential. They are quite tall and key players on the attack.\nCluster 4 takes the best players in the arena. They are still performing high and are the key players in their big teams. Arguably, they have been succeeding on their teams for quite a long time. Now, as they are old, their release clauses are also very low. Messi, Ronaldo, and Lewandowski should be in this cluster.\n","permalink":"http://localhost:1313/posts/fifa23_players_analysis/","summary":"\u003cp\u003eThis project is carried out by me independently. The dataset is obtained publicly from Kaggle. All codes and their explanations are stored in \u003ca href=\"https://github.com/mrzeynalli/fifa_23_players_analysis\"\u003emy GitHub repository\u003c/a\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eLanguage:\u003c/em\u003e Python\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eLibraries:\u003c/em\u003e sklearn, pandas, numpy, matplotlib, seaborn\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eIDE:\u003c/em\u003e Jupyter notebook\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eProject type:\u003c/em\u003e Machine learning, Unsupervised learning, K-Means clustering\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFIFA 23 is a football video game created by Electronic Arts (EA). It became the best-selling football video game in \u003ca href=\"https://www.gamesindustry.biz/fifa-is-christmas-no1-as-god-of-war-drops-to-third-place-uk-boxed-charts#:~:text=FIFA%2023%20was%20the%20best,a%2013%25%20boost%20in%20sales.\"\u003eChristmas UK retail charts\u003c/a\u003e. According to \u003ca href=\"https://www.ea.com/games/fifa/news/fifa-23-all-leagues-clubs-teams-list#:~:text=With%20more%20than%2019%2C000%20players,%2C%20Bundesliga%2C%20LaLiga%20Santander%2C%20CONMEBOL\"\u003eEA statistics\u003c/a\u003e, the game contains more than 700 teams with over 19,000 football players, playing in at least 30 football leagues. The data used in this project is taken from \u003ca href=\"https://www.kaggle.com/datasets/bryanb/fifa-player-stats-database\"\u003eKaggle\u003c/a\u003e. The objective of this project is to classify the players into various segments.\u003c/p\u003e","title":"FIFA23 Players Analysis: k-Means Clustering"},{"content":"This project is carried out by me and my classmate under an assignment for one of my courses at the University of Edinburgh. The project received an A grade; all the coding and visualizations along with a mini report are shared on my GitHub account.\nProject description\nLanguage: Python Working file: Jupyter notebook In 2018, former President of South Africa, Jacob Zuma, established a commission of enquiry in state capture, known as The Judicial Commission of Inquiry into Allegations of State Capture, Corruption and Fraud in the Public Sector including Organs of State, or simply the Zondo Commission. The commission collected one exabyte of evidence, and on 22 June 2022 released its final report. The reports are available publicly in this link.\nImage is taken from www.corruptionwatch.org.za\nThis project aims to analyze the contents of these reports to capture the names of publicly traded companies whose names are mentioned more frequently. The results would be beneficial for prudent investors who seek ethical investments opportunities in South Africa.\nData collection and cleaning Two datasets were used in this project. First dataset were created by converting the report .pdf files into .csv format. PyPDF2 module was used for transforming each pdf content into a text-formatted object, and Pandas was used to store those text-formatted objects into a dataframe. The final dataset of the texts were converted to .csv file and uploaded to Github, where the file\u0026rsquo;s raw format was used.\nimport pandas, PyPDF2 # storing all the pdf files in a list report_paths = [\u0026#39;OCR version - State Capture Commission Report Part 1 Vol I.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part II Vol II.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part III Vol I - Bosasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part III Vol II - Bosasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part III Vol III - Bosasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part III Vol IV - Bosasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part IV Vol I - NT,EOH,COJ,Alexkor.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part IV Vol II- FS.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part IV Vol III - Eskom.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part IV Vol IV - Eskom.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part V Vol I - SSA.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part V Vol II - SABC,Waterkloof,Prasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part VI Vol I - Estina,Vrede.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part VI Vol II - CR.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part VI Vol III - Flow of Funds.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part VI Vol IV - Recommendations.pdf\u0026#39;] # creating a dataframe file zondo_reports = pd.DataFrame(columns=[\u0026#39;report\u0026#39;, \u0026#39;text\u0026#39;]) i = 0 # iterating through the pdf list for path in report_paths: # converting each pdf content into a text object pdfFileObj = open(path, \u0026#39;rb\u0026#39;) print(\u0026#39;opened\u0026#39;, path) pdfReader = PyPDF2.PdfReader(pdfFileObj) text=\u0026#39;\u0026#39; for page in pdfReader.pages: text += page.extract_text() print(\u0026#39;extracted text for\u0026#39;, path) # adding the converted text objects into dataframe zondo_reports.loc[i] = [path, text] i+=1 pdfFileObj.close() print(\u0026#39;closed\u0026#39;, path) After obtaining the first dataset, we needed another dataset consisting of the companies that are publicly traded in South Africa. For this objective, we used Johannesburg Stock Exchange (JSE) portal to pick up the candidate companies. January 2021 dataset of the companies were downloaded from the JSE website in .xlsx format. The file then were re-formatted into .csv to be readable from GitHub. The .csv file was uploaded to my GitHub account, and its raw format was called using read_csv method of Pandas.\nAs of now, we have a dataset containing reports contents and a dataset containing the publicly traded companies in South Africa.\nData analysis To carry out the text analysis, nltk module was primarily used. The below code displays how different classes of that module were imported:\nimport nltk, string from nltk.tokenize import word_tokenize nltk.download(\u0026#39;punkt\u0026#39;) from nltk.corpus import stopwords nltk.download(\u0026#39;stopwords\u0026#39;) from nltk.probability import FreqDist Firstly, we used word_tokenize function to tokenize all the reports contents into a single variable. Although the best practice is to lowercase all the tokens, we intentionally left the words as they were, given the fact that we were looking for company names, which are proper nouns. Thus, leaving proper nouns as they were was a better choice for the analysis.\nTo get rid of unnecessary words, we created a variable consisting of stop words from nltk.corpus and punctuations from string module. Further, some additional junk words were added to be removed from the tokens.\n# remove some stops from the tokens junk_tokens = [\u0026#39;Mr\u0026#39;,\u0026#39;Ms\u0026#39;,\u0026#39;Dr\u0026#39;,\u0026#39;P\u0026#39;,\u0026#39;``\u0026#39;, \u0026#39;\\\u0026#39;s\u0026#39;,\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\\\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\\\u0026#39;\\\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\u0026#34;\u0026#39;,\u0026#39;\u0026#34;\u0026#39;,\u0026#39;................................\u0026#39;,\u0026#39;L\u0026#39;] removables = set(stopwords.words(\u0026#39;English\u0026#39;) + list(string.punctuation) + list(string.digits) + junk_tokens) filtered_tokens = [token for token in all_content_tokens if token not in removables] Numerous company names come with additional descriptive words attached to them such as \u0026lsquo;Holding\u0026rsquo;, \u0026lsquo;Corporation\u0026rsquo;, \u0026lsquo;Limited\u0026rsquo;, and etc. We created a function that checks the name for each company and leaves those additions out. The new names were added to the dataframe under a column name \u0026lsquo;search term\u0026rsquo;. Afterwards, finally, time came for searching for company names (using the search terms) inside the reports (texts that are stored in the first dataframe).\nFor companies with one search term, we just searched the tokens as they are single words. For the companies with two or three words in their names, we used bigrams and trigrams from nltk, respectively. An additional column was added to the dataframe indicating True if the company name is mentioned in the reports and False otherwise.\n# Bigrams and trigrams are created to search for two-word and three-word search terms individually. bigrams = list(nltk.bigrams(filtered_tokens)) trigrams = list(nltk.trigrams(filtered_tokens)) # Create new column that will store whether a company name is mentioned or not. listed_companies[\u0026#34;FoundInReport\u0026#34;] = \u0026#34;False\u0026#34; for ind in listed_companies.index: searchterm = listed_companies[\u0026#39;SearchTerm\u0026#39;][ind] if len(searchterm) == 1: if searchterm[0] in filtered_tokens: # search for one-word search terms print(\u0026#39;1 word company appeared in text:\u0026#39;, searchterm) listed_companies.at[ind,\u0026#39;FoundInReport\u0026#39;] = True if len(searchterm) == 2: if searchterm in bigrams: # search for two-word search terms print(\u0026#39;2 word company appeared in text:\u0026#39;, searchterm) listed_companies.at[ind,\u0026#39;FoundInReport\u0026#39;] = True if len(searchterm) == 3: if searchterm in trigrams: # search for three-word search terms print(\u0026#39;3 word company appeared in text:\u0026#39;, searchterm) listed_companies.at[ind,\u0026#39;FoundInReport\u0026#39;] = True Next, we used FreqDist function from nltk.probability to calculate how many times each company name is mentioned in all reports combined.\nData visualization and project results Matplotlib and seaborn libraries were used for the purpose of visualization. The following figures were drawn to illustrate the results.\nFigure 1: The frequency of company name mentions by sector Figure 1 shows how many times the names of companies in each sector is mentioned in the reports. Banking companies top the chart, by having 236 name mentions in total. It is followed by Mining, Software\u0026amp;Computing Services, and Media, by being mentioned 197, 110, and 76 times, respectively.\nFigure 2: Frequency of mentions for company Figure 2 clearly depicts the company names that were mentioned in the reports. Glencore Plc is the company with the highest quantity of mentions (190). Standard Bank Group Limited and Nedbank Group Limited are the main banks whose names are widely used in the reports. The following are the EOH Holding Limited and MultiChoice Group Limited.\nNote: It is important to note that the results of this analysis do not impose any kind of allegation against any of the companies mentioned above. As a matter of fact, the analysis focuses on the numerical counts rather than the context of the mentions. More clearly, it does not claim that the company names are mentioned in a negative manner. Thus, this project has only one primary purpose to be a guide for the investors. Additional in-depth research is up to the investors when they ponder of investing on those firms.\n","permalink":"http://localhost:1313/posts/test/","summary":"\u003cp\u003eThis project is carried out by me and my classmate under an assignment for one of my courses at the University of Edinburgh. The project received an A grade; all the coding and visualizations along with a mini report are shared on \u003ca href=\"https://github.com/mrzeynalli/Zondo-Reports-Analysis\"\u003emy GitHub account.\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cem\u003eProject description\u003c/em\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eLanguage:\u003c/em\u003e Python\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eWorking file:\u003c/em\u003e Jupyter notebook\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn 2018, former President of South Africa, Jacob Zuma, established a commission of enquiry in state capture, known as The Judicial Commission of Inquiry into Allegations of State Capture, Corruption and Fraud in the Public Sector including Organs of State, or simply the Zondo Commission. The commission collected one exabyte of evidence, and on 22 June 2022 released its final report. The reports are available publicly in this link.\u003c/p\u003e","title":"Zondo Reports — Text Analysis"}]