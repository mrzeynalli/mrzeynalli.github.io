[{"content":"Project description\nLanguage: Python, HTML Libraries: folium, pandas, numpy, requests, json IDE: Microsoft Visual Studio Code, Jupyter Notebook Project type: Data analytics, Web scraping, Map formation, API Keywords: Scotland, schools, deprivation, map Photo by ali elliott on Unsplash\nPostcodes API The available contact information for the schools solely consists of their postcodes and seedcodes. Consequently, in order to accurately locate each school on a map, I required latitude and longitude values for each establishment. In my search for geolocation information, I discovered an open platform called Postcodes IO. This platform offers a free API that allows access to the server and retrieval of geolocation information for every UK postcode. The collected data was presented in a JSON format. To obtain a comprehensive dataset from postcodes.io, I utilized various endpoints of the API, including the following:\nhttps://api.postcodes.io/postcodes/QUERY — this endpoint allowed me to query a single postcode and get its geo-information. https://api.postcodes.io/postcodes/QUERY/validate — this endpoint allowed me to check the validity of the query postcode. https://api.postcodes.io/postcodes/terminated_postcodes/QUERY — this endpoint allowed me to check if the postcode has already been terminated. https://api.postcodes.io/postcodes/BULK_QUERY — for the very first endpoint, the API also allowed the query of postcodes in a bulk. So, it is possible to request the information of 100 postcodes in a single request call. Upon completion of the final debugging and error handling process, the Postcodes API caller class object was ultimately concluded and subsequently archived in the designated directory for utilization in the map building application. The object is capable of providing four distinct information points pertaining to each queried postcode, namely: latitude, longitude, city, and zone. The codes necessary for constructing the API caller are available in my GitHub repository and can be accessed accordingly.\nBuilding the API Caller Object # import necessary libraries import requests # create an object to call the API class PostcodeApi: # create initialiser def __init__(self): self.api = \u0026#39;https://api.postcodes.io\u0026#39; # create a function to check the validity of the input postcode def check_validity(self, p): val = self.api + \u0026#39;/postcodes/\u0026#39; + p + \u0026#39;/validate\u0026#39; # formulate validity endpoint result = requests.get(val).json()[\u0026#39;result\u0026#39;] # get the request result if result == False: # check if the entered postcode is valid print(f\u0026#39;Non-valid post code: {p}\u0026#39;) return None else: return True # create a function to check if the postcode is terminated def check_termination(self, p): ter = self.api + \u0026#39;/terminated_postcodes/\u0026#39; + p # formulate termination endpoint if requests.get(ter).status_code == 200: # check if the entered postcode is terminated print(f\u0026#39;The postcode {p} is terminated.\u0026#39;) return None else: return True ### THE REST OF THE CODES FOLLOW THE NEXT In order to construct the API object, I exclusively utilize the requests library, as it suffices to dispatch requests to the endpoints. Following the definition of the object’s initializer, I proceeded to establish two functions, namely check_validity() and check_termination(), which verify the accuracy of the inputted postcode and its termination status. Both functions transmit the postcode to the pertinent endpoints and assess the call status prior to forwarding it to the postcode endpoint. Subsequently, I developed functions to retrieve information for either a singular postcode or a multitude of postcodes.\n# create a function to request the postcode info def get_pos_info(self, pos): if self.check_termination(pos): # apply validity check if self.check_validity(pos): # apply termination check q = self.api + \u0026#39;/postcodes/\u0026#39; + pos # formulate the postcode query endpoint result = requests.get(q).json()[\u0026#39;result\u0026#39;] # collect the result in a JSON format lat = result[\u0026#39;latitude\u0026#39;] # latitude lon = result[\u0026#39;longitude\u0026#39;] # longitude city = result[\u0026#39;admin_district\u0026#39;] # city zone = result[\u0026#39;parliamentary_constituency\u0026#39;] # zone return {\u0026#39;loc\u0026#39; : [lat,lon], \u0026#39;city\u0026#39; : city, \u0026#39;zone\u0026#39; : zone} # return the findings # create a function to request the bulk postcodes info def get_bulk_pos_info(self, pos_bulk): # define the URL url = self.api + \u0026#39;/postcodes/\u0026#39; # define the JSON payload data = {\u0026#34;postcodes\u0026#34;: pos_bulk} # set the headers headers = {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;} # send the POST request response = requests.post(url, json=data, headers=headers) # check the response if response.status_code == 200: # the request was successful, and you can parse the response JSON result = response.json()[\u0026#39;result\u0026#39;] # convert the result into a dictionary format result_items = { \u0026#39;locs\u0026#39; : [ (r[\u0026#39;result\u0026#39;][\u0026#39;latitude\u0026#39;], r[\u0026#39;result\u0026#39;][\u0026#39;longitude\u0026#39;]) if r[\u0026#39;result\u0026#39;] != None else None for r in result], \u0026#39;cities\u0026#39; : [r[\u0026#39;result\u0026#39;][\u0026#39;admin_district\u0026#39;] if r[\u0026#39;result\u0026#39;] != None else None for r in result], \u0026#39;zones\u0026#39; : [r[\u0026#39;result\u0026#39;][\u0026#39;parliamentary_constituency\u0026#39;] if r[\u0026#39;result\u0026#39;] != None else None for r in result] } return result_items else: # handle if error is encountered print(f\u0026#34;Error: {response.status_code} - {response.text}\u0026#34;) return None The get_pos_info() function is designed to retrieve information for a single postcode, following a validation process. It returns the latitude, longitude, city, and zone details in a dictionary format. On the other hand, the get_bulk_pos_info() function offers a more advanced functionality by accepting multiple postcodes in bulk. Notably, this function utilizes a different approach as it posts the input bulk of postcodes to the endpoint, rather than making a request to obtain the information. The postcodes are sent in a dictionary format with headers {\u0026quot;Content-Type\u0026quot;: \u0026quot;application/json\u0026quot;}. The returned value is a list of information for each entered postcode. Further guidance on sending bulk requests can be found in the postcodes.io documentation.\nGetting the Postcodes Data Considering the existence of more than 2,000 schools and the limitation of the bulk request to accept only 100 postcodes at a time, it became necessary for me to iterate through the schools in batches of 100. The subsequent code effectively manages this task. Prior to this, I had prepared a dictionary named \u0026ldquo;sch_loc_info\u0026rdquo; to accommodate the additional information that would be collected.\n# create a dictionary to store the collected data sch_loc_info = {\u0026#39;locs\u0026#39; : [], \u0026#39;cities\u0026#39; : [], \u0026#39;zones\u0026#39; : []} # NOTE: When sending bulk postcodes, the API gate can only take 100 at a time. So, we request info 100 by 100 for i in range(100,len(postcodes),100): sch_loc_info_100 = PostcodeApi().get_bulk_pos_info(postcodes[i-100:i]) # request the info for each 100 call sch_loc_info[\u0026#39;locs\u0026#39;] += sch_loc_info_100[\u0026#39;locs\u0026#39;] # store the location info sch_loc_info[\u0026#39;cities\u0026#39;] += sch_loc_info_100[\u0026#39;cities\u0026#39;] # store the cities sch_loc_info[\u0026#39;zones\u0026#39;] += sch_loc_info_100[\u0026#39;zones\u0026#39;] # store the zones if (len(postcodes) - i) \u0026lt; 100: # include the final call which would have less than 100 postcodes. sch_loc_info_less = PostcodeApi().get_bulk_pos_info(postcodes[i:len(postcodes)]) sch_loc_info[\u0026#39;locs\u0026#39;] += sch_loc_info_less[\u0026#39;locs\u0026#39;] sch_loc_info[\u0026#39;cities\u0026#39;] += sch_loc_info_less[\u0026#39;cities\u0026#39;] sch_loc_info[\u0026#39;zones\u0026#39;] += sch_loc_info_less[\u0026#39;zones\u0026#39;] Building the Map The open-source folium library from Python was utilized to construct the map, with HTML being employed to enhance its features. The codes utilized to create the map are presented below and are accessible in the project’s GitHub repository. The map was generated using the code provided and was saved in an HTML format within a folder named ‘map’ in the present working directory.\n# Create a map centered at Edinburgh, the capital of Scotland m = folium.Map(location=[55.941457, -3.205744], zoom_start=6.5) # Create a custom color scale from light to dark blue colors = { 1: \u0026#39;#08306b\u0026#39;, # Dark blue (most deprived) 2: \u0026#39;#08519c\u0026#39;, 3: \u0026#39;#3182bd\u0026#39;, 4: \u0026#39;#63b7f4\u0026#39;, 5: \u0026#39;#a6e1fa\u0026#39; # Light blue (least deprived) } # length of the schools l = len(sch_df) # Create circles and digits for each data point for i in range(l): school = sch_df[\u0026#39;School Name_x\u0026#39;].iloc[i] # school name pos = sch_df[\u0026#39;Post Code\u0026#39;].iloc[i] # postcode loc = sch_df[\u0026#39;locs\u0026#39;].iloc[i] # location city = sch_df[\u0026#39;cities\u0026#39;].iloc[i] # city zone = sch_df[\u0026#39;zones\u0026#39;].iloc[i] # zone pupils = sch_df[\u0026#39;Total pupils\u0026#39;].iloc[i] # total pupils type = sch_df[\u0026#39;School Type\u0026#39;].iloc[i] # type of the school: secondary, primary, or special mag = dep_rates.get(pos, 3) # get the deprivation score as magnitude; 3 if the postcode is not assigned a score # add circle markers pointing each school folium.CircleMarker( location=loc, radius=(pupils/100 if pupils != 0 else 1), # radius equivalent to the total pupils count in each school color=colors[mag], fill=True, fill_opacity=0.8, ).add_to(m) # create an HTML pop-up for each school popup_html = f\u0026#34;\u0026#34;\u0026#34; \u0026lt;h3\u0026gt;{school}\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Type:\u0026lt;/strong\u0026gt; {type}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Local Authority:\u0026lt;/strong\u0026gt; {city}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Zone:\u0026lt;/strong\u0026gt; {zone}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Pupils:\u0026lt;/strong\u0026gt; {pupils if pupils != 0 else \u0026#39;N/A\u0026#39;}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Deprivation:\u0026lt;/strong\u0026gt; {mag}\u0026lt;/p\u0026gt;\u0026#34;\u0026#34;\u0026#34; folium.Marker( location=loc, popup=folium.Popup(popup_html, max_width=150), icon=folium.DivIcon(html=f\u0026#39;\u0026lt;div style=\u0026#34;width: 0px; height: 0px;\u0026#34;\u0026gt;\u0026lt;/div\u0026gt;\u0026#39;), ).add_to(m) # Create a custom HTML legend legend_html = \u0026#34;\u0026#34;\u0026#34; \u0026lt;div style=\u0026#34;position: fixed; top: 10px; right: 10px; background-color: white; padding: 10px; border: 2px solid black; z-index: 1000;\u0026#34;\u0026gt; \u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Legend\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span style=\u0026#34;color: black;\u0026#34;\u0026gt;\u0026lt;span style=\u0026#34;background-color: #08306b; width: 20px; height: 20px; display: inline-block;\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; 1 - Most Deprived\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span style=\u0026#34;color: black;\u0026#34;\u0026gt;\u0026lt;span style=\u0026#34;background-color: #08519c; width: 20px; height: 20px; display: inline-block;\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; 2\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span style=\u0026#34;color: black;\u0026#34;\u0026gt;\u0026lt;span style=\u0026#34;background-color: #3182bd; width: 20px; height: 20px; display: inline-block;\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; 3\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span style=\u0026#34;color: black;\u0026#34;\u0026gt;\u0026lt;span style=\u0026#34;background-color: #63b7f4; width: 20px; height: 20px; display: inline-block;\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; 4\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span style=\u0026#34;color: black;\u0026#34;\u0026gt;\u0026lt;span style=\u0026#34;background-color: #a6e1fa; width: 20px; height: 20px; display: inline-block;\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; 5 - Least Deprived\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026#34;\u0026#34;\u0026#34; m.get_root().html.add_child(folium.Element(legend_html)) # Create the folder to save the map cwd = os.getcwd() folder = os.path.join(cwd,\u0026#39;map\u0026#39;) os.makedirs(folder, exist_ok=True) # save the map as an HTML file m.save(os.path.join(folder, \u0026#39;scottish_schools_map.html\u0026#39;)) Upon clicking on a school displayed on the map, an HTML-generated pop-up menu will appear, providing relevant information about the school. Furthermore, an HTML-based legend has been created to display the range of deprivation. The code will save the file as \u0026lsquo;scottish_schools_map.html\u0026rsquo;. A screenshot of the map is presented in Image 1. The radius of the circles on the map is determined by the number of pupils in each school, with larger circles representing schools with a greater number of pupils. The intensity of the blue color indicates the level of deprivation, with darker markers indicating higher levels of deprivation in the area.\nImage 1: Map of Scotland locating all schools (screenshot from the HTML-based app)\nThe image presented below depicts the appearance of a pop-up menu upon clicking on a school icon (Image 2). The menu exhibits pertinent information such as the school type, local authority, zone, pupils count, and deprivation score.\nImage 2: A pop-up information menu of Leith Academy (screenshot from the HTML-based app)\nEnd The objective of this project was to construct a map that would visually represent schools in Scotland, taking into account the number of students and deprivation score. The map was developed using the folium module. The codes and map are available for unrestricted use without my authorization. Please leave a comment if you have any inquiries or recommendations. The map could be enhanced with additional HTML, and possibly Javascript integration.\nFor information on implementing k-Means clustering to classify school data, please refer to my other article via this link.\nFor potential collaborations, please contact me at ezeynalli@hotmail.com.\n","permalink":"http://localhost:1313/posts/scottish_schools_map/","summary":"\u003cp\u003e\u003cstrong\u003eProject description\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eLanguage:\u003c/em\u003e Python, HTML\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eLibraries:\u003c/em\u003e folium, pandas, numpy, requests, json\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eIDE:\u003c/em\u003e Microsoft Visual Studio Code, Jupyter Notebook\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eProject type:\u003c/em\u003e Data analytics, Web scraping, Map formation, API\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eKeywords:\u003c/em\u003e Scotland, schools, deprivation, map\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cimg alt=\"Photo by ali elliott on Unsplash\" loading=\"lazy\" src=\"/images/scottish_state_schools_map/1.webp\"\u003e\u003cbr\u003e\n\u003cem\u003ePhoto by ali elliott on Unsplash\u003c/em\u003e\u003c/p\u003e\n\u003ch2 id=\"postcodes-api\"\u003ePostcodes API\u003c/h2\u003e\n\u003cp\u003eThe available contact information for the schools solely consists of their postcodes and seedcodes. Consequently, in order to accurately locate each school on a map, I required latitude and longitude values for each establishment. In my search for geolocation information, I discovered an open platform called \u003ca href=\"https://postcodes.io/\"\u003ePostcodes IO\u003c/a\u003e. This platform offers a free API that allows access to the server and retrieval of geolocation information for every UK postcode. The collected data was presented in a JSON format. To obtain a comprehensive dataset from postcodes.io, I utilized various endpoints of the API, including the following:\u003c/p\u003e","title":"Analysis of State Schools in Scotland: Map of Scotland with Schools"},{"content":"The objective of this project is to conduct a comprehensive analysis of Scottish schools in order to derive valuable insights. The analysis includes clustering and descriptive analysis, taking into account factors such as deprivation rate and total number of pupils. Additionally, an interactive map of Scotland has been developed to visually represent the location of each school. The clustering model has categorized local authorities into three distinct clusters based on average pupil count and deprivation score. The map highlights areas predominantly occupied by schools facing high levels of deprivation. The aim is to assist charities or non-governmental organizations (NGOs) involved in projects supporting these school pupils. All data used in this analysis is publicly available and sourced from the Scottish Government website. The Postcodes.io API was utilized to gather latitude and longitude coordinates for each school based on their respective postcodes. The codes for clustering, visualizations, map generation, and API application can be found in my GitHub repository. You are welcome to access and utilize the repository\u0026rsquo;s contents for personal or commercial purposes without seeking my consent. I hope you find this information enlightening and enjoyable to read.\nProject description\nLanguage: Python Libraries: sklearn, pandas, numpy, matplotlib, seaborn, os IDE: Microsoft Visual Studio Code, Jupyter Notebook Project type: Machine learning, unsupervised learning, k-means clustering, data analytics, data visualisation Photo by Adam Wilson on Unsplash\nThe educational institutions in Scotland hold significant influence over the country\u0026rsquo;s economy. Scotland\u0026rsquo;s socio-demographic governmental structure places substantial emphasis on investing in the education of its population, as evidenced by the provision of free meals in all state schools. The economic status of local areas is largely determined by the location and prestige of state schools, with real estate prices being particularly dependent on this factor. Therefore, the level of deprivation in schools is a crucial determinant of areas that require further attention from either the government or non-governmental organizations. In this project, my objective is to assist concerned institutions in gaining a better understanding of the distribution of schools and the local authorities to which they are assigned.\nI have undertaken two projects for Scottish schools, one of which involves the implementation of K-Means Clustering. The other project involves the creation of a map of Scotland that displays the schools, which can be accessed via this link or my Medium page (soon).\nSchools Datasets Description All data pertaining to the schools for this project has been sourced from the official statistics website of the Scottish Government. The consolidation of information regarding Scottish schools is based on three datasets:\nPostcode_deprivation: This dataset encompasses postcodes in Scotland along with their corresponding deprivation scores. Both pieces of information are utilized in this project. The dataset can be officially accessed from the following link. Scottish_schools_contact: This dataset comprises contact information for Scottish schools, including their respective postcodes. The project utilizes the postcode and seedcode associated with each school. The dataset can be officially accessed from the following link. Scottish_schools_stats: This dataset contains demographic statistics for Scottish schools. It has been formulated by merging various datasets. The data can be officially accessed from the following link. Analysis Descriptive analysis Initially, a descriptive analysis was conducted on the collected information from schools. Subsequently, the datasets were merged and null values were dropped, resulting in a conclusion of 2,431 schools (out of 2,458 state schools in Scotland) with a total of 692,729 pupils. The average number of pupils per school was found to be 284.9, with a standard deviation of 309.7. The school with the highest number of pupils had 2,226, while the lowest school had no pupils at present. In terms of deprivation quintile, the average score was 2.8, with a standard deviation of 1.2. The number of schools falling under each deprivation quintile score is provided below:\n1 quintile — 448 school 2 quintile — 507 school 3 quintile — 634 school 4 quintile — 557 school 5 quintile — 285 school Figure 1: Total number of pupils and schools per school type\nThe data presented in Figure 1 illustrates the distribution of schools and pupils across various school types. It is evident that primary schools hold the majority in terms of both school count and pupil enrollment. Notably, the number of secondary schools is disproportionately low in comparison to the significant number of pupils. Special schools represent a minority within the overall distribution.\nFigure 2: TOP10 Scottish local authorities with the highest number of pupils and schools\nFigure 2 displays the pupil and school statistics for the top 10 local authorities. Notably, Glasgow city, Edinburgh city, and Fife emerge as the frontrunners in terms of pupil population, whereas Highland, Glasgow, and Aberdeenshire dominate the representation of schools. A noteworthy observation is the significant concentration of pupils in Edinburgh, the capital city of Scotland, despite the relatively lower number of schools. Conversely, Highland boasts the highest number of schools, despite having a comparatively smaller pupil population.\nUnsupervised Learning: Clustering Following the descriptive analytics, a discernible pattern emerged, enabling the categorization of local authorities based on their average deprivation score and average pupil count. To accomplish this, I proceeded to develop a data clustering algorithm utilizing unsupervised machine learning models. After careful consideration, I selected the k-Means clustering method due to its extensive applicability and widespread usage. Prior to implementing the clustering technique, I performed an initial data transformation to ensure optimal results.\nPreprocessing before Clustering Due to the significant disparity in numerical values observed, with deprivation scores ranging from 1 to 5 and total pupil counts ranging from 0 to over 2000, I made the decision to normalize these values in order to enhance the efficiency of the clustering process. To accomplish this, I employed the StandardScaler object from the sklearn module.\n# Import standard scaler to z-score normalize the data from sklearn.preprocessing import StandardScaler # specify the numerical features numeric_columns = [\u0026#39;Pupils\u0026#39;, \u0026#39;DeprivationScore\u0026#39;] # Create a scaler object scaler = StandardScaler() # Scale the numeric columns scaled_values = scaler.fit_transform(df[numeric_columns]) # Sonvert the numpy array of scaled values into a dataframe scaled_values = pd.DataFrame(scaled_values, columns = numeric_columns) Finding k: the optimal number of clusters When considering k-Means clustering, a crucial step is determining the number of clusters (k), which must be predetermined. However, identifying the optimal k is not a straightforward process and requires both domain knowledge and technical expertise. Given the lack of personal expertise in the school industry, I chose to rely on my technical knowledge. To ensure the accuracy of k, I employed two different methods to determine the best value. To achieve this objective, I utilized the Within-Cluster Sum of Squares (WCSS) and Average Silhouette Method.\nWithin-Cluster Sum of Squares method The WCSS method assesses the variations within each cluster by summing the squared differences between values. By testing different k values, it compares the differences in WCSS scores and selects the value at which the subsequent scores stabilize. The code implementation for this method is as follows:\n# Create a list to store WCSS values wcss = [] # Iterate in a range from 2 to 10, inclusive for k in range(2, 11): km = KMeans(n_clusters = k, n_init = 25, random_state = 1234) # Create a cluster object for each k km.fit(scaled_values) # Fit the scaled data wcss.append(km.inertia_) # Add the inertia score to wcss list # Convert the wcss list into a pandas series object wcss_series = pd.Series(wcss, index = range(2, 11)) # Draw a line chart showing the inertia score, or WCSS, for each k iterated plt.figure(figsize=(8, 6)) ax = sns.lineplot(y = wcss_series, x = wcss_series.index) ax = sns.scatterplot(y = wcss_series, x = wcss_series.index, s = 150) ax = ax.set(xlabel = \u0026#39;Number of Clusters (k)\u0026#39;, ylabel = \u0026#39;Within Cluster Sum of Squares (WCSS)\u0026#39;) The provided codes generate Figure 3, which allows for the observation that the scores exhibit minimal variation beyond the value of 3. Consequently, it can be inferred that 3 is the optimal k, as determined through the utilization of the WCSS method.\nFigure 3: Within-Cluster Sum of Squares method results\nHowever, this cannot be the sole basis for formulating three clusters. Therefore, it is necessary to run the Silhouette method as well.\nSilhouette method The Silhouette method measures the similarity of a data point within its own cluster in comparison to other clusters. For each data point, it calculates its distance from its local points (a) and neighboring points (b). It then applies the formula (b-a)/max(a,b) to calculate the Silhouette score. The average Silhouette scores of all the points within one cluster determine the final score. A higher Silhouette value indicates better locality and thus a better cluster count. The code implementation for this method is as follows:\n# Import silhouette_score function from sklearn.metrics import silhouette_score # Create a list to store silhouette values silhouette = [] # Iterate in a range from 2 to 10, inclusive for k in range(2, 11): km = KMeans(n_clusters = k, n_init = 25, random_state = 1234) # Create a cluster object for each k km.fit(scaled_values) # Fit the scaled data silhouette.append(silhouette_score(scaled_values, km.labels_)) # Add the silhouette score to silhouette list # Convert the silhouette list into a pandas series object silhouette_series = pd.Series(silhouette, index = range(2, 11)) # Draw a line chart showing the average silhouette score for each k iterated plt.figure(figsize=(8, 6)) ax = sns.lineplot(y = silhouette_series, x = silhouette_series.index, color=\u0026#39;green\u0026#39;) ax = sns.scatterplot(y = silhouette_series, x = silhouette_series.index, s = 150, color=\u0026#39;green\u0026#39;) ax = ax.set(xlabel = \u0026#39;Number of Clusters (k)\u0026#39;, ylabel = \u0026#39;Average Silhouette Score\u0026#39;) Figure 4 below represents the outcome of the aforementioned codes, displaying the results of Silhouette testing. The chart unequivocally indicates that 3 possesses the highest average silhouette score. Consequently, this test execution ultimately determines 3 as the optimal value for k, as well.\nFigure 4: Silhouette method results\nAs both tests have indicated that the optimal value for k is 3, we can proceed with confidence to implement clustering with 3 clusters. The subsequent sub-section will construct the k-Means clustering and elucidate its outcomes.\nk-Means clustering In order to construct the model, the KMeans object from the sklearn library was utilized. The implementation of this object can be achieved through the following code:\n# Create kmeans object km = KMeans(n_clusters = 3, n_init = 25, random_state = 1234) # Fit the scaled values km.fit(scaled_values) The parameter \u0026rsquo;n_init\u0026rsquo; determines the number of times the algorithm will be executed with different initializations of cluster centroids. Increasing this value is recommended in order to achieve better clustering results. After conducting multiple tests, it was determined that a value of 25 is sufficient for this project. The \u0026lsquo;random_state\u0026rsquo; parameter is used to control the randomness or randomness seed for various operations involving randomness. It is advisable to set it to 1234 for most machine learning models. Among the three constructed clusters, Cluster 0 consists of 5 local authorities, Cluster 1 consists of 15 local authorities, and Cluster 2 consists of 13 local authorities.\nTable 1: Clustering results\ncluster Pupils DeprivationScore 0 438.29 3.50 1 331.48 2.39 2 175.87 3.31 The clustering results can be found in Table 1.\nCluster 0\nThis is the first cluster and is characterized by a relatively high number of pupils and a higher deprivation score. In other words, these are localities with a low level of deprivation but a large number of pupils. Despite the high number of pupils, these localities are relatively less deprived.\nCluster 1\nThis group of localities also has a large number of pupils, but the average deprivation score is the lowest among the three clusters. This indicates that the local authorities in this category require the most attention.\nCluster 2\nThe localities in this group have a lower number of pupils but a fair deprivation score. This group likely requires the least attention, as they are not highly deprived and the pupil count is not significant.\nThe scatter plot in Figure 5 illustrates the assignment of each locality to its respective cluster.\nZoom image will be displayed\nFigure 5: Scottish local authorities clustering\nThe complete list of localities in each cluster is provided below:\nCluster 0: Aberdeen City, East Dunbartonshire, East Lothian, East Renfrewshire, Edinburgh City Cluster 1: Clackmannanshire, Dundee City, East Ayrshire, Falkirk, Fife, Glasgow City, Inverclyde, Midlothian, North Ayrshire, North Lanarkshire, Renfrewshire, South Ayrshire, South Lanarkshire, West Dunbartonshire, West Lothian Cluster 2: Aberdeenshire, Angus, Argyll \u0026amp; Bute, Dumfries \u0026amp; Galloway, Grant aided, Highland, Moray, Na h-Eileanan Siar, Orkney Islands, Perth \u0026amp; Kinross, Scottish Borders, Shetland Islands, Stirling Conclusion I had two primary objectives in undertaking this project:\nPersonal development — My aim was to showcase my abilities and enhance my career prospects by implementing similar projects and gaining valuable experience. Educational exploration — As an international citizen of Scotland, I was keen to deepen my understanding of the country\u0026rsquo;s educational system. Additionally, I am actively involved in a charity organization that carries out projects for Scottish schools. If you found this project insightful and beneficial, I encourage you to show your appreciation by clapping and sharing it with your peers. Furthermore, feel free to utilize the project for your personal and professional purposes without seeking my consent, as I am a strong advocate of open-source projects and actively contribute to the community.\nShould you have any questions, concerns, or suggestions, please do not hesitate to comment below or contact me via email at ezeynalli@hotmail.com.\n","permalink":"http://localhost:1313/posts/scottish_schools_clustering/","summary":"\u003cp\u003eThe objective of this project is to conduct a comprehensive analysis of Scottish schools in order to derive valuable insights. The analysis includes clustering and descriptive analysis, taking into account factors such as deprivation rate and total number of pupils. Additionally, an interactive map of Scotland has been developed to visually represent the location of each school. The clustering model has categorized local authorities into three distinct clusters based on average pupil count and deprivation score. The map highlights areas predominantly occupied by schools facing high levels of deprivation. The aim is to assist charities or non-governmental organizations (NGOs) involved in projects supporting these school pupils. All data used in this analysis is publicly available and sourced from the Scottish Government website. The Postcodes.io API was utilized to gather latitude and longitude coordinates for each school based on their respective postcodes. The codes for clustering, visualizations, map generation, and API application can be found in \u003ca href=\"https://github.com/mrzeynalli/scotland_schools.git\"\u003emy GitHub repository\u003c/a\u003e. You are welcome to access and utilize the repository\u0026rsquo;s contents for personal or commercial purposes without seeking my consent. I hope you find this information enlightening and enjoyable to read.\u003c/p\u003e","title":"Analysis of State Schools in Scotland: K-Means Clustering by Deprivation Rate and Pupils Quantity"},{"content":"In this article, I’m going to explain the concept of a web crawler, how search engines work, and guide you in building a simple Crawler bot. All the code I’ve written for this can be found on my GitHub repository, and feel free to use, modify, or replicate it for personal or commercial purposes without needing my consent.\nProject description\nLanguage: Python Libraries: request, bs4, regex, os, json IDE: Microsoft Visual Studio Code Project type: Web crawling/scraping Photo by Timothy Dykes on Unsplash\nAs the name suggests, a web crawler is an application that explores the web like a spider, gathering the desired information. Well-known search engines like Google, Bing, and Yahoo have incredibly fast crawlers that navigate the internet in a matter of seconds (although they don’t crawl the entire web all the time, they minimize the number of web pages to consider using indexing). The good news is, you too can create your own web crawler using Python, and it only requires around 100 lines of code (actually, it’s exactly 100 lines!).\nWeb Crawler First, let’s break down the process into smaller steps to understand how to build the crawling bot (“First Principles”). The steps the bot needs to follow are as follows:\nAccess the webpage Request its content (HTML source code) Analyze the content and gather internal links Extract text information from each link Repeat until a pre-defined threshold is reached To perform these steps, we need to install/import specific Python libraries. For the first two steps, we will use the requests library, which allows us to send a request to a webpage to get its content. Next, we will use the bs4 library to help us understand the collected content and parse it for extracting the desired information, which in this case is the internal links and their text content. The bot will also follow these steps for the collected links. Now, let’s delve into the technical details of building the bot.\nWebCrawler Class We need to create a class named “WebCrawler” and define its parameters:\n# build the web crawler object class WebCrawler: # create three variables: start_url, max_depth, list of visited urls def __init__(self, start_url, max_depth=2): self.start_url = start_url self.max_depth = max_depth self.visited = set() The class requires two input parameters: start_url and max_depth, along with an initial parameter visited.\nstart_url — This is the main webpage we want to crawl (format: https://example.co.uk)/). max_depth — This indicates the depth of the crawling. Setting it to 1 will make our bot only follow the links collected from the starting URL. When set to 2, it will follow the links within those internal (from the initial URL) pages. The time complexity increases exponentially as the max_depth value increases. visited — This variable is a set that lists the URLs that have already been visited. It prevents the bot from revisiting the same link multiple times. Note that this variable is a set object rather than a list, which avoids adding duplicate entries (URLs). Checking URL Success Inside the WebCrawler class, we need to create another function to ensure that our request to the starting page is successful. This can be determined by checking the status code of the HTML request. Please note that a status code of 200 indicates a successful request.\n# create a function to make sure that the primary url is valid def is_successful(self): try: response = requests.get(self.start_url, timeout=20) # request the page info response.raise_for_status() # raises exception when not a 2xx response if response.status_code == 200: # check if the status code is 200, a.k.a successful return True else: # if not, print the error with the status code print(f\u0026#34;The crawling could not being becasue of unsuccessful request with the status code of {response.status_code}.\u0026#34;) except requests.HTTPError as e: # if HTTPS Error occured, print the error message print(f\u0026#34;HTTP Error occurred: {e}\u0026#34;) except Exception as e: # if any other error occured, print the error message print(f\u0026#34;An error occurred: {e}\u0026#34;) With this debugging approach, the crawling process will only be executed if the request is successful. Otherwise, it will either return the status code of an unsuccessful request or an HTTP or other encountered error. The timeout parameter determines the amount of time, in seconds, that the request attempt should wait before raising an error. It is different from the time.sleep(10) function. In this case, if the request is successful, the information will be retrieved immediately. However, if an error occurs, it will wait for 10 seconds before concluding an error. This is an important parameter to set up as sometimes the server may take a few seconds to respond.\nProcessing Pages The next step is to create a function that will enter a webpage and gather the links present as well as the text.\n# create a function to get the links def process_page(self, url, depth): # apply depth threshold if depth \u0026gt; self.max_depth or url in self.visited: return set(), \u0026#39;\u0026#39; # return empty set and string self.visited.add(url) # add the visited url to the set links = set() # create a set to store the collected links content = \u0026#39;\u0026#39; # create a variable to store the content extracted try: r = requests.get(url, timeout=10) # request the content of a url r.raise_for_status() # check if the request status is successful soup = BeautifulSoup(r.text, \u0026#39;html.parser\u0026#39;) # parse the content of the collected HTML # Extract the links anchors = soup.find_all(\u0026#39;a\u0026#39;) # find all the anchors for anchor in anchors: # merge the anchor with the starting url link = requests.compat.urljoin(url, anchor.get(\u0026#39;href\u0026#39;)) # get the link and join it with the starting url links.add(link) # add the link to the previously created set # Extract the content from the url content = \u0026#39; \u0026#39;.join([par.text for par in soup.find_all(\u0026#39;p\u0026#39;)]) # get all the text content = re.sub(r\u0026#39;[\\n\\r\\t]\u0026#39;, \u0026#39;\u0026#39;, content) # remove the sequence characters except requests.RequestException: # if the request encounters an error, pass pass return links, content # return the set of the collected links and the contet of the current url The function called process_page is inside the WebCrawler class and is responsible for collecting the link extensions found on a page, combining them with the primary URL, and extracting the text from each URL. Initially, it checks if the threshold (max depth, in this case) has been reached. If the depth is still below the predetermined limit, the crawling process continues. First, the bot adds the starting URL to the visited set and creates another set to collect the links found on that page. It is important to note that the links on a page do not follow a URL structure but are presented as link extensions (explained below). Therefore, it is crucial to merge them before appending them as final links.\nlink or URL — it begins with an HTTPS code and contains the complete reference structure (examples: https://example.co.uk, https://example.co.uk/contact) link extension — this refers only to the extension of the primary URL, essentially the part of the URL that comes after the “/” character (examples: /home, /contact, /careers) The function then utilizes the BeautifulSoup object from the bs4 library to parse the HTML source code of the webpage and scrape the page content. This content includes everything present on the page in text format, including the URL extensions. The bot searches for anchor elements within the soup by looking for ‘a’ tags, which contain the hyperlinks to other pages. After collecting the anchors, it iterates through each one to search for the ‘href’ tag, which contains the actual links. The compat.urljoin method from the request library is used to join the link with the URL and add the final URL to the set of links. Next, it looks for the ‘p’ tags, which contain plain text units, and joins them together within a particular page, storing the content in the content variable. To remove any escape sequences such as “/n”, “/r”, and “/t” (representing new line, return, and tab characters) and retain only the plain text, the sub method from the regex module (imported as re) is used to replace these sequences with an empty string. Finally, this process is debugged to handle any potential errors with internal links, in addition to the starting URL. Ultimately, this function returns the collected links and the text content of the requested URL.\nCrawling the Webpage The scraping function has been implemented, and now it’s time to build the crawling function. This function applies the scraping process to each of the links collected from a URL. Here is the code for this function:\n# crawl the web within the depth determined def crawl(self): if self.is_successful(): # check if the requesting the starting url info is valid to continue crawling urls_content = {} # create a dictionary to store the links as keys and contents as values urls_to_crawl = {self.start_url} # start crawling from the initial url # crawl the web within the depth determined for depth in range(self.max_depth + 1): new_urls = set() # create a set to store the internal new urls for url in urls_to_crawl: # crawl through the urls if url not in self.visited: # check and make sure that the url is not crawled before links, content = self.process_page(url, depth) # return the links and content of the crawled url urls_content[url] = content # add the url as a key and content as a value to the disctionary created previously new_urls.update(links) # add the internal links to the previously created set urls_to_crawl = new_urls # change the urls to crawl list to crawl through the internal links # create a folder to store the crawled websites current_dir = os.getcwd() # get the current working directory folder_dir = os.path.join(current_dir,\u0026#39;crawled_websites\u0026#39;) # create a folder inside the current directory if not os.isdir(folder_dir): # check if the folder already exists os.makedirs(folder_dir) # if not, create the folder directory filename = re.sub(r\u0026#39;\\W+\u0026#39;, \u0026#39;_\u0026#39;, self.start_url) + \u0026#39;_crawling_results.json\u0026#39; # format the filename to modify unsupported characters # save the results as a json file in the local directory with open(os.path.join(folder_dir,filename), \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as file: json.dump(urls_content, file, ensure_ascii=False, indent=10) # ensure to keep the unicode characters and indent to make it more readable return urls_content # return the disctionary storing all urls and their content The crawling process begins by checking the URL’s validity using the previously created function. If the check is successful, the function creates a dictionary to store the links along with their respective text content. It then iterates through the range of the maximum depth set in the object. For each depth, it creates a set to store the collected links to be iterated through subsequently. After confirming that the URL has not been visited before, it enters the current URL and uses the process_page function to extract all the internal links and text content. The new_urls set is then updated with the collected links, and each content is added to the dictionary with the internal link as the key. Once the crawling of a URL is complete, the newly collected URLs replace the urls_to_crawl, and the same steps are repeated for each of them.\nBefore returning the dictionary with lists as keys and content as values, the function also saves the information as a JSON file in a folder created in the current working directory. To accomplish this, the JSON and OS modules of Python are used. The folder name is defined and joined with the current directory using os.path.join. The function checks if the folder already exists with os.isdir(foldername). If it doesn’t exist, the function creates the folder and dumps the dictionary information into a JSON file within the created folder.\nCan I crawl the entire web? Photo by Timothy Dykes on Unsplash\nWhile it is technically possible to crawl a large portion of the clear web (unlike the dark web), it’s important to be aware of legal and ethical considerations. Some websites have specific permissions regarding web crawling, and not adhering to these permissions can lead to legal and ethical issues. Therefore, it’s always best practice to ensure proper authorization before proceeding with web crawling activities. You can usually find the necessary authorization information in the website’s “robots.txt” file. Take a moment to review this file and follow its guidelines.\nIt’s worth noting that the speed at which giant search engines like Google and Bing crawl the web is quite impressive and difficult to replicate. These search engines employ a vast amount of computational power and infrastructure to crawl and index web pages efficiently. They have dedicated teams of engineers and data centers around the world to support their crawling operations. Creating web crawlers that match the speed and efficiency of these search engines requires is practically impossible for an individual with a laptop in his/her basement as it requires substantial resources, including powerful servers, advanced algorithms, and a team of experts.\n","permalink":"http://localhost:1313/posts/python_web_crawler/","summary":"\u003cp\u003eIn this article, I’m going to explain the concept of a web crawler, how search engines work, and guide you in building a simple Crawler bot. All the code I’ve written for this can be found on \u003ca href=\"https://github.com/mrzeynalli/web_crawler/tree/main\"\u003emy GitHub repository\u003c/a\u003e, and feel free to use, modify, or replicate it for personal or commercial purposes without needing my consent.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cstrong\u003eProject description\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eLanguage:\u003c/em\u003e Python\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eLibraries:\u003c/em\u003e request, bs4, regex, os, json\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eIDE:\u003c/em\u003e Microsoft Visual Studio Code\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eProject type:\u003c/em\u003e Web crawling/scraping\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg alt=\"Photo by Timothy Dykes on Unsplash\" loading=\"lazy\" src=\"/images/web_crawler/1.webp\"\u003e\n\u003cem\u003ePhoto by Timothy Dykes on Unsplash\u003c/em\u003e\u003c/p\u003e","title":"Building a Web Crawler using Python"},{"content":"Project description\nLanguage: Python Working file: Microsoft Visual Studio Code Project type: Web Analytics Companies receive traffic to their website from various resources. The more a company learns about the visitors\u0026rsquo; traffic to its website, the better it understands the reasons behind their travel to the website. Besides the number of visitors to a website, it is important to learn from which sources those people arrive at the website. This can help the firms to see which external social account or network campaign is prone to bringing new customers. Furthermore, a company can also see the patterns in the behaviour of the visitors on different pages. By using various page analytics techniques, successful as well as problematic pages can be detected.\nPhoto by Adem AY on Unsplash\nIn this project, I used web analytics techniques to analyze the potential of three marketing campaigns run by a hypothetical company.\nData description and conversion Description The dataset consists of 160,000 rows, each corresponding to a different visit to a website and its clickstream — the pages the visitor entered. Each row also has corresponding \u0026lsquo;origin\u0026rsquo; and \u0026lsquo;platform\u0026rsquo; info. The \u0026lsquo;origin\u0026rsquo; value indicates through which source a user ended up on the website. Each origin value is one of the 7 different sources (the first three being paid champaigns):\nFacebook advertisement: When a user clicks on an ad that has been posted on Facebook. LinkedIn advertisement: When a user clicks on an ad that has been posted on LinkedIn. Partner advertisement: When a user clicks on an ad on a partner\u0026rsquo;s website. Facebook share: When a user clicks on a post that has been shared by a friend on Facebook. LinkedIn share: When a user clicks on a post that has been shared by a friend on LinkedIn. Direct: When a user directly types the website\u0026rsquo;s URL into their browser. Search: When a user finds the website by entering a search term into a search engine. The platform values are either \u0026lsquo;windows\u0026rsquo;, \u0026lsquo;mac\u0026rsquo;, \u0026lsquo;android\u0026rsquo;, \u0026lsquo;ios\u0026rsquo; or \u0026lsquo;unknown\u0026rsquo;. The latter is a result of the case in which the platform is not detected (due to various reasons such as the presence of an ad-blocker or VPN).\nConverstion The data is presented in a CSV file with two columns: the first indicates the origin and the second indicates the platform. The remaining columns to the right indicate the pages the user visited. Since not all rows have the same number of columns, a different data convention is needed for analysis. To achieve this, the function \u0026ldquo;generate_data_dict\u0026rdquo; takes a line as input and returns the values stored under their corresponding key in a dictionary. This function is used for each row in the dataset. After reading the dataset using the open() function in Python and using the readlines() method to convert each row into a line, I iterated through each row to obtain the values using the aforementioned function. The values for each visit were stored in a separate list created to contain all the visit info. After retrieving all visit values and closing the dataset file, I converted the list into a dataframe using the data argument, which was equal to the list. Since the list values were in a dictionary format, the keys were automatically constructed as columns, thus there was no need to add the columns argument separately. The data conversion codes are given below:\n# Define a function that converts a into a line and stores the values accordingly def generate_data_dict(line): line = line.rstrip(\u0026#34;\\n\u0026#34;) # convert the row into a line by stripping over a new-line character data_rows = line.split(\u0026#34;,\u0026#34;) # split the values in the row by comma chacater # Return the value for each feature in a dictionary format return {\u0026#39;Source\u0026#39;: data_rows[0], # the first value in a row - origin/source \u0026#39;Platform\u0026#39;: data_rows[1], # the second value in a row - platform \u0026#39;Clickstream\u0026#39;: data_rows[2:], # the remaining values in a row - pages visited \u0026#39;# of pages visited\u0026#39;: len(data_rows[2:]),} # the count of the pages visited ########################################################################################################## # Create a list that wil store the values for each visit visitor_data_list = [] with open(\u0026#39;visitor_data_clickstream.csv\u0026#39;, \u0026#39;r\u0026#39;) as file: # open the dataset file rows = file.readlines() # read each row in the dataset for row in rows: # iterate through the rows data_dict = generate_data_dict(row) # store the values for each row intoa dictionary visitor_data_list.append(data_dict) # add the values into a list file.close() # close the dataset file # Convert the visit values into a dataframe visitor_data_df = pd.DataFrame(data=visitor_data_list) Web analytics techniques Now that we have an intended dataframe of the clickstream on hand, it is time to start the analytics. But before, we need to discuss certain web analytics metrics. In practice, web analytics is usually carried out by observing a visitor\u0026rsquo;s behaviour over pages. Conversion, drop-out, and bounce rates were used in this project to analyze the pages.\nConversion rate shows the proportion of visitors that ended their visits successfully by carrying out a purchase (or any other intended outcome) Drop-out rate shows the proportion of visitors that entered the purchase page (or the processing page of any other intended outcome) but left without finishing Bounce rate shows the proportion of visitors that visited only one page and bounced away immediately Basically, each visit can be tagged if it has ended in one or more of the ways outlined above. I decided to add a separate column for each metric and tag 1 or 0, depending on whether the selected scenario (success, drop-out, bounce) happened in that particular visit. The below code shows how the columns and their indications are added to the dataframe:\npurchase_success_status_list = [] # create a list to visits that ended up in success drop_out_status_list = [] # create a list to visits that dropped out single_page_status_list = [] # create a list to visits that visited only one page # iterate through the indices of the dataframe for index in visitor_data_df.index: clickstream_list = visitor_data_df.loc[index][\u0026#39;Clickstream\u0026#39;] # seperate the clickstream value, i.e., pages visited if \u0026#39;purchase_success\u0026#39; in clickstream_list: # if the visit was successful, i.e., contains \u0026#39;purhcase_success\u0026#39; page purchase_success_status_list.append(1) # add 1 if YES else: purchase_success_status_list.append(0) # add 0 if NO if \u0026#39;purchase_start\u0026#39; in clickstream_list and \u0026#39;purchase_success\u0026#39; not in clickstream_list: # if the visit was dropped out, i.e., entrance into purchasing page without a success drop_out_status_list.append(1) # add 1 if YES else: drop_out_status_list.append(0) # add 0 if NO if len(clickstream_list) == 1: # if the visit contains only 1 page. i.e., a visitor bounced after a single page visit single_page_status_list.append(1) # add 1 if YES else: single_page_status_list.append(0) # add 0 if NO # add the respective list values into the dataframe visitor_data_df[\u0026#39;Conversion\u0026#39;] = purchase_success_status_list visitor_data_df[\u0026#39;Drop-out\u0026#39;] = drop_out_status_list visitor_data_df[\u0026#39;Bounce\u0026#39;] = single_page_status_list Simply, I created three lists for each metric and, by iterating through the indices of the dataframe, where an index is an individual visit, I checked if the clickstream of that visit had the relevant pages. Either 1, indicating a positive response, or 0, indicating a negative response, was added correspondingly.\nConversion — if the clickstream has a \u0026lsquo;purhcase_success\u0026rsquo; page, it means the visitor made a purchase successfully Drop-out — if the clickstream has a \u0026lsquo;purchase_start\u0026rsquo; page but not a \u0026lsquo;purhcase_success\u0026rsquo; one, it means the visitor entered the purchasing processing page but didn\u0026rsquo;t make a purchase for some reason Bounce — if the length of the clickstream is equal to 1, it means the visitor didn\u0026rsquo;t visit more than 1 page These columns only indicate whether an individual visit ended in one or more of the mentioned ways. Yet, I needed to calculate and see ratios for each platform or source. To achieve this, I created a function, which required three input values, numerator, denominator, and dataframe, and returned ratio values:\n# Create a function that returns a ratio for a given metric given numerator, denominator, and dataframe def generate_ratio(numerator, denominator, dataframe): ratios = [] # Create a list to store the ratio values for index in dataframe.index: # For each index numerator_value = dataframe.at[index, numerator] # Take the numeric from that index denominator_value = dataframe.at[index, denominator] # Take the denominator from that index ratio = numerator_value / denominator_value # Calculate the ratio ratios.append(round(ratio,2)) # Round the ratio into two decimal points return ratios # Return the ratios list However, I needed to have a dataframe with summed values for each metric (numerator), which then would be divided by the total visit (denominator). For this purpose, corresponding to my project\u0026rsquo;s two main analytical directions (analyzing Source and Platform), I used groupby() method of dataframe to create two new dataframe objects, indexed by their columns and summing their respective numeric values. Then, I added the total number of visits for each Source and Platform:\n# Groupyby all the values by source, summing the numeric values only source_group_df = visitor_data_df.groupby(\u0026#39;Source\u0026#39;).sum(numeric_only=True) # Create a list that stores the total number of visits per source number_of_visits_per_source = list(visitor_data_df.groupby(\u0026#39;Source\u0026#39;).count()[\u0026#39;Platform\u0026#39;]) # Add the total visits to the grouped by dataframe source_group_df[\u0026#39;Total Visits\u0026#39;] = number_of_visits_per_source ############################################################################################### # Groupyby all the values by platform, summing the numeric values only platform_group_df = visitor_data_df.groupby(\u0026#39;Platform\u0026#39;).sum(numeric_only=True) # Create a list that stores the total number of visits per platform number_of_visits_per_platform = list(visitor_data_df.groupby(\u0026#39;Platform\u0026#39;).count()[\u0026#39;Source\u0026#39;]) # Add the total visits to the grouped by dataframe platform_group_df[\u0026#39;Total visits\u0026#39;] = number_of_visits_per_platform Now we have two useful dataframe objects and a function to generate ratios, by clarifying the columns whose ratios will be calculated, I added the ratios of all three metrics (by dividing the sums of 1\u0026rsquo;s by the total visits) to each dataframe:\n# Create the list for columns whose ratios will be calculated list_of_columns_for_ratio_calculation = [\u0026#39;Conversion\u0026#39;, \u0026#39;Drop-out\u0026#39;, \u0026#39;Bounce\u0026#39;] # ADD RATIOS FOR THE DATAFRAME GROUPED BY SOURCE # Iterate through the columns list for column in list_of_columns_for_ratio_calculation: # Formulate the column name by adding ratio in the front column_name = column + \u0026#39; rate\u0026#39; # Generate the ratios and add to the dataframe under the formulated column name source_group_df[column_name] = generate_ratio(column, \u0026#39;Total Visits\u0026#39;, source_group_df) # ADD RATIOS FOR THE DATAFRAME GROUPED BY PLATFORM # Iterate through the columns list for column in list_of_columns_for_ratio_calculation: # Formulate the column name by adding ratio in the front column_name = column + \u0026#39; rate\u0026#39; # Generate the ratios and add to the dataframe under the formulated column name platform_group_df[column_name] = generate_ratio(column, \u0026#39;Total visits\u0026#39;, platform_group_df) Results Over tables Table 1: The Visit Statistics per Source\nSource # of pages visited Conversion Drop-out Bounce Total Visits Conversion rate Drop-out rate Bounce rate direct 47724 4275 1191 1287 13500 0.32 0.09 0.10 facebook_advert 19334 93 2728 4262 10000 0.01 0.27 0.43 facebook_share 177253 6657 12345 8125 51300 0.13 0.24 0.16 linkedin_advert 8292 504 896 0 2000 0.25 0.45 0.00 linkedin_share 71015 2546 4913 4239 21200 0.12 0.23 0.20 partner_advert 19034 545 3000 0 5000 0.11 0.60 0.00 search 188925 8720 12127 10537 57400 0.15 0.21 0.18 Table 1 presents comprehensive visit statistics that have been carefully filtered by source. The data shows the number of visitors, conversion, drop-out, and bounce rates for each source, giving you a complete picture of the traffic generated by different sources. This information can be used to optimize your marketing strategy by focusing on the sources that generate the most traffic and minimizing efforts on sources that underperform. Furthermore, the data can be analyzed to identify trends and patterns that can inform future marketing decisions. Overall, the data presented in Table 1 is a valuable resource for anyone looking to improve their website\u0026rsquo;s traffic and engagement metrics.\nTable 2: The Visit Statistics per Platform\nPlatform # of pages visited Conversion Drop-out Bounce Total visits Conversion rate Drop-out rate Bounce rate android 148686 6642 10003 8003 44500 0.15 0.22 0.18 ios 145639 5649 10969 8001 44500 0.13 0.25 0.18 mac 94268 4481 6296 4497 28000 0.16 0.22 0.16 unknown 48723 1976 3704 3341 15400 0.13 0.24 0.22 windows 94261 4592 6228 4608 28000 0.16 0.22 0.16 Observing Table 2, we can see the visit statistics filtered by platform. It\u0026rsquo;s important to note that this information can provide us with valuable insights into user behaviour and preferences, which we can then use to inform future decision-making. For example, if we notice that a certain platform has significantly more visits than others, we may want to consider investing more resources into that platform to maximize our reach and engagement. Alternatively, if we notice that a certain platform has a high bounce rate, we may want to investigate why that is and make adjustments to improve the user experience. By taking a more nuanced approach to analyzing these visit statistics, we can gain a deeper understanding of our audience and optimize our strategies accordingly.\nOver graphs I analyzed the traffic to the website and found significant differences in visits, conversion rates, and bounce rates among the three advertisement campaigns. LinkedIn had the highest conversion rate at 25%, followed by Partner Websites at 11% and Facebook at 1%.\nThe bounce rate for Facebook was 43%, while the other two campaigns had no such visitors. Interestingly, visitors from shared posts on social media and search engines generated more traffic than the advertisements. Direct traffic had the highest conversion rate at 32%, followed by search engine visitors at 15%, and shared posts on Facebook and LinkedIn at 13% and 12%, respectively.\nMobile visitors were much more numerous than desktop visitors, but desktop visitors had a slightly higher conversion rate and lower bounce rate. IOS platform visitors had a 25% drop-out rate, while other platforms had a 22% rate (except for unknown visitors at 24%). Visitors had difficulty navigating beyond the \u0026lsquo;Blog 1\u0026rsquo; page, with a 10% bounce rate for desktop users and a 6% bounce rate for IOS and Android users. The \u0026lsquo;Home\u0026rsquo; page had a 9% bounce rate for desktop users and a 6% bounce rate for IOS and Android users. Blog 1 was the most challenging page for visitors, but 24% of visitors who completed a successful purchase had read it. Blog 2 had a 19% success rate.\nTo increase conversion rates and reduce bounce rates, the company should invest more in LinkedIn advertisements and improve its search engine optimization. It should also improve the layout of \u0026lsquo;Blog 1\u0026rsquo; and \u0026lsquo;Home\u0026rsquo; pages, especially for Windows and Mac users. Additionally, it should write more blogs similar to the content of Blog 1 to contribute to higher conversion rates.\nKey takeaways: LinkedIn had the highest conversion rate at 25%. Mobile visitors were much more numerous than desktop visitors. Direct traffic had the highest conversion rate at 32%. Blog 1 was the most challenging page for visitors but was read by 24% of successful purchasers. The company should invest more in LinkedIn advertisements and improve search engine optimization to increase conversion rates and reduce bounce rates. ","permalink":"http://localhost:1313/posts/web_analytics_clickstream/","summary":"\u003cp\u003e\u003cstrong\u003eProject description\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eLanguage:\u003c/em\u003e Python\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eWorking file:\u003c/em\u003e Microsoft Visual Studio Code\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eProject type:\u003c/em\u003e Web Analytics\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003eCompanies receive traffic to their website from various resources. The more a company learns about the visitors\u0026rsquo; traffic to its website, the better it understands the reasons behind their travel to the website. Besides the number of visitors to a website, it is important to learn from which sources those people arrive at the website. This can help the firms to see which external social account or network campaign is prone to bringing new customers. Furthermore, a company can also see the patterns in the behaviour of the visitors on different pages. By using various page analytics techniques, successful as well as problematic pages can be detected.\u003c/p\u003e","title":"Web Analytics: Analyzing clickstream of 160,000 visitors"},{"content":"This project is carried out by me independently. The dataset is obtained publicly from Kaggle. All codes and their explanations are stored in my GitHub repository.\nLanguage: Python Libraries: sklearn, pandas, numpy, matplotlib, seaborn IDE: Jupyter notebook Project type: Machine learning, Unsupervised learning, K-Means clustering FIFA 23 is a football video game created by Electronic Arts (EA). It became the best-selling football video game in Christmas UK retail charts. According to EA statistics, the game contains more than 700 teams with over 19,000 football players, playing in at least 30 football leagues. The data used in this project is taken from Kaggle. The objective of this project is to classify the players into various segments.\nPhoto by Hatem Boukhit on Unsplash\nAlthough I implemented this project just out of my interest in football and my general engagement in playing the FIFA video game, the results may be helpful for any external observer such that the valuation of players might be determined or justified based on the segment they are in. Additionally, a team\u0026rsquo;s success may be tested over the number of players from each segment with the purpose of finding out any correlation.\nData collection and cleaning The dataset used initially contained 17,660 rows and 29 columns. In other words, 17,600 players were present in the dataset explained with 29 features. The features were both quantitative and qualitative, ranging from the player\u0026rsquo;s name to their market value. I first used Pandas to call the raw format of the CSV file from the GitHub repo. After careful observation of the dataset, I noticed that some players, who already retired from a club, are still shown as the players of that particular club. I decided to eliminate those players in order to have the most up-to-date statistics. To tackle this problem, an interesting point in the dataset helped me.\nThe already-retired players\u0026rsquo; names start with their latest kit number (\u0026ldquo;15 Xavi\u0026rdquo;, \u0026ldquo;22 D. Alves\u0026rdquo;). This enabled me to separate those players by applying an algorithm to detect the player names that start with digits. The codes are as follows:\n# This list object will store the indices of players whose names start with digit player_indices_to_remove = [] # This loop iterates through the indices of all players to detect the ones with digit-starting player names for index in fifa23_df.index.to_list(): player = fifa23_df[\u0026#39;Name\u0026#39;].loc[index] player_first_name = fifa23_df[\u0026#39;Name\u0026#39;].loc[index][0] # If the player name starts with digit, it adds the index of that observation to removable indices list if player_first_name.isnumeric(): player_indices_to_remove.append(index) # Now, we drop the indices of players who are unnecesarily included in the dataset fifa23_df.drop(player_indices_to_remove,axis=0,inplace=True) # We reset the indices of the dataframe fifa23_df.reset_index(drop=True,inplace=True) The codes go over each player\u0026rsquo;s name by iterating through the indices of the observations, taking out the very first character of that name, and checking if that character is numeric. Later, indices of the positive cases are added to the previously created list that aims to store retired players. Subsequently, those players are dropped out of the dataset, and the indexing is re-set.\nAfter tackling this problem, I faced yet another data problem. The numeric values indicating monetary values are put in with their respective currency and the prefixes, M and K, for millions and thousands, respectively. I needed to first eliminate the currency symbol and convert the values into their actual value. I formulated the below function that handles this specific duty:\ndef curreny_correction(column,dataframe, curreny_sign = str): # Split the value by the given currency symbol: euro in our instance splits = dataframe[column].str.split(curreny_sign, expand = True)[1] values = splits.str[:-1] # Store the values prefixes= splits.str[-1:] # Store the prefixes # Create a list object that will store the float-converted format of the values values_float = [] for value, prefix in zip(values, prefixes): # The wage and value point are either in thousands (K) or millions (M) or 0 if prefix == \u0026#39;M\u0026#39;: # Checks if letter is \u0026#39;M\u0026#39; or the value is million try: # When values are zero, they cannot be converted into float and raises ValueError. #I debug the coding for those occasions float_value = float(value) * 1000000 values_float.append(float_value) except ValueError: # Adds just 0 when ValueError is raised float_value = 0 values_float.append(float_value) elif prefix == \u0026#39;K\u0026#39;: # If the letter is \u0026#39;K\u0026#39; or the value is thousands try: # When values are zero, they cannot be converted into float and raises ValueError. #I debug the coding for those occasions float_value = float(value) * 1000 values_float.append(float_value) except ValueError: # Adds just 0 when ValueError is raised float_value = 0 values_float.append(float_value) # Returns the float values that are stripped of currency symbol and exponential letters return values_float Features including \u0026ldquo;Value\u0026rdquo;, \u0026ldquo;Wage\u0026rdquo;, and \u0026ldquo;Release Clause\u0026rdquo; were converted using the above function. Different scripts were written to convert the features (\u0026ldquo;Position\u0026rdquo;, \u0026ldquo;Height\u0026rdquo;, \u0026ldquo;Weight\u0026rdquo;) with slightly different characters.\n# Correcting \u0026#39;Position\u0026#39; feature fifa23_df[\u0026#39;Position\u0026#39;] = fifa23_df[\u0026#39;Position\u0026#39;].str.split(\u0026#39;\u0026gt;\u0026#39;, expand=True)[1] # Correcting \u0026#39;Height\u0026#39; feature heigh_values = [float(value) for value in fifa23_df[\u0026#39;Height\u0026#39;].str.split(\u0026#34;cm\u0026#34;, expand = True)[0]] fifa23_df[\u0026#39;Height\u0026#39;] = heigh_values # Correcting \u0026#39;Weight\u0026#39; feature weight_values = [float(value) for value in fifa23_df[\u0026#39;Weight\u0026#39;].str.split(\u0026#34;kg\u0026#34;, expand = True)[0]] fifa23_df[\u0026#39;Weight\u0026#39;] = weight_values I eliminated 11 features based on whether they are non-useful for the analysis or have huge null values. The final dataset ready for the analysis contained 17 features and 10,104 observations.\nData visualization I created a couple of pre-analytics graphs to get a better understanding of the data I am working with. Seaborn and matplotlib modules were used for visualization purposes.\nFigure 1: Overall Rating Score of players per their Preferred Foot Figure 1 displays how the players are distributed on their rating score based on their preferred foot. There seems to be not a big difference between the feet. However, a very slight superiority can be observed for the left foot (presumably, because of Leo Messi the GOAT).\nFigure 2: Scatter plot of Overall Rating Score and Age When it comes to the relationship between age and overall rating, an apparent positive linear relationship is visible (Figure 2). Seemingly, the greater the age of the player, the higher the rating is. The relationship seems to fade away after the age of 35.\nFigure 3: Correlation matrix The correlation among the numeric variables of the dataset can be seen in Figure 3. High correlation scores are visible between Wage and Value, Height and Weight, Value and Overall, and Value and Potential. Although Overall Rating and Age are highly positively correlated, interestingly, there is not much correlation between Potential and Age. This signals that the young players who do not have high overall at the moment can increase their rating score substantially.\nk-Means clustering Initially, the numeric features need to be scaled, given the fact that the presence of outliers along with huge variations among the ranges of different variables can negatively impact the k-Means clustering process. I used StandardScaler from sklearn.preprocessing to standardize the quantitative variables.\nFor specifying the best number of clusters, I carried out Within-Cluster Sum of Squares (WCSS) and Average Silhouette methods.\nFigure 4: WSCC method WSCC method calculates the cluster differences for each cluster number. By observing the above plot (Figure 4) for each k, it can be observed that the variations tend to slow down after around 5. Thus, the graph signals 5 to be the best number of clusters.\nFigure 5: Average Silhouette method In the average silhouette method, the distances between neighboring cluster items and local cluster items are calculated. Figure 5 shows the final average silhouette scores for each k cluster number. Accordingly, the highest score signals the best k (besides 2).\nAs a result, I have enough proof to use 5 as my cluster number.\nProject results: Final clusters I used the kMeans function from sklearn.cluster to do the clustering. 5 different clusters were formed for the players in the dataset. The clusters and the number of players in each cluster are the following:\nCluster 0 has 2640 players Cluster 1 has 3228 players Cluster 2 has 903 players Cluster 3 has 3197 players Cluster 4 has 136 players Table 1: Cluster results\ncluster Age Overall Potential Value Wage Height Weight Release Clause 0 20.47 56.40 67.42 386295.45 44692.80 180.29 72.88 7.700856e+05 1 23.80 67.22 73.81 2275497.21 14163.88 175.94 69.68 4.378210e+06 2 26.35 78.79 81.78 19393023.26 49131.78 182.18 75.93 3.738128e+07 3 25.44 66.95 72.08 1892169.22 15918.36 187.44 81.06 3.557997e+06 4 26.29 85.38 88.04 67459558.82 147213.24 182.27 76.98 1.309640e+08 Table 1 demonstrates how the players are separated based on the clusters.\nCluster 0 is characterized by younger players, who have relatively low rating scores and potential. Their values and wages are low, correspondingly. They are the kind of football players, who play in and are transferred by middle-sized clubs. Yet, they usually have a high release clause because of their young age.\nCluster 1 accommodates young and middle-aged players with yet low rating scores. These players usually start and end their careers in small- and middle-sized teams. However, given their still young age, they have a high release clause, as well.\nCuster 2 has, on average, the oldest players among the clusters. The values and wages the players of this category have are fairly large, which signals their importance in the team. They mostly play in middle-sized and big teams, given their high values. Given their height and rating, I reckon that these are the strikes that play a crucial role in the attack.\nCluster 3 seems to have the same type of players as cluster 2. The difference is that cluster 3 players play in small- and middle-sized teams, explained by their low value and wage as well as low rating score and potential. They are quite tall and key players on the attack.\nCluster 4 takes the best players in the arena. They are still performing high and are the key players in their big teams. Arguably, they have been succeeding on their teams for quite a long time. Now, as they are old, their release clauses are also very low. Messi, Ronaldo, and Lewandowski should be in this cluster.\n","permalink":"http://localhost:1313/posts/fifa23_players_analysis/","summary":"\u003cp\u003eThis project is carried out by me independently. The dataset is obtained publicly from Kaggle. All codes and their explanations are stored in \u003ca href=\"https://github.com/mrzeynalli/fifa_23_players_analysis\"\u003emy GitHub repository\u003c/a\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eLanguage:\u003c/em\u003e Python\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eLibraries:\u003c/em\u003e sklearn, pandas, numpy, matplotlib, seaborn\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eIDE:\u003c/em\u003e Jupyter notebook\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eProject type:\u003c/em\u003e Machine learning, Unsupervised learning, K-Means clustering\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFIFA 23 is a football video game created by Electronic Arts (EA). It became the best-selling football video game in \u003ca href=\"https://www.gamesindustry.biz/fifa-is-christmas-no1-as-god-of-war-drops-to-third-place-uk-boxed-charts#:~:text=FIFA%2023%20was%20the%20best,a%2013%25%20boost%20in%20sales.\"\u003eChristmas UK retail charts\u003c/a\u003e. According to \u003ca href=\"https://www.ea.com/games/fifa/news/fifa-23-all-leagues-clubs-teams-list#:~:text=With%20more%20than%2019%2C000%20players,%2C%20Bundesliga%2C%20LaLiga%20Santander%2C%20CONMEBOL\"\u003eEA statistics\u003c/a\u003e, the game contains more than 700 teams with over 19,000 football players, playing in at least 30 football leagues. The data used in this project is taken from \u003ca href=\"https://www.kaggle.com/datasets/bryanb/fifa-player-stats-database\"\u003eKaggle\u003c/a\u003e. The objective of this project is to classify the players into various segments.\u003c/p\u003e","title":"FIFA23 Players Analysis: k-Means Clustering"},{"content":"Project description\nLanguage: Python Libraries: requests, pandas, numpy, matplotlib, seaborn, os, math, time, datetime, json IDE: Microsoft Visual Studio Code, Jupyter Notebook Project type: Data analytics, Web scraping, API Companies House is an agency formed by the British Government to maintain the registration of all the companies in the UK. It maintains a database that stores the information of the registered companies. Each company has features such as company name, Standard Industrial Classification (SIC) code, creation date, cessation date (if ceased operating), company board and shareholder info, etc. By using an API, it is possible to scrape the data from that database by using various specifications.\nThis project aims to scrape the data of the tech companies in the UK and figure out the main tech areas (cities) besides London, the capital of the UK. The idea is to decentralize the tech processes from the capital city. The results of this project can be beneficial for people seeking low-competitive employment in the tech sphere or investors seeking conservative investment options in the UK tech arena.\nPhoto by Rodrigo Santos on Unsplash\nData Collection and Cleaning The project uses the requests library to request data from Companies House. The scraped data is converted into json format and combined into a single dataframe. The following Python libraries are used:\nimport pandas as pd import numpy as np import requests as rq import json import math import time import datetime as dt import matplotlib.pyplot as plt import seaborn as sns import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) A class-based API caller function was built to collect company data and return it as a dataframe:\nclass api_caller: root_url = \u0026#39;https://api.companieshouse.gov.uk/\u0026#39; key = \u0026#34;YOUR_API_KEY\u0026#34; def return_dataframe(self, url_extention): url = self.root_url + url_extention query_result = rq.get(url, auth=(self.key,\u0026#39;\u0026#39;)) if query_result.status_code == 200: json_file = json.JSONDecoder().decode(query_result.text) items_file = json_file[\u0026#39;items\u0026#39;] keys = items_file[0].keys() companies_df = pd.DataFrame(items_file, columns = keys) return companies_df else: return None Full SIC code lists were scraped and processed. A CSV file of SIC codes is available on my GitHub.\nI created functions to query companies based on single or multiple SIC codes, as well as functions to process address data and extract \u0026rsquo;locality\u0026rsquo; and \u0026lsquo;postal_code\u0026rsquo; fields from nested dictionaries.\nData Analysis I began by extracting SIC codes for tech-related industries using keywords like \u0026rsquo;technology\u0026rsquo;, \u0026rsquo;engineering\u0026rsquo;, \u0026lsquo;software\u0026rsquo;, and \u0026lsquo;hardware\u0026rsquo;. Duplicate entries were removed. I also extracted city names from company addresses and grouped the data by city (excluding London).\nData Visualization and Project Results Using matplotlib and seaborn, I visualized the distribution of tech companies across non-capital cities in the UK.\nFigure 1: Proportion of Tech Companies in TOP10 Non-Capital Cities of UK Manchester, Birmingham, and Bristol emerge as the top non-capital cities with tech companies, capturing 19.1%, 14.0%, and 10.6% of the share respectively.\nFigure 2: Tech Companies in TOP10 Non-Capital Cities per Company Status Birmingham leads in active tech companies, followed by Manchester and Cambridge.\nConclusion Manchester, Birmingham, Bristol, and Cambridge are identified as the top non-capital UK cities with high-tech potential. These cities are recommended for individuals seeking less-competitive job markets or investors targeting decentralized tech hubs.\n","permalink":"http://localhost:1313/posts/uk_cities_tech_potential/","summary":"\u003cp\u003e\u003cstrong\u003eProject description\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eLanguage:\u003c/em\u003e Python\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eLibraries:\u003c/em\u003e requests, pandas, numpy, matplotlib, seaborn, os, math, time, datetime, json\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eIDE:\u003c/em\u003e Microsoft Visual Studio Code, Jupyter Notebook\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eProject type:\u003c/em\u003e Data analytics, Web scraping, API\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003e\u003ca href=\"https://www.gov.uk/government/organisations/companies-house\"\u003eCompanies House\u003c/a\u003e is an agency formed by the British Government to maintain the registration of all the companies in the UK. It maintains a database that stores the information of the registered companies. Each company has features such as company name, Standard Industrial Classification (SIC) code, creation date, cessation date (if ceased operating), company board and shareholder info, etc. By using an API, it is possible to scrape the data from that database by using various specifications.\u003c/p\u003e","title":"Non-capital UK Cities with High-tech Potential"},{"content":"Project description\nLanguage: Python Working file: Jupyter notebook In 2018, former President of South Africa, Jacob Zuma, established a commission of enquiry in state capture, known as The Judicial Commission of Inquiry into Allegations of State Capture, Corruption and Fraud in the Public Sector including Organs of State, or simply the Zondo Commission. The commission collected one exabyte of evidence, and on 22 June 2022 released its final report. The reports are available publicly in this link.\nImage is taken from www.corruptionwatch.org.za\nThis project aims to analyze the contents of these reports to capture the names of publicly traded companies whose names are mentioned more frequently. The results would be beneficial for prudent investors who seek ethical investments opportunities in South Africa.\nData Collection and Cleaning Two datasets were used in this project. First dataset were created by converting the report .pdf files into .csv format. PyPDF2 module was used for transforming each pdf content into a text-formatted object, and Pandas was used to store those text-formatted objects into a dataframe. The final dataset of the texts were converted to .csv file and uploaded to Github, where the file\u0026rsquo;s raw format was used.\nimport pandas, PyPDF2 # storing all the pdf files in a list report_paths = [\u0026#39;OCR version - State Capture Commission Report Part 1 Vol I.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part II Vol II.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part III Vol I - Bosasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part III Vol II - Bosasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part III Vol III - Bosasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part III Vol IV - Bosasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part IV Vol I - NT,EOH,COJ,Alexkor.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part IV Vol II- FS.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part IV Vol III - Eskom.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part IV Vol IV - Eskom.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part V Vol I - SSA.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part V Vol II - SABC,Waterkloof,Prasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part VI Vol I - Estina,Vrede.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part VI Vol II - CR.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part VI Vol III - Flow of Funds.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part VI Vol IV - Recommendations.pdf\u0026#39;] # creating a dataframe file zondo_reports = pd.DataFrame(columns=[\u0026#39;report\u0026#39;, \u0026#39;text\u0026#39;]) i = 0 # iterating through the pdf list for path in report_paths: # converting each pdf content into a text object pdfFileObj = open(path, \u0026#39;rb\u0026#39;) print(\u0026#39;opened\u0026#39;, path) pdfReader = PyPDF2.PdfReader(pdfFileObj) text=\u0026#39;\u0026#39; for page in pdfReader.pages: text += page.extract_text() print(\u0026#39;extracted text for\u0026#39;, path) # adding the converted text objects into dataframe zondo_reports.loc[i] = [path, text] i+=1 pdfFileObj.close() print(\u0026#39;closed\u0026#39;, path) After obtaining the first dataset, we needed another dataset consisting of the companies that are publicly traded in South Africa. For this objective, we used Johannesburg Stock Exchange (JSE) portal to pick up the candidate companies. January 2021 dataset of the companies were downloaded from the JSE website in .xlsx format. The file then were re-formatted into .csv to be readable from GitHub. The .csv file was uploaded to my GitHub account, and its raw format was called using read_csv method of Pandas.\nAs of now, we have a dataset containing reports contents and a dataset containing the publicly traded companies in South Africa.\nAnalysis To carry out the text analysis, nltk module was primarily used. The below code displays how different classes of that module were imported:\nimport nltk, string from nltk.tokenize import word_tokenize nltk.download(\u0026#39;punkt\u0026#39;) from nltk.corpus import stopwords nltk.download(\u0026#39;stopwords\u0026#39;) from nltk.probability import FreqDist Firstly, we used word_tokenize function to tokenize all the reports contents into a single variable. Although the best practice is to lowercase all the tokens, we intentionally left the words as they were, given the fact that we were looking for company names, which are proper nouns. Thus, leaving proper nouns as they were was a better choice for the analysis.\nTo get rid of unnecessary words, we created a variable consisting of stop words from nltk.corpus and punctuations from string module. Further, some additional junk words were added to be removed from the tokens.\n# remove some stops from the tokens junk_tokens = [\u0026#39;Mr\u0026#39;,\u0026#39;Ms\u0026#39;,\u0026#39;Dr\u0026#39;,\u0026#39;P\u0026#39;,\u0026#39;``\u0026#39;, \u0026#39;\\\u0026#39;s\u0026#39;,\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\\\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\\\u0026#39;\\\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\u0026#34;\u0026#39;,\u0026#39;\u0026#34;\u0026#39;,\u0026#39;................................\u0026#39;,\u0026#39;L\u0026#39;] removables = set(stopwords.words(\u0026#39;English\u0026#39;) + list(string.punctuation) + list(string.digits) + junk_tokens) filtered_tokens = [token for token in all_content_tokens if token not in removables] Numerous company names come with additional descriptive words attached to them such as \u0026lsquo;Holding\u0026rsquo;, \u0026lsquo;Corporation\u0026rsquo;, \u0026lsquo;Limited\u0026rsquo;, and etc. We created a function that checks the name for each company and leaves those additions out. The new names were added to the dataframe under a column name \u0026lsquo;search term\u0026rsquo;. Afterwards, finally, time came for searching for company names (using the search terms) inside the reports (texts that are stored in the first dataframe).\nFor companies with one search term, we just searched the tokens as they are single words. For the companies with two or three words in their names, we used bigrams and trigrams from nltk, respectively. An additional column was added to the dataframe indicating True if the company name is mentioned in the reports and False otherwise.\n# Bigrams and trigrams are created to search for two-word and three-word search terms individually. bigrams = list(nltk.bigrams(filtered_tokens)) trigrams = list(nltk.trigrams(filtered_tokens)) # Create new column that will store whether a company name is mentioned or not. listed_companies[\u0026#34;FoundInReport\u0026#34;] = \u0026#34;False\u0026#34; for ind in listed_companies.index: searchterm = listed_companies[\u0026#39;SearchTerm\u0026#39;][ind] if len(searchterm) == 1: if searchterm[0] in filtered_tokens: # search for one-word search terms print(\u0026#39;1 word company appeared in text:\u0026#39;, searchterm) listed_companies.at[ind,\u0026#39;FoundInReport\u0026#39;] = True if len(searchterm) == 2: if searchterm in bigrams: # search for two-word search terms print(\u0026#39;2 word company appeared in text:\u0026#39;, searchterm) listed_companies.at[ind,\u0026#39;FoundInReport\u0026#39;] = True if len(searchterm) == 3: if searchterm in trigrams: # search for three-word search terms print(\u0026#39;3 word company appeared in text:\u0026#39;, searchterm) listed_companies.at[ind,\u0026#39;FoundInReport\u0026#39;] = True Next, we used FreqDist function from nltk.probability to calculate how many times each company name is mentioned in all reports combined.\nResults Matplotlib and seaborn libraries were used for the purpose of visualization. The following figures were drawn to illustrate the results.\nFigure 1: The frequency of company name mentions by sector Figure 1 shows how many times the names of companies in each sector is mentioned in the reports. Banking companies top the chart, by having 236 name mentions in total. It is followed by Mining, Software\u0026amp;Computing Services, and Media, by being mentioned 197, 110, and 76 times, respectively.\nFigure 2: Frequency of mentions for company Figure 2 clearly depicts the company names that were mentioned in the reports. Glencore Plc is the company with the highest quantity of mentions (190). Standard Bank Group Limited and Nedbank Group Limited are the main banks whose names are widely used in the reports. The following are the EOH Holding Limited and MultiChoice Group Limited.\nNote: It is important to note that the results of this analysis do not impose any kind of allegation against any of the companies mentioned above. As a matter of fact, the analysis focuses on the numerical counts rather than the context of the mentions. More clearly, it does not claim that the company names are mentioned in a negative manner. Thus, this project has only one primary purpose to be a guide for the investors. Additional in-depth research is up to the investors when they ponder of investing on those firms.\n","permalink":"http://localhost:1313/posts/zondo_reports_analysis/","summary":"\u003cp\u003e\u003cstrong\u003eProject description\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eLanguage:\u003c/em\u003e Python\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eWorking file:\u003c/em\u003e Jupyter notebook\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003eIn 2018, former President of South Africa, Jacob Zuma, established a commission of enquiry in state capture, known as The Judicial Commission of Inquiry into Allegations of State Capture, Corruption and Fraud in the Public Sector including Organs of State, or simply the Zondo Commission. The commission collected one exabyte of evidence, and on 22 June 2022 released its final report. The reports are available publicly in this link.\u003c/p\u003e","title":"Zondo Reports Text Analysis"},{"content":"Project description\nLanguage: Python, HTML Libraries: folium, pandas, numpy, requests, json IDE: Microsoft Visual Studio Code, Jupyter Notebook Project type: Data analytics, Web scraping, Map formation, API Keywords: Scotland, schools, deprivation, map Photo by ali elliott on Unsplash\nPostcodes API The available contact information for the schools solely consists of their postcodes and seedcodes. Consequently, in order to accurately locate each school on a map, I required latitude and longitude values for each establishment. In my search for geolocation information, I discovered an open platform called Postcodes IO. This platform offers a free API that allows access to the server and retrieval of geolocation information for every UK postcode. The collected data was presented in a JSON format. To obtain a comprehensive dataset from postcodes.io, I utilized various endpoints of the API, including the following:\nhttps://api.postcodes.io/postcodes/QUERY — this endpoint allowed me to query a single postcode and get its geo-information. https://api.postcodes.io/postcodes/QUERY/validate — this endpoint allowed me to check the validity of the query postcode. https://api.postcodes.io/postcodes/terminated_postcodes/QUERY — this endpoint allowed me to check if the postcode has already been terminated. https://api.postcodes.io/postcodes/BULK_QUERY — for the very first endpoint, the API also allowed the query of postcodes in a bulk. So, it is possible to request the information of 100 postcodes in a single request call. Upon completion of the final debugging and error handling process, the Postcodes API caller class object was ultimately concluded and subsequently archived in the designated directory for utilization in the map building application. The object is capable of providing four distinct information points pertaining to each queried postcode, namely: latitude, longitude, city, and zone. The codes necessary for constructing the API caller are available in my GitHub repository and can be accessed accordingly.\nBuilding the API Caller Object # import necessary libraries import requests # create an object to call the API class PostcodeApi: # create initialiser def __init__(self): self.api = \u0026#39;https://api.postcodes.io\u0026#39; # create a function to check the validity of the input postcode def check_validity(self, p): val = self.api + \u0026#39;/postcodes/\u0026#39; + p + \u0026#39;/validate\u0026#39; # formulate validity endpoint result = requests.get(val).json()[\u0026#39;result\u0026#39;] # get the request result if result == False: # check if the entered postcode is valid print(f\u0026#39;Non-valid post code: {p}\u0026#39;) return None else: return True # create a function to check if the postcode is terminated def check_termination(self, p): ter = self.api + \u0026#39;/terminated_postcodes/\u0026#39; + p # formulate termination endpoint if requests.get(ter).status_code == 200: # check if the entered postcode is terminated print(f\u0026#39;The postcode {p} is terminated.\u0026#39;) return None else: return True ### THE REST OF THE CODES FOLLOW THE NEXT In order to construct the API object, I exclusively utilize the requests library, as it suffices to dispatch requests to the endpoints. Following the definition of the object’s initializer, I proceeded to establish two functions, namely check_validity() and check_termination(), which verify the accuracy of the inputted postcode and its termination status. Both functions transmit the postcode to the pertinent endpoints and assess the call status prior to forwarding it to the postcode endpoint. Subsequently, I developed functions to retrieve information for either a singular postcode or a multitude of postcodes.\n# create a function to request the postcode info def get_pos_info(self, pos): if self.check_termination(pos): # apply validity check if self.check_validity(pos): # apply termination check q = self.api + \u0026#39;/postcodes/\u0026#39; + pos # formulate the postcode query endpoint result = requests.get(q).json()[\u0026#39;result\u0026#39;] # collect the result in a JSON format lat = result[\u0026#39;latitude\u0026#39;] # latitude lon = result[\u0026#39;longitude\u0026#39;] # longitude city = result[\u0026#39;admin_district\u0026#39;] # city zone = result[\u0026#39;parliamentary_constituency\u0026#39;] # zone return {\u0026#39;loc\u0026#39; : [lat,lon], \u0026#39;city\u0026#39; : city, \u0026#39;zone\u0026#39; : zone} # return the findings # create a function to request the bulk postcodes info def get_bulk_pos_info(self, pos_bulk): # define the URL url = self.api + \u0026#39;/postcodes/\u0026#39; # define the JSON payload data = {\u0026#34;postcodes\u0026#34;: pos_bulk} # set the headers headers = {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;} # send the POST request response = requests.post(url, json=data, headers=headers) # check the response if response.status_code == 200: # the request was successful, and you can parse the response JSON result = response.json()[\u0026#39;result\u0026#39;] # convert the result into a dictionary format result_items = { \u0026#39;locs\u0026#39; : [ (r[\u0026#39;result\u0026#39;][\u0026#39;latitude\u0026#39;], r[\u0026#39;result\u0026#39;][\u0026#39;longitude\u0026#39;]) if r[\u0026#39;result\u0026#39;] != None else None for r in result], \u0026#39;cities\u0026#39; : [r[\u0026#39;result\u0026#39;][\u0026#39;admin_district\u0026#39;] if r[\u0026#39;result\u0026#39;] != None else None for r in result], \u0026#39;zones\u0026#39; : [r[\u0026#39;result\u0026#39;][\u0026#39;parliamentary_constituency\u0026#39;] if r[\u0026#39;result\u0026#39;] != None else None for r in result] } return result_items else: # handle if error is encountered print(f\u0026#34;Error: {response.status_code} - {response.text}\u0026#34;) return None The get_pos_info() function is designed to retrieve information for a single postcode, following a validation process. It returns the latitude, longitude, city, and zone details in a dictionary format. On the other hand, the get_bulk_pos_info() function offers a more advanced functionality by accepting multiple postcodes in bulk. Notably, this function utilizes a different approach as it posts the input bulk of postcodes to the endpoint, rather than making a request to obtain the information. The postcodes are sent in a dictionary format with headers {\u0026quot;Content-Type\u0026quot;: \u0026quot;application/json\u0026quot;}. The returned value is a list of information for each entered postcode. Further guidance on sending bulk requests can be found in the postcodes.io documentation.\nGetting the Postcodes Data Considering the existence of more than 2,000 schools and the limitation of the bulk request to accept only 100 postcodes at a time, it became necessary for me to iterate through the schools in batches of 100. The subsequent code effectively manages this task. Prior to this, I had prepared a dictionary named \u0026ldquo;sch_loc_info\u0026rdquo; to accommodate the additional information that would be collected.\n# create a dictionary to store the collected data sch_loc_info = {\u0026#39;locs\u0026#39; : [], \u0026#39;cities\u0026#39; : [], \u0026#39;zones\u0026#39; : []} # NOTE: When sending bulk postcodes, the API gate can only take 100 at a time. So, we request info 100 by 100 for i in range(100,len(postcodes),100): sch_loc_info_100 = PostcodeApi().get_bulk_pos_info(postcodes[i-100:i]) # request the info for each 100 call sch_loc_info[\u0026#39;locs\u0026#39;] += sch_loc_info_100[\u0026#39;locs\u0026#39;] # store the location info sch_loc_info[\u0026#39;cities\u0026#39;] += sch_loc_info_100[\u0026#39;cities\u0026#39;] # store the cities sch_loc_info[\u0026#39;zones\u0026#39;] += sch_loc_info_100[\u0026#39;zones\u0026#39;] # store the zones if (len(postcodes) - i) \u0026lt; 100: # include the final call which would have less than 100 postcodes. sch_loc_info_less = PostcodeApi().get_bulk_pos_info(postcodes[i:len(postcodes)]) sch_loc_info[\u0026#39;locs\u0026#39;] += sch_loc_info_less[\u0026#39;locs\u0026#39;] sch_loc_info[\u0026#39;cities\u0026#39;] += sch_loc_info_less[\u0026#39;cities\u0026#39;] sch_loc_info[\u0026#39;zones\u0026#39;] += sch_loc_info_less[\u0026#39;zones\u0026#39;] Building the Map The open-source folium library from Python was utilized to construct the map, with HTML being employed to enhance its features. The codes utilized to create the map are presented below and are accessible in the project’s GitHub repository. The map was generated using the code provided and was saved in an HTML format within a folder named ‘map’ in the present working directory.\n# Create a map centered at Edinburgh, the capital of Scotland m = folium.Map(location=[55.941457, -3.205744], zoom_start=6.5) # Create a custom color scale from light to dark blue colors = { 1: \u0026#39;#08306b\u0026#39;, # Dark blue (most deprived) 2: \u0026#39;#08519c\u0026#39;, 3: \u0026#39;#3182bd\u0026#39;, 4: \u0026#39;#63b7f4\u0026#39;, 5: \u0026#39;#a6e1fa\u0026#39; # Light blue (least deprived) } # length of the schools l = len(sch_df) # Create circles and digits for each data point for i in range(l): school = sch_df[\u0026#39;School Name_x\u0026#39;].iloc[i] # school name pos = sch_df[\u0026#39;Post Code\u0026#39;].iloc[i] # postcode loc = sch_df[\u0026#39;locs\u0026#39;].iloc[i] # location city = sch_df[\u0026#39;cities\u0026#39;].iloc[i] # city zone = sch_df[\u0026#39;zones\u0026#39;].iloc[i] # zone pupils = sch_df[\u0026#39;Total pupils\u0026#39;].iloc[i] # total pupils type = sch_df[\u0026#39;School Type\u0026#39;].iloc[i] # type of the school: secondary, primary, or special mag = dep_rates.get(pos, 3) # get the deprivation score as magnitude; 3 if the postcode is not assigned a score # add circle markers pointing each school folium.CircleMarker( location=loc, radius=(pupils/100 if pupils != 0 else 1), # radius equivalent to the total pupils count in each school color=colors[mag], fill=True, fill_opacity=0.8, ).add_to(m) # create an HTML pop-up for each school popup_html = f\u0026#34;\u0026#34;\u0026#34; \u0026lt;h3\u0026gt;{school}\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Type:\u0026lt;/strong\u0026gt; {type}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Local Authority:\u0026lt;/strong\u0026gt; {city}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Zone:\u0026lt;/strong\u0026gt; {zone}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Pupils:\u0026lt;/strong\u0026gt; {pupils if pupils != 0 else \u0026#39;N/A\u0026#39;}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Deprivation:\u0026lt;/strong\u0026gt; {mag}\u0026lt;/p\u0026gt;\u0026#34;\u0026#34;\u0026#34; folium.Marker( location=loc, popup=folium.Popup(popup_html, max_width=150), icon=folium.DivIcon(html=f\u0026#39;\u0026lt;div style=\u0026#34;width: 0px; height: 0px;\u0026#34;\u0026gt;\u0026lt;/div\u0026gt;\u0026#39;), ).add_to(m) # Create a custom HTML legend legend_html = \u0026#34;\u0026#34;\u0026#34; \u0026lt;div style=\u0026#34;position: fixed; top: 10px; right: 10px; background-color: white; padding: 10px; border: 2px solid black; z-index: 1000;\u0026#34;\u0026gt; \u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Legend\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span style=\u0026#34;color: black;\u0026#34;\u0026gt;\u0026lt;span style=\u0026#34;background-color: #08306b; width: 20px; height: 20px; display: inline-block;\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; 1 - Most Deprived\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span style=\u0026#34;color: black;\u0026#34;\u0026gt;\u0026lt;span style=\u0026#34;background-color: #08519c; width: 20px; height: 20px; display: inline-block;\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; 2\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span style=\u0026#34;color: black;\u0026#34;\u0026gt;\u0026lt;span style=\u0026#34;background-color: #3182bd; width: 20px; height: 20px; display: inline-block;\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; 3\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span style=\u0026#34;color: black;\u0026#34;\u0026gt;\u0026lt;span style=\u0026#34;background-color: #63b7f4; width: 20px; height: 20px; display: inline-block;\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; 4\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span style=\u0026#34;color: black;\u0026#34;\u0026gt;\u0026lt;span style=\u0026#34;background-color: #a6e1fa; width: 20px; height: 20px; display: inline-block;\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; 5 - Least Deprived\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026#34;\u0026#34;\u0026#34; m.get_root().html.add_child(folium.Element(legend_html)) # Create the folder to save the map cwd = os.getcwd() folder = os.path.join(cwd,\u0026#39;map\u0026#39;) os.makedirs(folder, exist_ok=True) # save the map as an HTML file m.save(os.path.join(folder, \u0026#39;scottish_schools_map.html\u0026#39;)) Upon clicking on a school displayed on the map, an HTML-generated pop-up menu will appear, providing relevant information about the school. Furthermore, an HTML-based legend has been created to display the range of deprivation. The code will save the file as \u0026lsquo;scottish_schools_map.html\u0026rsquo;. A screenshot of the map is presented in Image 1. The radius of the circles on the map is determined by the number of pupils in each school, with larger circles representing schools with a greater number of pupils. The intensity of the blue color indicates the level of deprivation, with darker markers indicating higher levels of deprivation in the area.\nImage 1: Map of Scotland locating all schools (screenshot from the HTML-based app)\nThe image presented below depicts the appearance of a pop-up menu upon clicking on a school icon (Image 2). The menu exhibits pertinent information such as the school type, local authority, zone, pupils count, and deprivation score.\nImage 2: A pop-up information menu of Leith Academy (screenshot from the HTML-based app)\nEnd The objective of this project was to construct a map that would visually represent schools in Scotland, taking into account the number of students and deprivation score. The map was developed using the folium module. The codes and map are available for unrestricted use without my authorization. Please leave a comment if you have any inquiries or recommendations. The map could be enhanced with additional HTML, and possibly Javascript integration.\nFor information on implementing k-Means clustering to classify school data, please refer to my other article via this link.\nFor potential collaborations, please contact me at ezeynalli@hotmail.com.\n","permalink":"http://localhost:1313/posts/scottish_schools_map/","summary":"\u003cp\u003e\u003cstrong\u003eProject description\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eLanguage:\u003c/em\u003e Python, HTML\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eLibraries:\u003c/em\u003e folium, pandas, numpy, requests, json\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eIDE:\u003c/em\u003e Microsoft Visual Studio Code, Jupyter Notebook\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eProject type:\u003c/em\u003e Data analytics, Web scraping, Map formation, API\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eKeywords:\u003c/em\u003e Scotland, schools, deprivation, map\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cimg alt=\"Photo by ali elliott on Unsplash\" loading=\"lazy\" src=\"/images/scottish_state_schools_map/1.webp\"\u003e\u003cbr\u003e\n\u003cem\u003ePhoto by ali elliott on Unsplash\u003c/em\u003e\u003c/p\u003e\n\u003ch2 id=\"postcodes-api\"\u003ePostcodes API\u003c/h2\u003e\n\u003cp\u003eThe available contact information for the schools solely consists of their postcodes and seedcodes. Consequently, in order to accurately locate each school on a map, I required latitude and longitude values for each establishment. In my search for geolocation information, I discovered an open platform called \u003ca href=\"https://postcodes.io/\"\u003ePostcodes IO\u003c/a\u003e. This platform offers a free API that allows access to the server and retrieval of geolocation information for every UK postcode. The collected data was presented in a JSON format. To obtain a comprehensive dataset from postcodes.io, I utilized various endpoints of the API, including the following:\u003c/p\u003e","title":"Analysis of State Schools in Scotland: Map of Scotland with Schools"},{"content":"The objective of this project is to conduct a comprehensive analysis of Scottish schools in order to derive valuable insights. The analysis includes clustering and descriptive analysis, taking into account factors such as deprivation rate and total number of pupils. Additionally, an interactive map of Scotland has been developed to visually represent the location of each school. The clustering model has categorized local authorities into three distinct clusters based on average pupil count and deprivation score. The map highlights areas predominantly occupied by schools facing high levels of deprivation. The aim is to assist charities or non-governmental organizations (NGOs) involved in projects supporting these school pupils. All data used in this analysis is publicly available and sourced from the Scottish Government website. The Postcodes.io API was utilized to gather latitude and longitude coordinates for each school based on their respective postcodes. The codes for clustering, visualizations, map generation, and API application can be found in my GitHub repository. You are welcome to access and utilize the repository\u0026rsquo;s contents for personal or commercial purposes without seeking my consent. I hope you find this information enlightening and enjoyable to read.\nProject description\nLanguage: Python Libraries: sklearn, pandas, numpy, matplotlib, seaborn, os IDE: Microsoft Visual Studio Code, Jupyter Notebook Project type: Machine learning, unsupervised learning, k-means clustering, data analytics, data visualisation Photo by Adam Wilson on Unsplash\nThe educational institutions in Scotland hold significant influence over the country\u0026rsquo;s economy. Scotland\u0026rsquo;s socio-demographic governmental structure places substantial emphasis on investing in the education of its population, as evidenced by the provision of free meals in all state schools. The economic status of local areas is largely determined by the location and prestige of state schools, with real estate prices being particularly dependent on this factor. Therefore, the level of deprivation in schools is a crucial determinant of areas that require further attention from either the government or non-governmental organizations. In this project, my objective is to assist concerned institutions in gaining a better understanding of the distribution of schools and the local authorities to which they are assigned.\nI have undertaken two projects for Scottish schools, one of which involves the implementation of K-Means Clustering. The other project involves the creation of a map of Scotland that displays the schools, which can be accessed via this link or my Medium page (soon).\nSchools Datasets Description All data pertaining to the schools for this project has been sourced from the official statistics website of the Scottish Government. The consolidation of information regarding Scottish schools is based on three datasets:\nPostcode_deprivation: This dataset encompasses postcodes in Scotland along with their corresponding deprivation scores. Both pieces of information are utilized in this project. The dataset can be officially accessed from the following link. Scottish_schools_contact: This dataset comprises contact information for Scottish schools, including their respective postcodes. The project utilizes the postcode and seedcode associated with each school. The dataset can be officially accessed from the following link. Scottish_schools_stats: This dataset contains demographic statistics for Scottish schools. It has been formulated by merging various datasets. The data can be officially accessed from the following link. Analysis Descriptive analysis Initially, a descriptive analysis was conducted on the collected information from schools. Subsequently, the datasets were merged and null values were dropped, resulting in a conclusion of 2,431 schools (out of 2,458 state schools in Scotland) with a total of 692,729 pupils. The average number of pupils per school was found to be 284.9, with a standard deviation of 309.7. The school with the highest number of pupils had 2,226, while the lowest school had no pupils at present. In terms of deprivation quintile, the average score was 2.8, with a standard deviation of 1.2. The number of schools falling under each deprivation quintile score is provided below:\n1 quintile — 448 school 2 quintile — 507 school 3 quintile — 634 school 4 quintile — 557 school 5 quintile — 285 school Figure 1: Total number of pupils and schools per school type\nThe data presented in Figure 1 illustrates the distribution of schools and pupils across various school types. It is evident that primary schools hold the majority in terms of both school count and pupil enrollment. Notably, the number of secondary schools is disproportionately low in comparison to the significant number of pupils. Special schools represent a minority within the overall distribution.\nFigure 2: TOP10 Scottish local authorities with the highest number of pupils and schools\nFigure 2 displays the pupil and school statistics for the top 10 local authorities. Notably, Glasgow city, Edinburgh city, and Fife emerge as the frontrunners in terms of pupil population, whereas Highland, Glasgow, and Aberdeenshire dominate the representation of schools. A noteworthy observation is the significant concentration of pupils in Edinburgh, the capital city of Scotland, despite the relatively lower number of schools. Conversely, Highland boasts the highest number of schools, despite having a comparatively smaller pupil population.\nUnsupervised Learning: Clustering Following the descriptive analytics, a discernible pattern emerged, enabling the categorization of local authorities based on their average deprivation score and average pupil count. To accomplish this, I proceeded to develop a data clustering algorithm utilizing unsupervised machine learning models. After careful consideration, I selected the k-Means clustering method due to its extensive applicability and widespread usage. Prior to implementing the clustering technique, I performed an initial data transformation to ensure optimal results.\nPreprocessing before Clustering Due to the significant disparity in numerical values observed, with deprivation scores ranging from 1 to 5 and total pupil counts ranging from 0 to over 2000, I made the decision to normalize these values in order to enhance the efficiency of the clustering process. To accomplish this, I employed the StandardScaler object from the sklearn module.\n# Import standard scaler to z-score normalize the data from sklearn.preprocessing import StandardScaler # specify the numerical features numeric_columns = [\u0026#39;Pupils\u0026#39;, \u0026#39;DeprivationScore\u0026#39;] # Create a scaler object scaler = StandardScaler() # Scale the numeric columns scaled_values = scaler.fit_transform(df[numeric_columns]) # Sonvert the numpy array of scaled values into a dataframe scaled_values = pd.DataFrame(scaled_values, columns = numeric_columns) Finding k: the optimal number of clusters When considering k-Means clustering, a crucial step is determining the number of clusters (k), which must be predetermined. However, identifying the optimal k is not a straightforward process and requires both domain knowledge and technical expertise. Given the lack of personal expertise in the school industry, I chose to rely on my technical knowledge. To ensure the accuracy of k, I employed two different methods to determine the best value. To achieve this objective, I utilized the Within-Cluster Sum of Squares (WCSS) and Average Silhouette Method.\nWithin-Cluster Sum of Squares method The WCSS method assesses the variations within each cluster by summing the squared differences between values. By testing different k values, it compares the differences in WCSS scores and selects the value at which the subsequent scores stabilize. The code implementation for this method is as follows:\n# Create a list to store WCSS values wcss = [] # Iterate in a range from 2 to 10, inclusive for k in range(2, 11): km = KMeans(n_clusters = k, n_init = 25, random_state = 1234) # Create a cluster object for each k km.fit(scaled_values) # Fit the scaled data wcss.append(km.inertia_) # Add the inertia score to wcss list # Convert the wcss list into a pandas series object wcss_series = pd.Series(wcss, index = range(2, 11)) # Draw a line chart showing the inertia score, or WCSS, for each k iterated plt.figure(figsize=(8, 6)) ax = sns.lineplot(y = wcss_series, x = wcss_series.index) ax = sns.scatterplot(y = wcss_series, x = wcss_series.index, s = 150) ax = ax.set(xlabel = \u0026#39;Number of Clusters (k)\u0026#39;, ylabel = \u0026#39;Within Cluster Sum of Squares (WCSS)\u0026#39;) The provided codes generate Figure 3, which allows for the observation that the scores exhibit minimal variation beyond the value of 3. Consequently, it can be inferred that 3 is the optimal k, as determined through the utilization of the WCSS method.\nFigure 3: Within-Cluster Sum of Squares method results\nHowever, this cannot be the sole basis for formulating three clusters. Therefore, it is necessary to run the Silhouette method as well.\nSilhouette method The Silhouette method measures the similarity of a data point within its own cluster in comparison to other clusters. For each data point, it calculates its distance from its local points (a) and neighboring points (b). It then applies the formula (b-a)/max(a,b) to calculate the Silhouette score. The average Silhouette scores of all the points within one cluster determine the final score. A higher Silhouette value indicates better locality and thus a better cluster count. The code implementation for this method is as follows:\n# Import silhouette_score function from sklearn.metrics import silhouette_score # Create a list to store silhouette values silhouette = [] # Iterate in a range from 2 to 10, inclusive for k in range(2, 11): km = KMeans(n_clusters = k, n_init = 25, random_state = 1234) # Create a cluster object for each k km.fit(scaled_values) # Fit the scaled data silhouette.append(silhouette_score(scaled_values, km.labels_)) # Add the silhouette score to silhouette list # Convert the silhouette list into a pandas series object silhouette_series = pd.Series(silhouette, index = range(2, 11)) # Draw a line chart showing the average silhouette score for each k iterated plt.figure(figsize=(8, 6)) ax = sns.lineplot(y = silhouette_series, x = silhouette_series.index, color=\u0026#39;green\u0026#39;) ax = sns.scatterplot(y = silhouette_series, x = silhouette_series.index, s = 150, color=\u0026#39;green\u0026#39;) ax = ax.set(xlabel = \u0026#39;Number of Clusters (k)\u0026#39;, ylabel = \u0026#39;Average Silhouette Score\u0026#39;) Figure 4 below represents the outcome of the aforementioned codes, displaying the results of Silhouette testing. The chart unequivocally indicates that 3 possesses the highest average silhouette score. Consequently, this test execution ultimately determines 3 as the optimal value for k, as well.\nFigure 4: Silhouette method results\nAs both tests have indicated that the optimal value for k is 3, we can proceed with confidence to implement clustering with 3 clusters. The subsequent sub-section will construct the k-Means clustering and elucidate its outcomes.\nk-Means clustering In order to construct the model, the KMeans object from the sklearn library was utilized. The implementation of this object can be achieved through the following code:\n# Create kmeans object km = KMeans(n_clusters = 3, n_init = 25, random_state = 1234) # Fit the scaled values km.fit(scaled_values) The parameter \u0026rsquo;n_init\u0026rsquo; determines the number of times the algorithm will be executed with different initializations of cluster centroids. Increasing this value is recommended in order to achieve better clustering results. After conducting multiple tests, it was determined that a value of 25 is sufficient for this project. The \u0026lsquo;random_state\u0026rsquo; parameter is used to control the randomness or randomness seed for various operations involving randomness. It is advisable to set it to 1234 for most machine learning models. Among the three constructed clusters, Cluster 0 consists of 5 local authorities, Cluster 1 consists of 15 local authorities, and Cluster 2 consists of 13 local authorities.\nTable 1: Clustering results\ncluster Pupils DeprivationScore 0 438.29 3.50 1 331.48 2.39 2 175.87 3.31 The clustering results can be found in Table 1.\nCluster 0\nThis is the first cluster and is characterized by a relatively high number of pupils and a higher deprivation score. In other words, these are localities with a low level of deprivation but a large number of pupils. Despite the high number of pupils, these localities are relatively less deprived.\nCluster 1\nThis group of localities also has a large number of pupils, but the average deprivation score is the lowest among the three clusters. This indicates that the local authorities in this category require the most attention.\nCluster 2\nThe localities in this group have a lower number of pupils but a fair deprivation score. This group likely requires the least attention, as they are not highly deprived and the pupil count is not significant.\nThe scatter plot in Figure 5 illustrates the assignment of each locality to its respective cluster.\nZoom image will be displayed\nFigure 5: Scottish local authorities clustering\nThe complete list of localities in each cluster is provided below:\nCluster 0: Aberdeen City, East Dunbartonshire, East Lothian, East Renfrewshire, Edinburgh City Cluster 1: Clackmannanshire, Dundee City, East Ayrshire, Falkirk, Fife, Glasgow City, Inverclyde, Midlothian, North Ayrshire, North Lanarkshire, Renfrewshire, South Ayrshire, South Lanarkshire, West Dunbartonshire, West Lothian Cluster 2: Aberdeenshire, Angus, Argyll \u0026amp; Bute, Dumfries \u0026amp; Galloway, Grant aided, Highland, Moray, Na h-Eileanan Siar, Orkney Islands, Perth \u0026amp; Kinross, Scottish Borders, Shetland Islands, Stirling Conclusion I had two primary objectives in undertaking this project:\nPersonal development — My aim was to showcase my abilities and enhance my career prospects by implementing similar projects and gaining valuable experience. Educational exploration — As an international citizen of Scotland, I was keen to deepen my understanding of the country\u0026rsquo;s educational system. Additionally, I am actively involved in a charity organization that carries out projects for Scottish schools. If you found this project insightful and beneficial, I encourage you to show your appreciation by clapping and sharing it with your peers. Furthermore, feel free to utilize the project for your personal and professional purposes without seeking my consent, as I am a strong advocate of open-source projects and actively contribute to the community.\nShould you have any questions, concerns, or suggestions, please do not hesitate to comment below or contact me via email at ezeynalli@hotmail.com.\n","permalink":"http://localhost:1313/posts/scottish_schools_clustering/","summary":"\u003cp\u003eThe objective of this project is to conduct a comprehensive analysis of Scottish schools in order to derive valuable insights. The analysis includes clustering and descriptive analysis, taking into account factors such as deprivation rate and total number of pupils. Additionally, an interactive map of Scotland has been developed to visually represent the location of each school. The clustering model has categorized local authorities into three distinct clusters based on average pupil count and deprivation score. The map highlights areas predominantly occupied by schools facing high levels of deprivation. The aim is to assist charities or non-governmental organizations (NGOs) involved in projects supporting these school pupils. All data used in this analysis is publicly available and sourced from the Scottish Government website. The Postcodes.io API was utilized to gather latitude and longitude coordinates for each school based on their respective postcodes. The codes for clustering, visualizations, map generation, and API application can be found in \u003ca href=\"https://github.com/mrzeynalli/scotland_schools.git\"\u003emy GitHub repository\u003c/a\u003e. You are welcome to access and utilize the repository\u0026rsquo;s contents for personal or commercial purposes without seeking my consent. I hope you find this information enlightening and enjoyable to read.\u003c/p\u003e","title":"Analysis of State Schools in Scotland: K-Means Clustering by Deprivation Rate and Pupils Quantity"},{"content":"In this article, I’m going to explain the concept of a web crawler, how search engines work, and guide you in building a simple Crawler bot. All the code I’ve written for this can be found on my GitHub repository, and feel free to use, modify, or replicate it for personal or commercial purposes without needing my consent.\nProject description\nLanguage: Python Libraries: request, bs4, regex, os, json IDE: Microsoft Visual Studio Code Project type: Web crawling/scraping Photo by Timothy Dykes on Unsplash\nAs the name suggests, a web crawler is an application that explores the web like a spider, gathering the desired information. Well-known search engines like Google, Bing, and Yahoo have incredibly fast crawlers that navigate the internet in a matter of seconds (although they don’t crawl the entire web all the time, they minimize the number of web pages to consider using indexing). The good news is, you too can create your own web crawler using Python, and it only requires around 100 lines of code (actually, it’s exactly 100 lines!).\nWeb Crawler First, let’s break down the process into smaller steps to understand how to build the crawling bot (“First Principles”). The steps the bot needs to follow are as follows:\nAccess the webpage Request its content (HTML source code) Analyze the content and gather internal links Extract text information from each link Repeat until a pre-defined threshold is reached To perform these steps, we need to install/import specific Python libraries. For the first two steps, we will use the requests library, which allows us to send a request to a webpage to get its content. Next, we will use the bs4 library to help us understand the collected content and parse it for extracting the desired information, which in this case is the internal links and their text content. The bot will also follow these steps for the collected links. Now, let’s delve into the technical details of building the bot.\nWebCrawler Class We need to create a class named “WebCrawler” and define its parameters:\n# build the web crawler object class WebCrawler: # create three variables: start_url, max_depth, list of visited urls def __init__(self, start_url, max_depth=2): self.start_url = start_url self.max_depth = max_depth self.visited = set() The class requires two input parameters: start_url and max_depth, along with an initial parameter visited.\nstart_url — This is the main webpage we want to crawl (format: https://example.co.uk)/). max_depth — This indicates the depth of the crawling. Setting it to 1 will make our bot only follow the links collected from the starting URL. When set to 2, it will follow the links within those internal (from the initial URL) pages. The time complexity increases exponentially as the max_depth value increases. visited — This variable is a set that lists the URLs that have already been visited. It prevents the bot from revisiting the same link multiple times. Note that this variable is a set object rather than a list, which avoids adding duplicate entries (URLs). Checking URL Success Inside the WebCrawler class, we need to create another function to ensure that our request to the starting page is successful. This can be determined by checking the status code of the HTML request. Please note that a status code of 200 indicates a successful request.\n# create a function to make sure that the primary url is valid def is_successful(self): try: response = requests.get(self.start_url, timeout=20) # request the page info response.raise_for_status() # raises exception when not a 2xx response if response.status_code == 200: # check if the status code is 200, a.k.a successful return True else: # if not, print the error with the status code print(f\u0026#34;The crawling could not being becasue of unsuccessful request with the status code of {response.status_code}.\u0026#34;) except requests.HTTPError as e: # if HTTPS Error occured, print the error message print(f\u0026#34;HTTP Error occurred: {e}\u0026#34;) except Exception as e: # if any other error occured, print the error message print(f\u0026#34;An error occurred: {e}\u0026#34;) With this debugging approach, the crawling process will only be executed if the request is successful. Otherwise, it will either return the status code of an unsuccessful request or an HTTP or other encountered error. The timeout parameter determines the amount of time, in seconds, that the request attempt should wait before raising an error. It is different from the time.sleep(10) function. In this case, if the request is successful, the information will be retrieved immediately. However, if an error occurs, it will wait for 10 seconds before concluding an error. This is an important parameter to set up as sometimes the server may take a few seconds to respond.\nProcessing Pages The next step is to create a function that will enter a webpage and gather the links present as well as the text.\n# create a function to get the links def process_page(self, url, depth): # apply depth threshold if depth \u0026gt; self.max_depth or url in self.visited: return set(), \u0026#39;\u0026#39; # return empty set and string self.visited.add(url) # add the visited url to the set links = set() # create a set to store the collected links content = \u0026#39;\u0026#39; # create a variable to store the content extracted try: r = requests.get(url, timeout=10) # request the content of a url r.raise_for_status() # check if the request status is successful soup = BeautifulSoup(r.text, \u0026#39;html.parser\u0026#39;) # parse the content of the collected HTML # Extract the links anchors = soup.find_all(\u0026#39;a\u0026#39;) # find all the anchors for anchor in anchors: # merge the anchor with the starting url link = requests.compat.urljoin(url, anchor.get(\u0026#39;href\u0026#39;)) # get the link and join it with the starting url links.add(link) # add the link to the previously created set # Extract the content from the url content = \u0026#39; \u0026#39;.join([par.text for par in soup.find_all(\u0026#39;p\u0026#39;)]) # get all the text content = re.sub(r\u0026#39;[\\n\\r\\t]\u0026#39;, \u0026#39;\u0026#39;, content) # remove the sequence characters except requests.RequestException: # if the request encounters an error, pass pass return links, content # return the set of the collected links and the contet of the current url The function called process_page is inside the WebCrawler class and is responsible for collecting the link extensions found on a page, combining them with the primary URL, and extracting the text from each URL. Initially, it checks if the threshold (max depth, in this case) has been reached. If the depth is still below the predetermined limit, the crawling process continues. First, the bot adds the starting URL to the visited set and creates another set to collect the links found on that page. It is important to note that the links on a page do not follow a URL structure but are presented as link extensions (explained below). Therefore, it is crucial to merge them before appending them as final links.\nlink or URL — it begins with an HTTPS code and contains the complete reference structure (examples: https://example.co.uk, https://example.co.uk/contact) link extension — this refers only to the extension of the primary URL, essentially the part of the URL that comes after the “/” character (examples: /home, /contact, /careers) The function then utilizes the BeautifulSoup object from the bs4 library to parse the HTML source code of the webpage and scrape the page content. This content includes everything present on the page in text format, including the URL extensions. The bot searches for anchor elements within the soup by looking for ‘a’ tags, which contain the hyperlinks to other pages. After collecting the anchors, it iterates through each one to search for the ‘href’ tag, which contains the actual links. The compat.urljoin method from the request library is used to join the link with the URL and add the final URL to the set of links. Next, it looks for the ‘p’ tags, which contain plain text units, and joins them together within a particular page, storing the content in the content variable. To remove any escape sequences such as “/n”, “/r”, and “/t” (representing new line, return, and tab characters) and retain only the plain text, the sub method from the regex module (imported as re) is used to replace these sequences with an empty string. Finally, this process is debugged to handle any potential errors with internal links, in addition to the starting URL. Ultimately, this function returns the collected links and the text content of the requested URL.\nCrawling the Webpage The scraping function has been implemented, and now it’s time to build the crawling function. This function applies the scraping process to each of the links collected from a URL. Here is the code for this function:\n# crawl the web within the depth determined def crawl(self): if self.is_successful(): # check if the requesting the starting url info is valid to continue crawling urls_content = {} # create a dictionary to store the links as keys and contents as values urls_to_crawl = {self.start_url} # start crawling from the initial url # crawl the web within the depth determined for depth in range(self.max_depth + 1): new_urls = set() # create a set to store the internal new urls for url in urls_to_crawl: # crawl through the urls if url not in self.visited: # check and make sure that the url is not crawled before links, content = self.process_page(url, depth) # return the links and content of the crawled url urls_content[url] = content # add the url as a key and content as a value to the disctionary created previously new_urls.update(links) # add the internal links to the previously created set urls_to_crawl = new_urls # change the urls to crawl list to crawl through the internal links # create a folder to store the crawled websites current_dir = os.getcwd() # get the current working directory folder_dir = os.path.join(current_dir,\u0026#39;crawled_websites\u0026#39;) # create a folder inside the current directory if not os.isdir(folder_dir): # check if the folder already exists os.makedirs(folder_dir) # if not, create the folder directory filename = re.sub(r\u0026#39;\\W+\u0026#39;, \u0026#39;_\u0026#39;, self.start_url) + \u0026#39;_crawling_results.json\u0026#39; # format the filename to modify unsupported characters # save the results as a json file in the local directory with open(os.path.join(folder_dir,filename), \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as file: json.dump(urls_content, file, ensure_ascii=False, indent=10) # ensure to keep the unicode characters and indent to make it more readable return urls_content # return the disctionary storing all urls and their content The crawling process begins by checking the URL’s validity using the previously created function. If the check is successful, the function creates a dictionary to store the links along with their respective text content. It then iterates through the range of the maximum depth set in the object. For each depth, it creates a set to store the collected links to be iterated through subsequently. After confirming that the URL has not been visited before, it enters the current URL and uses the process_page function to extract all the internal links and text content. The new_urls set is then updated with the collected links, and each content is added to the dictionary with the internal link as the key. Once the crawling of a URL is complete, the newly collected URLs replace the urls_to_crawl, and the same steps are repeated for each of them.\nBefore returning the dictionary with lists as keys and content as values, the function also saves the information as a JSON file in a folder created in the current working directory. To accomplish this, the JSON and OS modules of Python are used. The folder name is defined and joined with the current directory using os.path.join. The function checks if the folder already exists with os.isdir(foldername). If it doesn’t exist, the function creates the folder and dumps the dictionary information into a JSON file within the created folder.\nCan I crawl the entire web? Photo by Timothy Dykes on Unsplash\nWhile it is technically possible to crawl a large portion of the clear web (unlike the dark web), it’s important to be aware of legal and ethical considerations. Some websites have specific permissions regarding web crawling, and not adhering to these permissions can lead to legal and ethical issues. Therefore, it’s always best practice to ensure proper authorization before proceeding with web crawling activities. You can usually find the necessary authorization information in the website’s “robots.txt” file. Take a moment to review this file and follow its guidelines.\nIt’s worth noting that the speed at which giant search engines like Google and Bing crawl the web is quite impressive and difficult to replicate. These search engines employ a vast amount of computational power and infrastructure to crawl and index web pages efficiently. They have dedicated teams of engineers and data centers around the world to support their crawling operations. Creating web crawlers that match the speed and efficiency of these search engines requires is practically impossible for an individual with a laptop in his/her basement as it requires substantial resources, including powerful servers, advanced algorithms, and a team of experts.\n","permalink":"http://localhost:1313/posts/python_web_crawler/","summary":"\u003cp\u003eIn this article, I’m going to explain the concept of a web crawler, how search engines work, and guide you in building a simple Crawler bot. All the code I’ve written for this can be found on \u003ca href=\"https://github.com/mrzeynalli/web_crawler/tree/main\"\u003emy GitHub repository\u003c/a\u003e, and feel free to use, modify, or replicate it for personal or commercial purposes without needing my consent.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cstrong\u003eProject description\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eLanguage:\u003c/em\u003e Python\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eLibraries:\u003c/em\u003e request, bs4, regex, os, json\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eIDE:\u003c/em\u003e Microsoft Visual Studio Code\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eProject type:\u003c/em\u003e Web crawling/scraping\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg alt=\"Photo by Timothy Dykes on Unsplash\" loading=\"lazy\" src=\"/images/web_crawler/1.webp\"\u003e\n\u003cem\u003ePhoto by Timothy Dykes on Unsplash\u003c/em\u003e\u003c/p\u003e","title":"Building a Web Crawler using Python"},{"content":"Project description\nLanguage: Python Working file: Microsoft Visual Studio Code Project type: Web Analytics Companies receive traffic to their website from various resources. The more a company learns about the visitors\u0026rsquo; traffic to its website, the better it understands the reasons behind their travel to the website. Besides the number of visitors to a website, it is important to learn from which sources those people arrive at the website. This can help the firms to see which external social account or network campaign is prone to bringing new customers. Furthermore, a company can also see the patterns in the behaviour of the visitors on different pages. By using various page analytics techniques, successful as well as problematic pages can be detected.\nPhoto by Adem AY on Unsplash\nIn this project, I used web analytics techniques to analyze the potential of three marketing campaigns run by a hypothetical company.\nData description and conversion Description The dataset consists of 160,000 rows, each corresponding to a different visit to a website and its clickstream — the pages the visitor entered. Each row also has corresponding \u0026lsquo;origin\u0026rsquo; and \u0026lsquo;platform\u0026rsquo; info. The \u0026lsquo;origin\u0026rsquo; value indicates through which source a user ended up on the website. Each origin value is one of the 7 different sources (the first three being paid champaigns):\nFacebook advertisement: When a user clicks on an ad that has been posted on Facebook. LinkedIn advertisement: When a user clicks on an ad that has been posted on LinkedIn. Partner advertisement: When a user clicks on an ad on a partner\u0026rsquo;s website. Facebook share: When a user clicks on a post that has been shared by a friend on Facebook. LinkedIn share: When a user clicks on a post that has been shared by a friend on LinkedIn. Direct: When a user directly types the website\u0026rsquo;s URL into their browser. Search: When a user finds the website by entering a search term into a search engine. The platform values are either \u0026lsquo;windows\u0026rsquo;, \u0026lsquo;mac\u0026rsquo;, \u0026lsquo;android\u0026rsquo;, \u0026lsquo;ios\u0026rsquo; or \u0026lsquo;unknown\u0026rsquo;. The latter is a result of the case in which the platform is not detected (due to various reasons such as the presence of an ad-blocker or VPN).\nConverstion The data is presented in a CSV file with two columns: the first indicates the origin and the second indicates the platform. The remaining columns to the right indicate the pages the user visited. Since not all rows have the same number of columns, a different data convention is needed for analysis. To achieve this, the function \u0026ldquo;generate_data_dict\u0026rdquo; takes a line as input and returns the values stored under their corresponding key in a dictionary. This function is used for each row in the dataset. After reading the dataset using the open() function in Python and using the readlines() method to convert each row into a line, I iterated through each row to obtain the values using the aforementioned function. The values for each visit were stored in a separate list created to contain all the visit info. After retrieving all visit values and closing the dataset file, I converted the list into a dataframe using the data argument, which was equal to the list. Since the list values were in a dictionary format, the keys were automatically constructed as columns, thus there was no need to add the columns argument separately. The data conversion codes are given below:\n# Define a function that converts a into a line and stores the values accordingly def generate_data_dict(line): line = line.rstrip(\u0026#34;\\n\u0026#34;) # convert the row into a line by stripping over a new-line character data_rows = line.split(\u0026#34;,\u0026#34;) # split the values in the row by comma chacater # Return the value for each feature in a dictionary format return {\u0026#39;Source\u0026#39;: data_rows[0], # the first value in a row - origin/source \u0026#39;Platform\u0026#39;: data_rows[1], # the second value in a row - platform \u0026#39;Clickstream\u0026#39;: data_rows[2:], # the remaining values in a row - pages visited \u0026#39;# of pages visited\u0026#39;: len(data_rows[2:]),} # the count of the pages visited ########################################################################################################## # Create a list that wil store the values for each visit visitor_data_list = [] with open(\u0026#39;visitor_data_clickstream.csv\u0026#39;, \u0026#39;r\u0026#39;) as file: # open the dataset file rows = file.readlines() # read each row in the dataset for row in rows: # iterate through the rows data_dict = generate_data_dict(row) # store the values for each row intoa dictionary visitor_data_list.append(data_dict) # add the values into a list file.close() # close the dataset file # Convert the visit values into a dataframe visitor_data_df = pd.DataFrame(data=visitor_data_list) Web analytics techniques Now that we have an intended dataframe of the clickstream on hand, it is time to start the analytics. But before, we need to discuss certain web analytics metrics. In practice, web analytics is usually carried out by observing a visitor\u0026rsquo;s behaviour over pages. Conversion, drop-out, and bounce rates were used in this project to analyze the pages.\nConversion rate shows the proportion of visitors that ended their visits successfully by carrying out a purchase (or any other intended outcome) Drop-out rate shows the proportion of visitors that entered the purchase page (or the processing page of any other intended outcome) but left without finishing Bounce rate shows the proportion of visitors that visited only one page and bounced away immediately Basically, each visit can be tagged if it has ended in one or more of the ways outlined above. I decided to add a separate column for each metric and tag 1 or 0, depending on whether the selected scenario (success, drop-out, bounce) happened in that particular visit. The below code shows how the columns and their indications are added to the dataframe:\npurchase_success_status_list = [] # create a list to visits that ended up in success drop_out_status_list = [] # create a list to visits that dropped out single_page_status_list = [] # create a list to visits that visited only one page # iterate through the indices of the dataframe for index in visitor_data_df.index: clickstream_list = visitor_data_df.loc[index][\u0026#39;Clickstream\u0026#39;] # seperate the clickstream value, i.e., pages visited if \u0026#39;purchase_success\u0026#39; in clickstream_list: # if the visit was successful, i.e., contains \u0026#39;purhcase_success\u0026#39; page purchase_success_status_list.append(1) # add 1 if YES else: purchase_success_status_list.append(0) # add 0 if NO if \u0026#39;purchase_start\u0026#39; in clickstream_list and \u0026#39;purchase_success\u0026#39; not in clickstream_list: # if the visit was dropped out, i.e., entrance into purchasing page without a success drop_out_status_list.append(1) # add 1 if YES else: drop_out_status_list.append(0) # add 0 if NO if len(clickstream_list) == 1: # if the visit contains only 1 page. i.e., a visitor bounced after a single page visit single_page_status_list.append(1) # add 1 if YES else: single_page_status_list.append(0) # add 0 if NO # add the respective list values into the dataframe visitor_data_df[\u0026#39;Conversion\u0026#39;] = purchase_success_status_list visitor_data_df[\u0026#39;Drop-out\u0026#39;] = drop_out_status_list visitor_data_df[\u0026#39;Bounce\u0026#39;] = single_page_status_list Simply, I created three lists for each metric and, by iterating through the indices of the dataframe, where an index is an individual visit, I checked if the clickstream of that visit had the relevant pages. Either 1, indicating a positive response, or 0, indicating a negative response, was added correspondingly.\nConversion — if the clickstream has a \u0026lsquo;purhcase_success\u0026rsquo; page, it means the visitor made a purchase successfully Drop-out — if the clickstream has a \u0026lsquo;purchase_start\u0026rsquo; page but not a \u0026lsquo;purhcase_success\u0026rsquo; one, it means the visitor entered the purchasing processing page but didn\u0026rsquo;t make a purchase for some reason Bounce — if the length of the clickstream is equal to 1, it means the visitor didn\u0026rsquo;t visit more than 1 page These columns only indicate whether an individual visit ended in one or more of the mentioned ways. Yet, I needed to calculate and see ratios for each platform or source. To achieve this, I created a function, which required three input values, numerator, denominator, and dataframe, and returned ratio values:\n# Create a function that returns a ratio for a given metric given numerator, denominator, and dataframe def generate_ratio(numerator, denominator, dataframe): ratios = [] # Create a list to store the ratio values for index in dataframe.index: # For each index numerator_value = dataframe.at[index, numerator] # Take the numeric from that index denominator_value = dataframe.at[index, denominator] # Take the denominator from that index ratio = numerator_value / denominator_value # Calculate the ratio ratios.append(round(ratio,2)) # Round the ratio into two decimal points return ratios # Return the ratios list However, I needed to have a dataframe with summed values for each metric (numerator), which then would be divided by the total visit (denominator). For this purpose, corresponding to my project\u0026rsquo;s two main analytical directions (analyzing Source and Platform), I used groupby() method of dataframe to create two new dataframe objects, indexed by their columns and summing their respective numeric values. Then, I added the total number of visits for each Source and Platform:\n# Groupyby all the values by source, summing the numeric values only source_group_df = visitor_data_df.groupby(\u0026#39;Source\u0026#39;).sum(numeric_only=True) # Create a list that stores the total number of visits per source number_of_visits_per_source = list(visitor_data_df.groupby(\u0026#39;Source\u0026#39;).count()[\u0026#39;Platform\u0026#39;]) # Add the total visits to the grouped by dataframe source_group_df[\u0026#39;Total Visits\u0026#39;] = number_of_visits_per_source ############################################################################################### # Groupyby all the values by platform, summing the numeric values only platform_group_df = visitor_data_df.groupby(\u0026#39;Platform\u0026#39;).sum(numeric_only=True) # Create a list that stores the total number of visits per platform number_of_visits_per_platform = list(visitor_data_df.groupby(\u0026#39;Platform\u0026#39;).count()[\u0026#39;Source\u0026#39;]) # Add the total visits to the grouped by dataframe platform_group_df[\u0026#39;Total visits\u0026#39;] = number_of_visits_per_platform Now we have two useful dataframe objects and a function to generate ratios, by clarifying the columns whose ratios will be calculated, I added the ratios of all three metrics (by dividing the sums of 1\u0026rsquo;s by the total visits) to each dataframe:\n# Create the list for columns whose ratios will be calculated list_of_columns_for_ratio_calculation = [\u0026#39;Conversion\u0026#39;, \u0026#39;Drop-out\u0026#39;, \u0026#39;Bounce\u0026#39;] # ADD RATIOS FOR THE DATAFRAME GROUPED BY SOURCE # Iterate through the columns list for column in list_of_columns_for_ratio_calculation: # Formulate the column name by adding ratio in the front column_name = column + \u0026#39; rate\u0026#39; # Generate the ratios and add to the dataframe under the formulated column name source_group_df[column_name] = generate_ratio(column, \u0026#39;Total Visits\u0026#39;, source_group_df) # ADD RATIOS FOR THE DATAFRAME GROUPED BY PLATFORM # Iterate through the columns list for column in list_of_columns_for_ratio_calculation: # Formulate the column name by adding ratio in the front column_name = column + \u0026#39; rate\u0026#39; # Generate the ratios and add to the dataframe under the formulated column name platform_group_df[column_name] = generate_ratio(column, \u0026#39;Total visits\u0026#39;, platform_group_df) Results Over tables Table 1: The Visit Statistics per Source\nSource # of pages visited Conversion Drop-out Bounce Total Visits Conversion rate Drop-out rate Bounce rate direct 47724 4275 1191 1287 13500 0.32 0.09 0.10 facebook_advert 19334 93 2728 4262 10000 0.01 0.27 0.43 facebook_share 177253 6657 12345 8125 51300 0.13 0.24 0.16 linkedin_advert 8292 504 896 0 2000 0.25 0.45 0.00 linkedin_share 71015 2546 4913 4239 21200 0.12 0.23 0.20 partner_advert 19034 545 3000 0 5000 0.11 0.60 0.00 search 188925 8720 12127 10537 57400 0.15 0.21 0.18 Table 1 presents comprehensive visit statistics that have been carefully filtered by source. The data shows the number of visitors, conversion, drop-out, and bounce rates for each source, giving you a complete picture of the traffic generated by different sources. This information can be used to optimize your marketing strategy by focusing on the sources that generate the most traffic and minimizing efforts on sources that underperform. Furthermore, the data can be analyzed to identify trends and patterns that can inform future marketing decisions. Overall, the data presented in Table 1 is a valuable resource for anyone looking to improve their website\u0026rsquo;s traffic and engagement metrics.\nTable 2: The Visit Statistics per Platform\nPlatform # of pages visited Conversion Drop-out Bounce Total visits Conversion rate Drop-out rate Bounce rate android 148686 6642 10003 8003 44500 0.15 0.22 0.18 ios 145639 5649 10969 8001 44500 0.13 0.25 0.18 mac 94268 4481 6296 4497 28000 0.16 0.22 0.16 unknown 48723 1976 3704 3341 15400 0.13 0.24 0.22 windows 94261 4592 6228 4608 28000 0.16 0.22 0.16 Observing Table 2, we can see the visit statistics filtered by platform. It\u0026rsquo;s important to note that this information can provide us with valuable insights into user behaviour and preferences, which we can then use to inform future decision-making. For example, if we notice that a certain platform has significantly more visits than others, we may want to consider investing more resources into that platform to maximize our reach and engagement. Alternatively, if we notice that a certain platform has a high bounce rate, we may want to investigate why that is and make adjustments to improve the user experience. By taking a more nuanced approach to analyzing these visit statistics, we can gain a deeper understanding of our audience and optimize our strategies accordingly.\nOver graphs I analyzed the traffic to the website and found significant differences in visits, conversion rates, and bounce rates among the three advertisement campaigns. LinkedIn had the highest conversion rate at 25%, followed by Partner Websites at 11% and Facebook at 1%.\nThe bounce rate for Facebook was 43%, while the other two campaigns had no such visitors. Interestingly, visitors from shared posts on social media and search engines generated more traffic than the advertisements. Direct traffic had the highest conversion rate at 32%, followed by search engine visitors at 15%, and shared posts on Facebook and LinkedIn at 13% and 12%, respectively.\nMobile visitors were much more numerous than desktop visitors, but desktop visitors had a slightly higher conversion rate and lower bounce rate. IOS platform visitors had a 25% drop-out rate, while other platforms had a 22% rate (except for unknown visitors at 24%). Visitors had difficulty navigating beyond the \u0026lsquo;Blog 1\u0026rsquo; page, with a 10% bounce rate for desktop users and a 6% bounce rate for IOS and Android users. The \u0026lsquo;Home\u0026rsquo; page had a 9% bounce rate for desktop users and a 6% bounce rate for IOS and Android users. Blog 1 was the most challenging page for visitors, but 24% of visitors who completed a successful purchase had read it. Blog 2 had a 19% success rate.\nTo increase conversion rates and reduce bounce rates, the company should invest more in LinkedIn advertisements and improve its search engine optimization. It should also improve the layout of \u0026lsquo;Blog 1\u0026rsquo; and \u0026lsquo;Home\u0026rsquo; pages, especially for Windows and Mac users. Additionally, it should write more blogs similar to the content of Blog 1 to contribute to higher conversion rates.\nKey takeaways: LinkedIn had the highest conversion rate at 25%. Mobile visitors were much more numerous than desktop visitors. Direct traffic had the highest conversion rate at 32%. Blog 1 was the most challenging page for visitors but was read by 24% of successful purchasers. The company should invest more in LinkedIn advertisements and improve search engine optimization to increase conversion rates and reduce bounce rates. ","permalink":"http://localhost:1313/posts/web_analytics_clickstream/","summary":"\u003cp\u003e\u003cstrong\u003eProject description\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eLanguage:\u003c/em\u003e Python\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eWorking file:\u003c/em\u003e Microsoft Visual Studio Code\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eProject type:\u003c/em\u003e Web Analytics\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003eCompanies receive traffic to their website from various resources. The more a company learns about the visitors\u0026rsquo; traffic to its website, the better it understands the reasons behind their travel to the website. Besides the number of visitors to a website, it is important to learn from which sources those people arrive at the website. This can help the firms to see which external social account or network campaign is prone to bringing new customers. Furthermore, a company can also see the patterns in the behaviour of the visitors on different pages. By using various page analytics techniques, successful as well as problematic pages can be detected.\u003c/p\u003e","title":"Web Analytics: Analyzing clickstream of 160,000 visitors"},{"content":"This project is carried out by me independently. The dataset is obtained publicly from Kaggle. All codes and their explanations are stored in my GitHub repository.\nProject description\nLanguage: Python Libraries: sklearn, pandas, numpy, matplotlib, seaborn IDE: Jupyter notebook Project type: Machine learning, Unsupervised learning, K-Means clustering FIFA 23 is a football video game created by Electronic Arts (EA). It became the best-selling football video game in Christmas UK retail charts. According to EA statistics, the game contains more than 700 teams with over 19,000 football players, playing in at least 30 football leagues. The data used in this project is taken from Kaggle. The objective of this project is to classify the players into various segments.\nPhoto by Hatem Boukhit on Unsplash\nAlthough I implemented this project just out of my interest in football and my general engagement in playing the FIFA video game, the results may be helpful for any external observer such that the valuation of players might be determined or justified based on the segment they are in. Additionally, a team\u0026rsquo;s success may be tested over the number of players from each segment with the purpose of finding out any correlation.\nData collection and cleaning The dataset used initially contained 17,660 rows and 29 columns. In other words, 17,600 players were present in the dataset explained with 29 features. The features were both quantitative and qualitative, ranging from the player\u0026rsquo;s name to their market value. I first used Pandas to call the raw format of the CSV file from the GitHub repo. After careful observation of the dataset, I noticed that some players, who already retired from a club, are still shown as the players of that particular club. I decided to eliminate those players in order to have the most up-to-date statistics. To tackle this problem, an interesting point in the dataset helped me.\nThe already-retired players\u0026rsquo; names start with their latest kit number (\u0026ldquo;15 Xavi\u0026rdquo;, \u0026ldquo;22 D. Alves\u0026rdquo;). This enabled me to separate those players by applying an algorithm to detect the player names that start with digits. The codes are as follows:\n# This list object will store the indices of players whose names start with digit player_indices_to_remove = [] # This loop iterates through the indices of all players to detect the ones with digit-starting player names for index in fifa23_df.index.to_list(): player = fifa23_df[\u0026#39;Name\u0026#39;].loc[index] player_first_name = fifa23_df[\u0026#39;Name\u0026#39;].loc[index][0] # If the player name starts with digit, it adds the index of that observation to removable indices list if player_first_name.isnumeric(): player_indices_to_remove.append(index) # Now, we drop the indices of players who are unnecesarily included in the dataset fifa23_df.drop(player_indices_to_remove,axis=0,inplace=True) # We reset the indices of the dataframe fifa23_df.reset_index(drop=True,inplace=True) The codes go over each player\u0026rsquo;s name by iterating through the indices of the observations, taking out the very first character of that name, and checking if that character is numeric. Later, indices of the positive cases are added to the previously created list that aims to store retired players. Subsequently, those players are dropped out of the dataset, and the indexing is re-set.\nAfter tackling this problem, I faced yet another data problem. The numeric values indicating monetary values are put in with their respective currency and the prefixes, M and K, for millions and thousands, respectively. I needed to first eliminate the currency symbol and convert the values into their actual value. I formulated the below function that handles this specific duty:\ndef curreny_correction(column,dataframe, curreny_sign = str): # Split the value by the given currency symbol: euro in our instance splits = dataframe[column].str.split(curreny_sign, expand = True)[1] values = splits.str[:-1] # Store the values prefixes= splits.str[-1:] # Store the prefixes # Create a list object that will store the float-converted format of the values values_float = [] for value, prefix in zip(values, prefixes): # The wage and value point are either in thousands (K) or millions (M) or 0 if prefix == \u0026#39;M\u0026#39;: # Checks if letter is \u0026#39;M\u0026#39; or the value is million try: # When values are zero, they cannot be converted into float and raises ValueError. #I debug the coding for those occasions float_value = float(value) * 1000000 values_float.append(float_value) except ValueError: # Adds just 0 when ValueError is raised float_value = 0 values_float.append(float_value) elif prefix == \u0026#39;K\u0026#39;: # If the letter is \u0026#39;K\u0026#39; or the value is thousands try: # When values are zero, they cannot be converted into float and raises ValueError. #I debug the coding for those occasions float_value = float(value) * 1000 values_float.append(float_value) except ValueError: # Adds just 0 when ValueError is raised float_value = 0 values_float.append(float_value) # Returns the float values that are stripped of currency symbol and exponential letters return values_float Features including \u0026ldquo;Value\u0026rdquo;, \u0026ldquo;Wage\u0026rdquo;, and \u0026ldquo;Release Clause\u0026rdquo; were converted using the above function. Different scripts were written to convert the features (\u0026ldquo;Position\u0026rdquo;, \u0026ldquo;Height\u0026rdquo;, \u0026ldquo;Weight\u0026rdquo;) with slightly different characters.\n# Correcting \u0026#39;Position\u0026#39; feature fifa23_df[\u0026#39;Position\u0026#39;] = fifa23_df[\u0026#39;Position\u0026#39;].str.split(\u0026#39;\u0026gt;\u0026#39;, expand=True)[1] # Correcting \u0026#39;Height\u0026#39; feature heigh_values = [float(value) for value in fifa23_df[\u0026#39;Height\u0026#39;].str.split(\u0026#34;cm\u0026#34;, expand = True)[0]] fifa23_df[\u0026#39;Height\u0026#39;] = heigh_values # Correcting \u0026#39;Weight\u0026#39; feature weight_values = [float(value) for value in fifa23_df[\u0026#39;Weight\u0026#39;].str.split(\u0026#34;kg\u0026#34;, expand = True)[0]] fifa23_df[\u0026#39;Weight\u0026#39;] = weight_values I eliminated 11 features based on whether they are non-useful for the analysis or have huge null values. The final dataset ready for the analysis contained 17 features and 10,104 observations.\nData visualization I created a couple of pre-analytics graphs to get a better understanding of the data I am working with. Seaborn and matplotlib modules were used for visualization purposes.\nFigure 1: Overall Rating Score of players per their Preferred Foot Figure 1 displays how the players are distributed on their rating score based on their preferred foot. There seems to be not a big difference between the feet. However, a very slight superiority can be observed for the left foot (presumably, because of Leo Messi the GOAT).\nFigure 2: Scatter plot of Overall Rating Score and Age When it comes to the relationship between age and overall rating, an apparent positive linear relationship is visible (Figure 2). Seemingly, the greater the age of the player, the higher the rating is. The relationship seems to fade away after the age of 35.\nFigure 3: Correlation matrix The correlation among the numeric variables of the dataset can be seen in Figure 3. High correlation scores are visible between Wage and Value, Height and Weight, Value and Overall, and Value and Potential. Although Overall Rating and Age are highly positively correlated, interestingly, there is not much correlation between Potential and Age. This signals that the young players who do not have high overall at the moment can increase their rating score substantially.\nk-Means clustering Initially, the numeric features need to be scaled, given the fact that the presence of outliers along with huge variations among the ranges of different variables can negatively impact the k-Means clustering process. I used StandardScaler from sklearn.preprocessing to standardize the quantitative variables.\nFor specifying the best number of clusters, I carried out Within-Cluster Sum of Squares (WCSS) and Average Silhouette methods.\nFigure 4: WSCC method WSCC method calculates the cluster differences for each cluster number. By observing the above plot (Figure 4) for each k, it can be observed that the variations tend to slow down after around 5. Thus, the graph signals 5 to be the best number of clusters.\nFigure 5: Average Silhouette method In the average silhouette method, the distances between neighboring cluster items and local cluster items are calculated. Figure 5 shows the final average silhouette scores for each k cluster number. Accordingly, the highest score signals the best k (besides 2).\nAs a result, I have enough proof to use 5 as my cluster number.\nProject results: Final clusters I used the kMeans function from sklearn.cluster to do the clustering. 5 different clusters were formed for the players in the dataset. The clusters and the number of players in each cluster are the following:\nCluster 0 has 2640 players Cluster 1 has 3228 players Cluster 2 has 903 players Cluster 3 has 3197 players Cluster 4 has 136 players Table 1: Cluster results\ncluster Age Overall Potential Value Wage Height Weight Release Clause 0 20.47 56.40 67.42 386295.45 44692.80 180.29 72.88 7.700856e+05 1 23.80 67.22 73.81 2275497.21 14163.88 175.94 69.68 4.378210e+06 2 26.35 78.79 81.78 19393023.26 49131.78 182.18 75.93 3.738128e+07 3 25.44 66.95 72.08 1892169.22 15918.36 187.44 81.06 3.557997e+06 4 26.29 85.38 88.04 67459558.82 147213.24 182.27 76.98 1.309640e+08 Table 1 demonstrates how the players are separated based on the clusters.\nCluster 0 is characterized by younger players, who have relatively low rating scores and potential. Their values and wages are low, correspondingly. They are the kind of football players, who play in and are transferred by middle-sized clubs. Yet, they usually have a high release clause because of their young age.\nCluster 1 accommodates young and middle-aged players with yet low rating scores. These players usually start and end their careers in small- and middle-sized teams. However, given their still young age, they have a high release clause, as well.\nCuster 2 has, on average, the oldest players among the clusters. The values and wages the players of this category have are fairly large, which signals their importance in the team. They mostly play in middle-sized and big teams, given their high values. Given their height and rating, I reckon that these are the strikes that play a crucial role in the attack.\nCluster 3 seems to have the same type of players as cluster 2. The difference is that cluster 3 players play in small- and middle-sized teams, explained by their low value and wage as well as low rating score and potential. They are quite tall and key players on the attack.\nCluster 4 takes the best players in the arena. They are still performing high and are the key players in their big teams. Arguably, they have been succeeding on their teams for quite a long time. Now, as they are old, their release clauses are also very low. Messi, Ronaldo, and Lewandowski should be in this cluster.\n","permalink":"http://localhost:1313/posts/fifa23_players_analysis/","summary":"\u003cp\u003eThis project is carried out by me independently. The dataset is obtained publicly from Kaggle. All codes and their explanations are stored in \u003ca href=\"https://github.com/mrzeynalli/fifa_23_players_analysis\"\u003emy GitHub repository\u003c/a\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cstrong\u003eProject description\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eLanguage:\u003c/em\u003e Python\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eLibraries:\u003c/em\u003e sklearn, pandas, numpy, matplotlib, seaborn\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eIDE:\u003c/em\u003e Jupyter notebook\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eProject type:\u003c/em\u003e Machine learning, Unsupervised learning, K-Means clustering\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFIFA 23 is a football video game created by Electronic Arts (EA). It became the best-selling football video game in \u003ca href=\"https://www.gamesindustry.biz/fifa-is-christmas-no1-as-god-of-war-drops-to-third-place-uk-boxed-charts#:~:text=FIFA%2023%20was%20the%20best,a%2013%25%20boost%20in%20sales.\"\u003eChristmas UK retail charts\u003c/a\u003e. According to \u003ca href=\"https://www.ea.com/games/fifa/news/fifa-23-all-leagues-clubs-teams-list#:~:text=With%20more%20than%2019%2C000%20players,%2C%20Bundesliga%2C%20LaLiga%20Santander%2C%20CONMEBOL\"\u003eEA statistics\u003c/a\u003e, the game contains more than 700 teams with over 19,000 football players, playing in at least 30 football leagues. The data used in this project is taken from \u003ca href=\"https://www.kaggle.com/datasets/bryanb/fifa-player-stats-database\"\u003eKaggle\u003c/a\u003e. The objective of this project is to classify the players into various segments.\u003c/p\u003e","title":"FIFA23 Players Analysis: k-Means Clustering"},{"content":"Project description\nLanguage: Python Libraries: requests, pandas, numpy, matplotlib, seaborn, os, math, time, datetime, json IDE: Microsoft Visual Studio Code, Jupyter Notebook Project type: Data analytics, Web scraping, API Companies House is an agency formed by the British Government to maintain the registration of all the companies in the UK. It maintains a database that stores the information of the registered companies. Each company has features such as company name, Standard Industrial Classification (SIC) code, creation date, cessation date (if ceased operating), company board and shareholder info, etc. By using an API, it is possible to scrape the data from that database by using various specifications.\nThis project aims to scrape the data of the tech companies in the UK and figure out the main tech areas (cities) besides London, the capital of the UK. The idea is to decentralize the tech processes from the capital city. The results of this project can be beneficial for people seeking low-competitive employment in the tech sphere or investors seeking conservative investment options in the UK tech arena.\nPhoto by Rodrigo Santos on Unsplash\nData Collection and Cleaning The project uses the requests library to request data from Companies House. The scraped data is converted into json format and combined into a single dataframe. The following Python libraries are used:\nimport pandas as pd import numpy as np import requests as rq import json import math import time import datetime as dt import matplotlib.pyplot as plt import seaborn as sns import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) A class-based API caller function was built to collect company data and return it as a dataframe:\nclass api_caller: root_url = \u0026#39;https://api.companieshouse.gov.uk/\u0026#39; key = \u0026#34;YOUR_API_KEY\u0026#34; def return_dataframe(self, url_extention): url = self.root_url + url_extention query_result = rq.get(url, auth=(self.key,\u0026#39;\u0026#39;)) if query_result.status_code == 200: json_file = json.JSONDecoder().decode(query_result.text) items_file = json_file[\u0026#39;items\u0026#39;] keys = items_file[0].keys() companies_df = pd.DataFrame(items_file, columns = keys) return companies_df else: return None Full SIC code lists were scraped and processed. A CSV file of SIC codes is available on my GitHub.\nI created functions to query companies based on single or multiple SIC codes, as well as functions to process address data and extract \u0026rsquo;locality\u0026rsquo; and \u0026lsquo;postal_code\u0026rsquo; fields from nested dictionaries.\nData Analysis I began by extracting SIC codes for tech-related industries using keywords like \u0026rsquo;technology\u0026rsquo;, \u0026rsquo;engineering\u0026rsquo;, \u0026lsquo;software\u0026rsquo;, and \u0026lsquo;hardware\u0026rsquo;. Duplicate entries were removed. I also extracted city names from company addresses and grouped the data by city (excluding London).\nData Visualization and Project Results Using matplotlib and seaborn, I visualized the distribution of tech companies across non-capital cities in the UK.\nFigure 1: Proportion of Tech Companies in TOP10 Non-Capital Cities of UK Manchester, Birmingham, and Bristol emerge as the top non-capital cities with tech companies, capturing 19.1%, 14.0%, and 10.6% of the share respectively.\nFigure 2: Tech Companies in TOP10 Non-Capital Cities per Company Status Birmingham leads in active tech companies, followed by Manchester and Cambridge.\nConclusion Manchester, Birmingham, Bristol, and Cambridge are identified as the top non-capital UK cities with high-tech potential. These cities are recommended for individuals seeking less-competitive job markets or investors targeting decentralized tech hubs.\n","permalink":"http://localhost:1313/posts/uk_cities_tech_potential/","summary":"\u003cp\u003e\u003cstrong\u003eProject description\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eLanguage:\u003c/em\u003e Python\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eLibraries:\u003c/em\u003e requests, pandas, numpy, matplotlib, seaborn, os, math, time, datetime, json\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eIDE:\u003c/em\u003e Microsoft Visual Studio Code, Jupyter Notebook\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eProject type:\u003c/em\u003e Data analytics, Web scraping, API\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003e\u003ca href=\"https://www.gov.uk/government/organisations/companies-house\"\u003eCompanies House\u003c/a\u003e is an agency formed by the British Government to maintain the registration of all the companies in the UK. It maintains a database that stores the information of the registered companies. Each company has features such as company name, Standard Industrial Classification (SIC) code, creation date, cessation date (if ceased operating), company board and shareholder info, etc. By using an API, it is possible to scrape the data from that database by using various specifications.\u003c/p\u003e","title":"Non-capital UK Cities with High-tech Potential"},{"content":"Project description\nLanguage: Python Working file: Jupyter notebook In 2018, former President of South Africa, Jacob Zuma, established a commission of enquiry in state capture, known as The Judicial Commission of Inquiry into Allegations of State Capture, Corruption and Fraud in the Public Sector including Organs of State, or simply the Zondo Commission. The commission collected one exabyte of evidence, and on 22 June 2022 released its final report. The reports are available publicly in this link.\nImage is taken from www.corruptionwatch.org.za\nThis project aims to analyze the contents of these reports to capture the names of publicly traded companies whose names are mentioned more frequently. The results would be beneficial for prudent investors who seek ethical investments opportunities in South Africa.\nData Collection and Cleaning Two datasets were used in this project. First dataset were created by converting the report .pdf files into .csv format. PyPDF2 module was used for transforming each pdf content into a text-formatted object, and Pandas was used to store those text-formatted objects into a dataframe. The final dataset of the texts were converted to .csv file and uploaded to Github, where the file\u0026rsquo;s raw format was used.\nimport pandas, PyPDF2 # storing all the pdf files in a list report_paths = [\u0026#39;OCR version - State Capture Commission Report Part 1 Vol I.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part II Vol II.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part III Vol I - Bosasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part III Vol II - Bosasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part III Vol III - Bosasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part III Vol IV - Bosasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part IV Vol I - NT,EOH,COJ,Alexkor.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part IV Vol II- FS.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part IV Vol III - Eskom.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part IV Vol IV - Eskom.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part V Vol I - SSA.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part V Vol II - SABC,Waterkloof,Prasa.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part VI Vol I - Estina,Vrede.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part VI Vol II - CR.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part VI Vol III - Flow of Funds.pdf\u0026#39;, \u0026#39;OCR version - State Capture Commission Report Part VI Vol IV - Recommendations.pdf\u0026#39;] # creating a dataframe file zondo_reports = pd.DataFrame(columns=[\u0026#39;report\u0026#39;, \u0026#39;text\u0026#39;]) i = 0 # iterating through the pdf list for path in report_paths: # converting each pdf content into a text object pdfFileObj = open(path, \u0026#39;rb\u0026#39;) print(\u0026#39;opened\u0026#39;, path) pdfReader = PyPDF2.PdfReader(pdfFileObj) text=\u0026#39;\u0026#39; for page in pdfReader.pages: text += page.extract_text() print(\u0026#39;extracted text for\u0026#39;, path) # adding the converted text objects into dataframe zondo_reports.loc[i] = [path, text] i+=1 pdfFileObj.close() print(\u0026#39;closed\u0026#39;, path) After obtaining the first dataset, we needed another dataset consisting of the companies that are publicly traded in South Africa. For this objective, we used Johannesburg Stock Exchange (JSE) portal to pick up the candidate companies. January 2021 dataset of the companies were downloaded from the JSE website in .xlsx format. The file then were re-formatted into .csv to be readable from GitHub. The .csv file was uploaded to my GitHub account, and its raw format was called using read_csv method of Pandas.\nAs of now, we have a dataset containing reports contents and a dataset containing the publicly traded companies in South Africa.\nAnalysis To carry out the text analysis, nltk module was primarily used. The below code displays how different classes of that module were imported:\nimport nltk, string from nltk.tokenize import word_tokenize nltk.download(\u0026#39;punkt\u0026#39;) from nltk.corpus import stopwords nltk.download(\u0026#39;stopwords\u0026#39;) from nltk.probability import FreqDist Firstly, we used word_tokenize function to tokenize all the reports contents into a single variable. Although the best practice is to lowercase all the tokens, we intentionally left the words as they were, given the fact that we were looking for company names, which are proper nouns. Thus, leaving proper nouns as they were was a better choice for the analysis.\nTo get rid of unnecessary words, we created a variable consisting of stop words from nltk.corpus and punctuations from string module. Further, some additional junk words were added to be removed from the tokens.\n# remove some stops from the tokens junk_tokens = [\u0026#39;Mr\u0026#39;,\u0026#39;Ms\u0026#39;,\u0026#39;Dr\u0026#39;,\u0026#39;P\u0026#39;,\u0026#39;``\u0026#39;, \u0026#39;\\\u0026#39;s\u0026#39;,\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\\\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\\\u0026#39;\\\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\u0026#34;\u0026#39;,\u0026#39;\u0026#34;\u0026#39;,\u0026#39;................................\u0026#39;,\u0026#39;L\u0026#39;] removables = set(stopwords.words(\u0026#39;English\u0026#39;) + list(string.punctuation) + list(string.digits) + junk_tokens) filtered_tokens = [token for token in all_content_tokens if token not in removables] Numerous company names come with additional descriptive words attached to them such as \u0026lsquo;Holding\u0026rsquo;, \u0026lsquo;Corporation\u0026rsquo;, \u0026lsquo;Limited\u0026rsquo;, and etc. We created a function that checks the name for each company and leaves those additions out. The new names were added to the dataframe under a column name \u0026lsquo;search term\u0026rsquo;. Afterwards, finally, time came for searching for company names (using the search terms) inside the reports (texts that are stored in the first dataframe).\nFor companies with one search term, we just searched the tokens as they are single words. For the companies with two or three words in their names, we used bigrams and trigrams from nltk, respectively. An additional column was added to the dataframe indicating True if the company name is mentioned in the reports and False otherwise.\n# Bigrams and trigrams are created to search for two-word and three-word search terms individually. bigrams = list(nltk.bigrams(filtered_tokens)) trigrams = list(nltk.trigrams(filtered_tokens)) # Create new column that will store whether a company name is mentioned or not. listed_companies[\u0026#34;FoundInReport\u0026#34;] = \u0026#34;False\u0026#34; for ind in listed_companies.index: searchterm = listed_companies[\u0026#39;SearchTerm\u0026#39;][ind] if len(searchterm) == 1: if searchterm[0] in filtered_tokens: # search for one-word search terms print(\u0026#39;1 word company appeared in text:\u0026#39;, searchterm) listed_companies.at[ind,\u0026#39;FoundInReport\u0026#39;] = True if len(searchterm) == 2: if searchterm in bigrams: # search for two-word search terms print(\u0026#39;2 word company appeared in text:\u0026#39;, searchterm) listed_companies.at[ind,\u0026#39;FoundInReport\u0026#39;] = True if len(searchterm) == 3: if searchterm in trigrams: # search for three-word search terms print(\u0026#39;3 word company appeared in text:\u0026#39;, searchterm) listed_companies.at[ind,\u0026#39;FoundInReport\u0026#39;] = True Next, we used FreqDist function from nltk.probability to calculate how many times each company name is mentioned in all reports combined.\nResults Matplotlib and seaborn libraries were used for the purpose of visualization. The following figures were drawn to illustrate the results.\nFigure 1: The frequency of company name mentions by sector Figure 1 shows how many times the names of companies in each sector is mentioned in the reports. Banking companies top the chart, by having 236 name mentions in total. It is followed by Mining, Software\u0026amp;Computing Services, and Media, by being mentioned 197, 110, and 76 times, respectively.\nFigure 2: Frequency of mentions for company Figure 2 clearly depicts the company names that were mentioned in the reports. Glencore Plc is the company with the highest quantity of mentions (190). Standard Bank Group Limited and Nedbank Group Limited are the main banks whose names are widely used in the reports. The following are the EOH Holding Limited and MultiChoice Group Limited.\nNote: It is important to note that the results of this analysis do not impose any kind of allegation against any of the companies mentioned above. As a matter of fact, the analysis focuses on the numerical counts rather than the context of the mentions. More clearly, it does not claim that the company names are mentioned in a negative manner. Thus, this project has only one primary purpose to be a guide for the investors. Additional in-depth research is up to the investors when they ponder of investing on those firms.\n","permalink":"http://localhost:1313/posts/zondo_reports_analysis/","summary":"\u003cp\u003e\u003cstrong\u003eProject description\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eLanguage:\u003c/em\u003e Python\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eWorking file:\u003c/em\u003e Jupyter notebook\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003eIn 2018, former President of South Africa, Jacob Zuma, established a commission of enquiry in state capture, known as The Judicial Commission of Inquiry into Allegations of State Capture, Corruption and Fraud in the Public Sector including Organs of State, or simply the Zondo Commission. The commission collected one exabyte of evidence, and on 22 June 2022 released its final report. The reports are available publicly in this link.\u003c/p\u003e","title":"Zondo Reports Text Analysis"}]