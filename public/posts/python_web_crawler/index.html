<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Building a Web Crawler using Python | Mr. Zeynalli&#39;s Blog</title>
<meta name="keywords" content="Web Analytics, Web Crawling, Python">
<meta name="description" content="A guide on building a simple web crawler using Python, including a deep dive into web crawling concepts and practical implementation.">
<meta name="author" content="Elvin Zeynalli">
<link rel="canonical" href="http://localhost:1313/posts/python_web_crawler/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css" integrity="sha256-j&#43;ECM6cGvIfy4Is8&#43;XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/python_web_crawler/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Mr. Zeynalli&#39;s Blog (Alt + H)">Mr. Zeynalli&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Building a Web Crawler using Python
    </h1>
    <div class="post-description">
      A guide on building a simple web crawler using Python, including a deep dive into web crawling concepts and practical implementation.
    </div>
    <div class="post-meta"><span title='2023-10-01 18:33:53 +0300 +0300'>October 1, 2023</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;2133 words&nbsp;·&nbsp;Elvin Zeynalli

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#webcrawler-class">WebCrawler Class</a></li>
    <li><a href="#checking-url-success">Checking URL Success</a></li>
    <li><a href="#processing-pages">Processing Pages</a></li>
    <li><a href="#crawling-the-webpage">Crawling the Webpage</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><p>In this article, I’m going to explain the concept of a web crawler, how search engines work, and guide you in building a simple Crawler bot. All the code I’ve written for this can be found on <a href="https://github.com/mrzeynalli/web_crawler/tree/main">my GitHub repository</a>, and feel free to use, modify, or replicate it for personal or commercial purposes without needing my consent.</p>
<hr>
<p><strong>Project description</strong></p>
<ul>
<li><em>Language:</em> Python</li>
<li><em>Libraries:</em> request, bs4, regex, os, json</li>
<li><em>IDE:</em> Microsoft Visual Studio Code</li>
<li><em>Project type:</em> Web crawling/scraping</li>
</ul>
<p><img alt="Photo by Timothy Dykes on Unsplash" loading="lazy" src="/images/web_crawler/1.webp">
<em>Photo by Timothy Dykes on Unsplash</em></p>
<p>As the name suggests, a web crawler is an application that explores the web like a spider, gathering the desired information. Well-known search engines like Google, Bing, and Yahoo have incredibly fast crawlers that navigate the internet in a matter of seconds (although they don’t crawl the entire web all the time, they minimize the number of web pages to consider using indexing). The good news is, you too can create your own web crawler using Python, and it only requires around 100 lines of code (actually, it’s exactly 100 lines!).</p>
<h1 id="web-crawler">Web Crawler<a hidden class="anchor" aria-hidden="true" href="#web-crawler">#</a></h1>
<p>First, let’s break down the process into smaller steps to understand how to build the crawling bot (“First Principles”). The steps the bot needs to follow are as follows:</p>
<ol>
<li>Access the webpage</li>
<li>Request its content (HTML source code)</li>
<li>Analyze the content and gather internal links</li>
<li>Extract text information from each link</li>
<li>Repeat until a pre-defined threshold is reached</li>
</ol>
<p>To perform these steps, we need to install/import specific Python libraries. For the first two steps, we will use the <strong>requests</strong> library, which allows us to send a request to a webpage to get its content. Next, we will use the <strong>bs4</strong> library to help us understand the collected content and parse it for extracting the desired information, which in this case is the internal links and their text content. The bot will also follow these steps for the collected links. Now, let’s delve into the technical details of building the bot.</p>
<h2 id="webcrawler-class">WebCrawler Class<a hidden class="anchor" aria-hidden="true" href="#webcrawler-class">#</a></h2>
<p>We need to create a class named “WebCrawler” and define its parameters:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># build the web crawler object</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">WebCrawler</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># create three variables: start_url, max_depth, list of visited urls</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start_url</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">start_url</span> <span class="o">=</span> <span class="n">start_url</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">visited</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
</span></span></code></pre></div><p>The class requires two input parameters: start_url and max_depth, along with an initial parameter visited.</p>
<ul>
<li><strong>start_url</strong> — This is the main webpage we want to crawl (format: <a href="https://example.co.uk">https://example.co.uk</a>)/).</li>
<li><strong>max_depth</strong> — This indicates the depth of the crawling. Setting it to 1 will make our bot only follow the links collected from the starting URL. When set to 2, it will follow the links within those internal (from the initial URL) pages. The time complexity increases exponentially as the max_depth value increases.</li>
<li><strong>visited</strong> — This variable is a set that lists the URLs that have already been visited. It prevents the bot from revisiting the same link multiple times. Note that this variable is a set object rather than a list, which avoids adding duplicate entries (URLs).</li>
</ul>
<h2 id="checking-url-success">Checking URL Success<a hidden class="anchor" aria-hidden="true" href="#checking-url-success">#</a></h2>
<p>Inside the <em>WebCrawler</em> class, we need to create another function to ensure that our request to the starting page is successful. This can be determined by checking the status code of the HTML request. Please note that a status code of 200 indicates a successful request.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># create a function to make sure that the primary url is valid</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">is_successful</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">start_url</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span> <span class="c1"># request the page info </span>
</span></span><span class="line"><span class="cl">        <span class="n">response</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span> <span class="c1"># raises exception when not a 2xx response</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span> <span class="c1"># check if the status code is 200, a.k.a successful</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span> <span class="c1"># if not, print the error with the status code</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;The crawling could not being becasue of unsuccessful request with the status code of </span><span class="si">{</span><span class="n">response</span><span class="o">.</span><span class="n">status_code</span><span class="si">}</span><span class="s2">.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">except</span> <span class="n">requests</span><span class="o">.</span><span class="n">HTTPError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span> <span class="c1"># if HTTPS Error occured, print the error message</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;HTTP Error occurred: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span> <span class="c1"># if any other error occured, print the error message</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;An error occurred: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p>With this debugging approach, the crawling process will only be executed if the request is successful. Otherwise, it will either return the status code of an unsuccessful request or an HTTP or other encountered error. The <em>timeout</em> parameter determines the amount of time, in seconds, that the request attempt should wait before raising an error. It is different from the <em>time.sleep(10)</em> function. In this case, if the request is successful, the information will be retrieved immediately. However, if an error occurs, it will wait for 10 seconds before concluding an error. This is an important parameter to set up as sometimes the server may take a few seconds to respond.</p>
<h2 id="processing-pages">Processing Pages<a hidden class="anchor" aria-hidden="true" href="#processing-pages">#</a></h2>
<p>The next step is to create a function that will enter a webpage and gather the links present as well as the text.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># create a function to get the links</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">process_page</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">,</span> <span class="n">depth</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># apply depth threshold</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">depth</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="ow">or</span> <span class="n">url</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">visited</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="nb">set</span><span class="p">(),</span> <span class="s1">&#39;&#39;</span> <span class="c1"># return empty set and string</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">visited</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">url</span><span class="p">)</span> <span class="c1"># add the visited url to the set</span>
</span></span><span class="line"><span class="cl">    <span class="n">links</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span> <span class="c1"># create a set to store the collected links</span>
</span></span><span class="line"><span class="cl">    <span class="n">content</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span> <span class="c1"># create a variable to store the content extracted</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># request the content of a url</span>
</span></span><span class="line"><span class="cl">        <span class="n">r</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span> <span class="c1"># check if the request status is successful</span>
</span></span><span class="line"><span class="cl">        <span class="n">soup</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="s1">&#39;html.parser&#39;</span><span class="p">)</span> <span class="c1"># parse the content of the collected HTML</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Extract the links</span>
</span></span><span class="line"><span class="cl">        <span class="n">anchors</span> <span class="o">=</span> <span class="n">soup</span><span class="o">.</span><span class="n">find_all</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">)</span> <span class="c1"># find all the anchors</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">anchor</span> <span class="ow">in</span> <span class="n">anchors</span><span class="p">:</span> <span class="c1"># merge the anchor with the starting url</span>
</span></span><span class="line"><span class="cl">            <span class="n">link</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">urljoin</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">anchor</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;href&#39;</span><span class="p">))</span> <span class="c1"># get the link and join it with the starting url</span>
</span></span><span class="line"><span class="cl">            <span class="n">links</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">link</span><span class="p">)</span> <span class="c1"># add the link to the previously created set</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Extract the content from the url</span>
</span></span><span class="line"><span class="cl">        <span class="n">content</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">par</span><span class="o">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">par</span> <span class="ow">in</span> <span class="n">soup</span><span class="o">.</span><span class="n">find_all</span><span class="p">(</span><span class="s1">&#39;p&#39;</span><span class="p">)])</span> <span class="c1"># get all the text</span>
</span></span><span class="line"><span class="cl">        <span class="n">content</span> <span class="o">=</span>  <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[\n\r\t]&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">content</span><span class="p">)</span> <span class="c1"># remove the sequence characters</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">except</span> <span class="n">requests</span><span class="o">.</span><span class="n">RequestException</span><span class="p">:</span> <span class="c1"># if the request encounters an error, pass</span>
</span></span><span class="line"><span class="cl">        <span class="k">pass</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">links</span><span class="p">,</span> <span class="n">content</span> <span class="c1"># return the set of the collected links and the contet of the current url</span>
</span></span></code></pre></div><p>The function called <em>process_page</em> is inside the WebCrawler class and is responsible for collecting the link extensions found on a page, combining them with the primary URL, and extracting the text from each URL. Initially, it checks if the threshold (max depth, in this case) has been reached. If the depth is still below the predetermined limit, the crawling process continues. First, the bot adds the starting URL to the <em>visited</em> set and creates another set to collect the links found on that page. It is important to note that the links on a page do not follow a URL structure but are presented as link extensions (explained below). Therefore, it is crucial to merge them before appending them as final links.</p>
<ul>
<li>link or URL — it begins with an HTTPS code and contains the complete reference structure (examples: <a href="https://example.co.uk">https://example.co.uk</a>, <a href="https://example.co.uk/contact">https://example.co.uk/contact</a>)</li>
<li>link extension — this refers only to the extension of the primary URL, essentially the part of the URL that comes after the “/” character (examples: /home, /contact, /careers)</li>
</ul>
<p>The function then utilizes the <em>BeautifulSoup</em> object from the <strong>bs4</strong> library to parse the HTML source code of the webpage and scrape the page content. This content includes everything present on the page in text format, including the URL extensions. The bot searches for anchor elements within the soup by looking for ‘a’ tags, which contain the hyperlinks to other pages. After collecting the anchors, it iterates through each one to search for the <em>‘href’</em> tag, which contains the actual links. The compat.urljoin method from the request library is used to join the link with the URL and add the final URL to the set of links. Next, it looks for the <em>‘p’</em> tags, which contain plain text units, and joins them together within a particular page, storing the content in the content variable. To remove any escape sequences such as “/n”, “/r”, and “/t” (representing new line, return, and tab characters) and retain only the plain text, the sub method from the <strong>regex</strong> module (imported as <strong>re</strong>) is used to replace these sequences with an empty string. Finally, this process is debugged to handle any potential errors with internal links, in addition to the starting URL. Ultimately, this function returns the collected links and the text content of the requested URL.</p>
<h2 id="crawling-the-webpage">Crawling the Webpage<a hidden class="anchor" aria-hidden="true" href="#crawling-the-webpage">#</a></h2>
<p>The scraping function has been implemented, and now it’s time to build the crawling function. This function applies the scraping process to each of the links collected from a URL. Here is the code for this function:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># crawl the web within the depth determined</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">crawl</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_successful</span><span class="p">():</span> <span class="c1"># check if the requesting the starting url info is valid to continue crawling</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">urls_content</span> <span class="o">=</span> <span class="p">{}</span> <span class="c1"># create a dictionary to store the links as keys and contents as values</span>
</span></span><span class="line"><span class="cl">        <span class="n">urls_to_crawl</span> <span class="o">=</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">start_url</span><span class="p">}</span> <span class="c1"># start crawling from the initial url</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># crawl the web within the depth determined</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">depth</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">new_urls</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span> <span class="c1"># create a set to store the internal new urls</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls_to_crawl</span><span class="p">:</span>  <span class="c1"># crawl through the urls</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">url</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">visited</span><span class="p">:</span> <span class="c1"># check and make sure that the url is not crawled before</span>
</span></span><span class="line"><span class="cl">                    <span class="n">links</span><span class="p">,</span> <span class="n">content</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_page</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">depth</span><span class="p">)</span> <span class="c1"># return the links and content of the crawled url</span>
</span></span><span class="line"><span class="cl">                    <span class="n">urls_content</span><span class="p">[</span><span class="n">url</span><span class="p">]</span> <span class="o">=</span> <span class="n">content</span> <span class="c1"># add the url as a key and content as a value to the disctionary created previously</span>
</span></span><span class="line"><span class="cl">                    <span class="n">new_urls</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">links</span><span class="p">)</span> <span class="c1"># add the internal links to the previously created set</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">urls_to_crawl</span> <span class="o">=</span> <span class="n">new_urls</span> <span class="c1"># change the urls to crawl list to crawl through the internal links</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># create a folder to store the crawled websites</span>
</span></span><span class="line"><span class="cl">        <span class="n">current_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">()</span> <span class="c1"># get the current working directory</span>
</span></span><span class="line"><span class="cl">        <span class="n">folder_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">current_dir</span><span class="p">,</span><span class="s1">&#39;crawled_websites&#39;</span><span class="p">)</span> <span class="c1"># create a folder inside the current directory</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">folder_dir</span><span class="p">):</span> <span class="c1"># check if the folder already exists</span>
</span></span><span class="line"><span class="cl">            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">folder_dir</span><span class="p">)</span> <span class="c1"># if not, create the folder directory</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">filename</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\W+&#39;</span><span class="p">,</span> <span class="s1">&#39;_&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_url</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;_crawling_results.json&#39;</span> <span class="c1"># format the filename to modify unsupported characters</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># save the results as a json file in the local directory</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">folder_dir</span><span class="p">,</span><span class="n">filename</span><span class="p">),</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">urls_content</span><span class="p">,</span> <span class="n">file</span><span class="p">,</span> <span class="n">ensure_ascii</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># ensure to keep the unicode characters and indent to make it more readable</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">urls_content</span> <span class="c1"># return the disctionary storing all urls and their content</span>
</span></span></code></pre></div><p>The crawling process begins by checking the URL’s validity using the previously created function. If the check is successful, the function creates a dictionary to store the links along with their respective text content. It then iterates through the range of the maximum depth set in the object. For each depth, it creates a set to store the collected links to be iterated through subsequently. After confirming that the URL has not been visited before, it enters the current URL and uses the <em>process_page</em> function to extract all the internal links and text content. The new_urls set is then updated with the collected links, and each content is added to the dictionary with the internal link as the key. Once the crawling of a URL is complete, the newly collected URLs replace the <em>urls_to_crawl</em>, and the same steps are repeated for each of them.</p>
<p>Before returning the dictionary with lists as keys and content as values, the function also saves the information as a JSON file in a folder created in the current working directory. To accomplish this, the <strong>JSON</strong> and <strong>OS</strong> modules of Python are used. The folder name is defined and joined with the current directory using <em>os.path.join</em>. The function checks if the folder already exists with <em>os.isdir(foldername)</em>. If it doesn’t exist, the function creates the folder and dumps the dictionary information into a JSON file within the created folder.</p>
<h1 id="can-i-crawl-the-entire-web">Can I crawl the entire web?<a hidden class="anchor" aria-hidden="true" href="#can-i-crawl-the-entire-web">#</a></h1>
<p><img alt="Photo by Timothy Dykes on Unsplash" loading="lazy" src="/images/web_crawler/2.webp">
<em>Photo by Timothy Dykes on Unsplash</em></p>
<p>While it is technically possible to crawl a large portion of the clear web (unlike the dark web), it’s important to be aware of legal and ethical considerations. Some websites have specific permissions regarding web crawling, and not adhering to these permissions can lead to legal and ethical issues. Therefore, it’s always best practice to ensure proper authorization before proceeding with web crawling activities. You can usually find the necessary authorization information in the website’s “robots.txt” file. Take a moment to review this file and follow its guidelines.</p>
<p>It’s worth noting that the speed at which giant search engines like Google and Bing crawl the web is quite impressive and difficult to replicate. These search engines employ a vast amount of computational power and infrastructure to crawl and index web pages efficiently. They have dedicated teams of engineers and data centers around the world to support their crawling operations. Creating web crawlers that match the speed and efficiency of these search engines requires is practically impossible for an individual with a laptop in his/her basement as it requires substantial resources, including powerful servers, advanced algorithms, and a team of experts.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/web-analytics/">Web Analytics</a></li>
      <li><a href="http://localhost:1313/tags/web-crawling/">Web Crawling</a></li>
      <li><a href="http://localhost:1313/tags/python/">Python</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/posts/scottish_schools_clustering/">
    <span class="title">« Prev</span>
    <br>
    <span>Analysis of State Schools in Scotland: K-Means Clustering by Deprivation Rate and Pupils Quantity</span>
  </a>
  <a class="next" href="http://localhost:1313/posts/web_analytics_clickstream/">
    <span class="title">Next »</span>
    <br>
    <span>Web Analytics: Analyzing clickstream of 160,000 visitors</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Building a Web Crawler using Python on x"
            href="https://x.com/intent/tweet/?text=Building%20a%20Web%20Crawler%20using%20Python&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fpython_web_crawler%2f&amp;hashtags=WebAnalytics%2cWebCrawling%2cPython">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Building a Web Crawler using Python on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fpython_web_crawler%2f&amp;title=Building%20a%20Web%20Crawler%20using%20Python&amp;summary=Building%20a%20Web%20Crawler%20using%20Python&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fpython_web_crawler%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Building a Web Crawler using Python on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fpython_web_crawler%2f&title=Building%20a%20Web%20Crawler%20using%20Python">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Building a Web Crawler using Python on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fpython_web_crawler%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Building a Web Crawler using Python on whatsapp"
            href="https://api.whatsapp.com/send?text=Building%20a%20Web%20Crawler%20using%20Python%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fpython_web_crawler%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Building a Web Crawler using Python on telegram"
            href="https://telegram.me/share/url?text=Building%20a%20Web%20Crawler%20using%20Python&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fpython_web_crawler%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Building a Web Crawler using Python on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Building%20a%20Web%20Crawler%20using%20Python&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fpython_web_crawler%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Mr. Zeynalli&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
